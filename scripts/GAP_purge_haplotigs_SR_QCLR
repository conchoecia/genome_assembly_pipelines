"""
Haplotig removal from several candidate genome assemblies.

This pipeline is stage two in diploid genome assembly for invertebrates.

It first characterizes how many reads are mapped to each genome assembly.
Then, with some user intervention it purges haplotigs.
After purging haplotigs it clips the overlapping ends of the genome assembly.
Lastly, it characterizes the resulting assemblies and makes a table for the user to look at
 to select the best assemblies.

This version also QCs with the long reads.
"""

from subprocess import Popen, PIPE
import os
import pandas as pd
import subprocess
import sys
import time
import functions

def percent_of_data_mapping(intext, inbam, outpermap):
    """
    This calculates the percent of data mapping to a bam file
     and saves the stats to a file.
    """
    transcript_to_len = {}
    added = set()
    tot_mapped = 0
    num_reads = 0
    tot_size = 0
    print("reading in the thing")
    with open(intext, "r") as f:
        for line in f:
            if line.strip():
                splitd = line.split()
                transcript_to_len[splitd[0]] = int(splitd[1])
                tot_size += int(splitd[1])
                num_reads += 1
    print("done reading in the read_id_to_length")
    command = "samtools view -F 4 {} | cut -f1".format(inbam)
    #command = "cat temp.sam"

    #print("command is: ", command)
    #sys.exit()
    process = Popen(command, stdout=PIPE, shell=True)
    blank_counter = 0
    while True:
        line = process.stdout.readline().rstrip().decode("utf-8").strip()
        if (len(added) % 1000) == 0:
           sys.stdout.write("  {0:.3f}% Done. {1:.3f}% of data mapped.\r".format(
                            100 * (len(added)/num_reads),
                            100 * (tot_mapped/tot_size)))
           sys.stdout.flush()
        if not line.strip():
            blank_counter += 1
            if blank_counter > 99999:
                break
        else:
            if line not in added:
                added.add(line)
                tot_mapped += transcript_to_len[line]

    with open(outpermap, "w") as f:
        print("Number of bases mapped: {}".format(tot_mapped), file=f)
        print("Number of bases total: {}".format(tot_size), file=f)
        print("{0:.4}%".format(100* (tot_mapped/tot_size)), file=f)
        print("", file=f)
        print("Number of reads mapped: {}".format(len(added)), file=f)
        print("Number of reads total: {}".format(num_reads), file=f)
        print("{0:.4}%".format(100* (len(added)/num_reads)), file=f)


filepath = os.path.dirname(os.path.realpath(__file__))
fs_path=os.path.join(filepath, "../bin/fasta_stats")

config["tool"] = "GAP_purge_haplotigs_SR"
CUTOFFS = [95, 90, 80, 70]

configfile: "config.yaml"

rule all:
    input:
        #expand("PH_pipeline/bams/SR_to_{nom}.sorted.bam", nom=config["assemblies"]),
        "PH_pipeline/data_output/SR_read_id_and_length.txt",
        "PH_pipeline/data_output/LR_read_id_and_length.txt",
        expand("PH_pipeline/before_PH/{nom}_SR.flagstat", nom=config["assemblies"]),
        expand("PH_pipeline/before_PH/{nom}_SR.permap.txt", nom=config["assemblies"]),
        expand("PH_pipeline/before_PH/{nom}_LR.flagstat", nom=config["assemblies"]),
        expand("PH_pipeline/before_PH/{nom}_LR.permap.txt", nom=config["assemblies"]),
        #get the histogram pdf
        expand("PH_pipeline/coverage/plot_histo_SR_{nom}.pdf", nom=config["assemblies"]),
        expand("PH_pipeline/PH_hist/SR_to_{nom}.sorted.bam.gencov", nom=config["assemblies"]),
        #purge
        expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta", nom=config["assemblies"], CO=CUTOFFS),
        #clip
        expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta", nom=config["assemblies"], CO=CUTOFFS),
        #now collect information about the clip
        expand("PH_pipeline/after_PH/{nom}_CO{CO}_SR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        expand("PH_pipeline/after_PH/{nom}_CO{CO}_SR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        expand("PH_pipeline/coverage_after_clip/plot_histo_SR_{nom}_CO{CO}.pdf", nom=config["assemblies"], CO=CUTOFFS),
        "PH_pipeline/final_results/final_results.tsv"

rule map_SR_to_assembly:
    input:
        SHOTF = config["shot_f"],
        SHOTR = config["shot_r"]
    output:
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam"
    params:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    threads:
        workflow.cores - 1
    shell:
        """
        bwa index {params.assembly}
        bwa mem -t {threads} {params.assembly} {input.SHOTF} {input.SHOTR} | \
            samtools view -hb -@ {threads} - | \
            samtools sort -@ {threads} - > {output.bam}
        samtools index {output.bam}
        """

rule map_LR_to_assembly:
    input:
        longreads = config["longreads"]
    output:
        bam = "PH_pipeline/bams/LR_to_{nom}.sorted.bam"
    params:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    threads:
        workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} {params.assembly} {input.longreads} | \
            samtools view -hb -@ {threads} - | \
            samtools sort -@ {threads} - > {output.bam}
        samtools index {output.bam}
        """

rule generate_SR_read_length:
    """
    This generates a file with the read id and length.
     This information is used to calculate the percent of data from reads
     that map to the assembly.

    This rule handles this for the short reads.
    """
    input:
        SR = config["shot_f"]
    output:
        txt = "PH_pipeline/data_output/SR_read_id_and_length.txt"
    threads:
        1
    shell:
        """
        bioawk -cfastx '{{ printf("%s\\t%s\\n", $name, length($seq)) }}' {input.SR} > {output.txt}
        """

rule generate_LR_read_length:
    """
    This generates a file with the read id and length.
     This information is used to calculate the percent of data from long reads
     that map to the assembly.

    This rule handles this for the long reads.
    """
    input:
        LR = config["longreads"]
    output:
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    threads:
        1
    shell:
        """
        bioawk -cfastx '{{ printf("%s\\t%s\\n", $name, length($seq)) }}' {input.LR} > {output.txt}
        """

rule flagstat_SR_mapped_before_purge:
    input:
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam"
    output:
        flagstat_info = "PH_pipeline/before_PH/{nom}_SR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule flagstat_LR_mapped_before_purge:
    input:
        bam = "PH_pipeline/bams/LR_to_{nom}.sorted.bam"
    output:
        flagstat_info = "PH_pipeline/before_PH/{nom}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule percent_of_SR_data_mapping_before:
    input:
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam",
        txt = "PH_pipeline/data_output/SR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/before_PH/{nom}_SR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.txt, input.bam, output.per_map)

rule percent_of_LR_data_mapping_before:
    input:
        bam = "PH_pipeline/bams/LR_to_{nom}.sorted.bam",
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/before_PH/{nom}_LR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.txt, input.bam, output.per_map)

rule cov_histo_check_SR:
    """
    just checks the average coverage of the SR bam files before running
     purge haplotigs
    """
    input:
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam",
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    output:
        cov = "PH_pipeline/coverage/SR_{nom}.cov",
        his = "PH_pipeline/coverage/histo_SR_{nom}.histo",
        pdf = "PH_pipeline/coverage/plot_histo_SR_{nom}.pdf"
    threads:
        1
    shell:
        """
        bedtools genomecov -d -ibam {input.bam} \
            -g {input.assembly} > {output.cov}
        cut -f3 {output.cov} | sort | uniq -c | \
            column -t | sort -k2 -n > {output.his}
        cat {output.his} | plot_uniq_c.py -x 15 -X 400 -d
        mv read_depth_histogram.pdf {output.pdf}
        """

rule PH_hist:
    input:
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam",
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    output:
        gc  = "PH_pipeline/PH_hist/SR_to_{nom}.sorted.bam.gencov",
        pdf = "PH_pipeline/PH_hist/SR_to_{nom}.sorted.bam.histogram.png"
    params:
        gc  = lambda wildcards: "SR_to_{}.sorted.bam.gencov".format(wildcards.nom),
        pdf = lambda wildcards: "SR_to_{}.sorted.bam.histogram.png".format(wildcards.nom)
    threads:
        10
    shell:
        """
        purge_haplotigs hist -b {input.bam} \
            -g {input.assembly} -t {threads} -d 500
        mv {params.gc} {output.gc}
        mv {params.pdf} {output.pdf}
        """

rule PH_cov:
    input:
        gc = "PH_pipeline/PH_hist/SR_to_{nom}.sorted.bam.gencov"
    output:
        csv = "PH_pipeline/PH_cov/{nom}_cov_stats.csv"
    params:
        low = config["low"],
        medium = config["medium"],
        high = config["high"]
    threads:
        1
    shell:
        """
        if [[ {params.low} -eq -1 ]]; then
            exit 1
        fi
        if [[ {params.medium} -eq -1 ]]; then
            exit 1
        fi
        if [[ {params.high} -eq -1 ]]; then
            exit 1
        fi
        purge_haplotigs  cov  -i {input.gc}  \
               -l {params.low} -m {params.medium} -h {params.high} \
               -o {output.csv} -j 80  -s 80
        """

rule PH_purge:
    input:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom],
        csv = "PH_pipeline/PH_cov/{nom}_cov_stats.csv",
        bam = "PH_pipeline/bams/SR_to_{nom}.sorted.bam"
    output:
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta",
        haplotigs = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.haplotigs.fasta"
    params:
        cutoff   = lambda wildcards: wildcards.CO,
        tempdir  = lambda wildcards: "temp_{}_{}".format(wildcards.nom, wildcards.CO),
        mv_these = lambda wildcards: "PH_purge_{0}_CO{1}".format(wildcards.nom, wildcards.CO),
        mkdir1   = "PH_pipeline/PH_purge",
        mkdir2   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies".format(wildcards.nom),
        outdir   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/".format(wildcards.nom, wildcards.CO),
    threads:
        workflow.cores - 1
    shell:
        """
        rm -rf {params.tempdir}
        mkdir {params.tempdir}
        cd {params.tempdir}
        mkdir {params.mv_these}

        purge_haplotigs purge -g {input.assembly} \
               -c ../{input.csv} -t {threads} \
               -d -b ../{input.bam} \
               -a {params.cutoff} \
               -o {params.mv_these}

        if [ ! -d ../{params.mkdir1} ]
        then
            mkdir -p ../{params.mkdir1}
        fi

        if [ ! -d ../{params.mkdir2} ]
        then
            mkdir -p ../{params.mkdir2}
        fi

        if [ ! -d ../{params.outdir} ]
        then
            mkdir -p ../{params.outdir}
        fi

        mv {params.mv_these}* ../{params.outdir}
        mv dotplots* ../{params.outdir}
        cd ..
        rm -rf {params.tempdir}
        """

rule PH_clip:
    input:
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta",
        haplotigs = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.haplotigs.fasta"
    output:
        clip = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    params:
        outdir   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/".format(wildcards.nom, wildcards.CO),
        mv_these = lambda wildcards: "PH_pipeline/PH_purge_{}_CO{}_clip".format(wildcards.nom, wildcards.CO)
    threads:
        workflow.cores - 1
    shell:
        """
        mkdir -p {params.mv_these}
        purge_haplotigs clip  -p {input.assembly} \
               -h {input.haplotigs} \
               -o {params.mv_these}
        # sometimes there are no overlaps, and the directory just ends up empty.
        # in that case we need to handle the file specially.
        if [ -z "$(ls -A /path/to/dir)" ]; then
            cp {input.assembly} {output.clip}
        else
            mv {params.mv_these}* {params.outdir}
        fi
        rm -rf {params.mv_these}
        """

rule map_SR_to_purged:
    input:
        SHOTF = config["shot_f"],
        SHOTR = config["shot_r"],
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    output:
        bam = temp("PH_pipeline/bams/clipped/SR_to_{nom}_CO{CO}_clip.sorted.bam")
    threads:
        workflow.cores - 1
    shell:
        """
        bwa index {input.assembly}
        bwa mem -t {threads} {input.assembly} {input.SHOTF} {input.SHOTR} | \
            samtools view -hb -@ {threads} - | \
            samtools sort -@ {threads} - > {output.bam}
        """

rule map_LR_to_purged:
    input:
        longreads = config["longreads"],
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    output:
        bam = temp("PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam")
    threads:
        workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} {input.assembly} {input.longreads} | \
            samtools view -hb -@ {threads} - | \
            samtools sort -@ {threads} - > {output.bam}
        """

rule flagstat_SR_mapped_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/SR_to_{nom}_CO{CO}_clip.sorted.bam"
    output:
        flagstat_info = "PH_pipeline/after_PH/{nom}_CO{CO}_SR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule flagstat_LR_mapped_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam"
    output:
        flagstat_info = "PH_pipeline/after_PH/{nom}_CO{CO}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule percent_of_SR_data_mapping_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/SR_to_{nom}_CO{CO}_clip.sorted.bam",
        txt = "PH_pipeline/data_output/SR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/after_PH/{nom}_CO{CO}_SR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.txt, input.bam, output.per_map)

rule percent_of_LR_data_mapping_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam",
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/after_PH/{nom}_CO{CO}_LR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.txt, input.bam, output.per_map)

rule cov_histo_check_after_purge:
    """
    just checks the average coverage of the bam files before running
     purge haplotigs
    """
    input:
        bam = "PH_pipeline/bams/clipped/SR_to_{nom}_CO{CO}_clip.sorted.bam",
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    output:
        cov = "PH_pipeline/coverage_after_clip/SR_{nom}_CO{CO}.cov",
        his = "PH_pipeline/coverage_after_clip/histo_SR_{nom}_CO{CO}.histo",
        pdf = "PH_pipeline/coverage_after_clip/plot_histo_SR_{nom}_CO{CO}.pdf"
    threads:
        1
    shell:
        """
        bedtools genomecov -d -ibam {input.bam} \
            -g {input.assembly} > {output.cov}
        cut -f3 {output.cov} | sort | uniq -c | \
            column -t | sort -k2 -n > {output.his}
        cat {output.his} | plot_uniq_c.py -x 15 -X 400 -d
        mv read_depth_histogram.pdf {output.pdf}
        """

rule make_genome_stats_table:
    """
    makes a table of all the relevant genome stats
    """
    input:
        assembly = expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta", nom=config["assemblies"], CO=CUTOFFS),
        per_before_SR = expand("PH_pipeline/before_PH/{nom}_SR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        per_map_SR = expand("PH_pipeline/after_PH/{nom}_CO{CO}_SR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_before_SR = expand("PH_pipeline/before_PH/{nom}_SR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_info_SR = expand("PH_pipeline/after_PH/{nom}_CO{CO}_SR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        per_before_LR = expand("PH_pipeline/before_PH/{nom}_LR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        per_map_LR = expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_before_LR = expand("PH_pipeline/before_PH/{nom}_LR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_info_LR = expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
    output:
        results_table = "PH_pipeline/final_results/final_results.tsv"
    threads: 1
    run:
        all_samples = []
        for nom in config["assemblies"]:
            for CO in CUTOFFS:
                tfile = "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/PH_purge_{0}_CO{1}_clip.fasta".format(nom, CO)
                this_dict = {}
                this_dict["assem_file"] = tfile
                this_dict["PH_CO"] = CO
                this_dict["name"] = nom
                # get the NCBI info first
                # now get the other assembly info from fasta_stats
                fstats_dict = functions.run_fasta_stats(tfile)
                z1 = {**this_dict, **fstats_dict}

                for readtype in ["SR", "LR"]:
                    # BEFORE PURGE HAPLOTIGS
                    # get the flagstat info
                    fstat_file = "PH_pipeline/before_PH/{}_{}.flagstat".format(nom, readtype)
                    z1["before_flagstat_pMap_{}".format(readtype)] = functions.parse_flagstat(fstat_file)
                    #get the manually calculated percent that maps (bases and reads)
                    pmap_file = "PH_pipeline/before_PH/{}_{}.permap.txt".format(nom, readtype)
                    results = functions.parse_permap(pmap_file)
                    z1["before_manual_pBMap_{}".format(readtype)] = results[0]
                    z1["before_manual_pMap_{}".format(readtype)] = results[1]

                    # AFTER PURGE HAPLOTIGS
                    # get the flagstat info
                    fstat_file = "PH_pipeline/after_PH/{0}_CO{1}_{2}.flagstat".format(nom, CO, readtype)
                    z1["after_flagstat_pMap_{}".format(readtype)] = functions.parse_flagstat(fstat_file)
                    #get the manually calculated percent that maps (bases and reads)
                    pmap_file = "PH_pipeline/after_PH/{0}_CO{1}_{2}.permap.txt".format(nom, CO, readtype)
                    results = functions.parse_permap(pmap_file)
                    z1["after_manual_pBMap_{}".format(readtype)] = results[0]
                    z1["after_manual_pMap_{}".format(readtype)] = results[1]

                    # DIFFERENCE
                    z1["before_minus_after_flagstat_pMap_{}".format(readtype)] = z1["after_flagstat_pMap_{}".format(readtype)] - z1["before_flagstat_pMap_{}".format(readtype)]
                    z1["before_minus_after_manual_pBMap_{}".format(readtype)]  = z1["after_manual_pBMap_{}".format(readtype)]  - z1["before_manual_pBMap_{}".format(readtype)]
                    z1["before_minus_after_manual_pMap_{}".format(readtype)]   = z1["after_manual_pMap_{}".format(readtype)]   - z1["before_manual_pMap_{}".format(readtype)]
                    all_samples.append(z1)
                    time.sleep(1)
        # now make a df with all the results
        df = pd.DataFrame(all_samples)
        df.to_csv(output.results_table, sep='\t', index=False)
