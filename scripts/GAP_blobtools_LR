"""
This snakefile runs blobtools on the assembly.
"""
from Bio import SeqIO
from collections import defaultdict
import pandas as pd
import json
import sys

configfile: "config.yaml"
config["tool"] = "GAP_blobtools_LR"

if "LR" not in config:
    raise IOError("You must specify the long reads in a yaml list in the config file. The reads must be in fastq.gz format")

if "minimap2arg" not in config:
    raise IOError("you must specify minimap2arg in the config to tell minimap2 what type of reads you're using.")

if config["minimap2arg"] not in ["map-pb", "map-ont", "map-hifi"]:
    raise IOError("You chose a read type for minimap2arg that will not work here. Choose map-pb, map-ont, or map-hifi")

# now we set up entrez to parse the taxinfo
from Bio import Entrez
if "NCBIemail" not in config:
    raise IOError("You must specify the email of your NCBI account in the config file with the 'NCBIemail' field")
if "NCBIapikey" not in config:
    raise IOError("You must specify the apikey of your NCBI account in the config file with the 'NCBIapikey' field")
Entrez.email  = config["NCBIemail"]
Entrez.api_key = config["NCBIapikey"]


rule all:
    input:
        expand(config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.json.bestsum.phylum.p7.span.100.blobplot.bam0.png",
               nom = config["assemblies"]),
        expand(config["tool"] + "/output/assembly_{nom}/{nom}_metazoan.fasta",
               nom = config["assemblies"]),
        expand(config["tool"] + "/output/assembly_{nom}/{nom}_non_metazoan.fasta",
               nom = config["assemblies"]),

        expand(config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{nom}.sorted.bam.bai",
                       hicnum = list(range(0, len(config["hic"]))),
                       nom = config["assemblies"]),

# this bit makes softlinks of the assemblies and maps the short reads
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

rule download_taxid:
    output:
        taxid_file = filepath + "/../db/nr_accesion_to_taxids.txt"
    threads: 1
    params:
        db = config["nr_db"]
    shell:
        """
        blastdbcmd -db {params.db} -entry all \
           -outfmt "%a %T" | \
           awk '{{print($1, "NCBI_TaxID", $2)}}' > {output.taxid_file}
        """

rule symlink_the_HiC:
    """
    Symlinks the HiC reads so they are easier to work with.
    """
    input:
        hicf = [config["hic"][x]["read1"] for x in config["hic"]],
        hicr = [config["hic"][x]["read2"] for x in config["hic"]]
    output:
        reads_f = expand(config["tool"] + "/input/hic/hic_{hicnum}_f.fastq.gz",
                       hicnum = list(range(0, len(config["hic"])))),
        reads_r = expand(config["tool"] + "/input/hic/hic_{hicnum}_r.fastq.gz",
                       hicnum = list(range(0, len(config["hic"])))),
    threads: workflow.cores - 1
    params:
        fileprefix = config["tool"] + "/input/hic/hic_",
        f_suffix = "_f.fastq.gz",
        r_suffix = "_r.fastq.gz",
    run:
        counter = 0
        for thislib in config["hic"]:
            read1 = config["hic"][thislib]["read1"]
            read2 = config["hic"][thislib]["read2"]
            dest = "{}{}{}".format(params.fileprefix, counter, params.f_suffix)
            os.symlink(read1, dest)
            dest = "{}{}{}".format(params.fileprefix, counter, params.r_suffix)
            os.symlink(read2, dest)
            counter += 1

rule index_ref:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        amb   = config["tool"] + "/input/assembly/{nom}_input.fasta.amb",
        ann   = config["tool"] + "/input/assembly/{nom}_input.fasta.ann",
        bwt   = config["tool"] + "/input/assembly/{nom}_input.fasta.bwt",
        pac   = config["tool"] + "/input/assembly/{nom}_input.fasta.pac",
    threads: 1
    shell:
        """
        bwa index {input.assem}
        """

rule map_hic_to_ref:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        amb   = config["tool"] + "/input/assembly/{nom}_input.fasta.amb",
        ann   = config["tool"] + "/input/assembly/{nom}_input.fasta.ann",
        bwt   = config["tool"] + "/input/assembly/{nom}_input.fasta.bwt",
        pac   = config["tool"] + "/input/assembly/{nom}_input.fasta.pac",
        left  = config["tool"] + "/input/hic/hic_{hicnum}_f.fastq.gz",
        right = config["tool"] + "/input/hic/hic_{hicnum}_r.fastq.gz"
    output:
        bam   = config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{nom}.sorted.bam"
    params:
        sort_threads = max(1, int((workflow.cores)/4))
    threads:
        workflow.cores - 1
    shell:
        """
        bwa mem -5SPM -t {threads} {input.assem} {input.left} {input.right} | \
            samtools view -hb -@ {params.sort_threads} - | \
            samtools sort -@ {params.sort_threads} - > {output.bam}
        """

rule index_hic_bams:
    input:
        bam = config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{nom}.sorted.bam"
    output:
        bai = config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{nom}.sorted.bam.bai"
    threads: 1
    shell:
        """
        samtools index {input.bam}
        """

rule symlink_the_LRs:
    """
    Symlinks the long reads so they are easier to work with in the rest of the
      pipeline
    """
    input:
        LR = config["LR"]
    output:
        assem = expand(config["tool"] + "/input/longreads/LR_{LRnum}.fastq.gz",
                       LRnum = list(range(0, len(config["LR"]))))
    params:
        fileprefix = config["tool"] + "/input/longreads/LR_",
        filesuffix = ".fastq.gz"
    run:
        for i in range(0, len(config["LR"])):
            thisLR = config["LR"][i]
            dest = "{}{}{}".format(params.fileprefix, i, params.filesuffix)
            os.symlink(thisLR, dest)

rule map_LR_to_genome:
    """
    maps the reads to the assembly
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        long_reads = config["tool"] + "/input/longreads/LR_{LRnum}.fastq.gz"
    output:
        bam = config["tool"] + "/output/bams/LR_{LRnum}_to_{nom}.bam"
    params:
        minimaparg = config["minimap2arg"]
    threads: workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} -ax {params.minimaparg} {input.assem} {input.long_reads} | \
          samtools view -hb - | \
          samtools sort - > {output.bam}
        """

rule index_bams:
    input:
        bam = config["tool"] + "/output/bams/LR_{LRnum}_to_{nom}.bam"
    output:
        bai = config["tool"] + "/output/bams/LR_{LRnum}_to_{nom}.bam.bai"
    threads: 1
    shell:
        """
        samtools index {input.bam}
        """

rule get_diamond_nr_hits:
    input:
        db = config["diamond_nr_db"],
        assembly = config["tool"] + "/input/assembly/{nom}_input.fasta",
        taxid_file = filepath + "/../db/nr_accesion_to_taxids.txt"
    output:
        dmnd_out = config["tool"] + "/input/dmnd/{nom}_dmnd.blastx"
    threads:
        workflow.cores - 1
    shell:
        """
        diamond blastx \
          --query {input.assembly} \
          --db {input.db} \
          --outfmt 6 \
          --sensitive \
          --evalue 0.001 \
          --threads {threads} \
          --out {output.dmnd_out}
        """

rule blobtools_taxonify:
    input:
        dmnd_out = config["tool"] + "/input/dmnd/{nom}_dmnd.blastx",
        taxid_file = filepath + "/../db/nr_accesion_to_taxids.txt"
    output:
        taxonify_out = config["tool"] + "/input/dmnd/{nom}_dmnd.blastx.taxified.out",
    params:
        outprefix = lambda wildcards: config["tool"] + "/input/dmnd/"
    shell:
        """
        blobtools taxify -f {input.dmnd_out} -m {input.taxid_file} -s 0 -t 2 -o {params.outprefix}
        """

rule blobtools_create:
    input:
        assembly = config["tool"] + "/input/assembly/{nom}_input.fasta",
        bams =   expand(config["tool"] + "/output/bams/LR_{LRnum}_to_{{nom}}.bam",
                      LRnum = list(range(len(config["LR"])))   ),
        bais =   expand(config["tool"] + "/output/bams/LR_{LRnum}_to_{{nom}}.bam.bai",
                      LRnum = list(range(len(config["LR"])))   ),
        hicbam = expand(config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{{nom}}.sorted.bam",
                      hicnum = list(range(len(config["hic"]))) ),
        hicbai = expand(config["tool"] + "/input/bams_shorts/hic_{hicnum}_to_{{nom}}.sorted.bam.bai",
                      hicnum = list(range(len(config["hic"]))) ),
        taxonify_out = config["tool"] + "/input/dmnd/{nom}_dmnd.blastx.taxified.out",
    output:
        blob_results = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.json"
    params:
        blob_prefix  = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools",
        bam_string   = "-b " + \
                       " -b ".join([config["tool"] + "/output/bams/LR_" + str(x) + "_to_{nom}.bam" 
                                   for x in range(len(config["LR"]))]),
        hic_string   = "-b " + \
                       " -b ".join([config["tool"] + "/input/bams_shorts/hic_" + str(x) + "_to_{nom}.sorted.bam" 
                                   for x in range(len(config["hic"]))]),
    shell:
        """
        # the last bam is the newest
        blobtools create \
           -i {input.assembly} \
           {params.bam_string} \
           {params.hic_string} \
           -t {input.taxonify_out} \
           -o {params.blob_prefix}
        """

rule blobtools_view_and_plot:
    input:
        blob_results = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.json"
    output:
        blob_results = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.json.bestsum.phylum.p7.span.100.blobplot.bam0.png",
        table = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.table.txt"
    params:
        mv_prefix = lambda wildcards: "{}_blobtools.blobDB.".format(wildcards.nom),
        mv_folder = lambda wildcards: config["tool"] + "/output/blobtools_{}/".format(wildcards.nom)
    threads: 1
    shell:
        """
        blobtools view -i {input.blob_results}
        blobtools plot -i {input.blob_results}
        mv {params.mv_prefix}* {params.mv_folder}
        """

rule fix_table_header:
    input:
        table = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.table.txt"
    output:
        table = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.table.fixed.txt"
    run:
        outhandle = open(output.table, "w")
        with open(input.table, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    if line.startswith("##"):
                        # these are just comments, we don't want them
                        pass
                    else:
                        if line.startswith("# name"):
                            print(line.replace("# name", "name"),
                                  file = outhandle)
                        else:
                            print(line, file=outhandle)

def is_metazoa(taxid):
    """
    Just return True if metazoan or False if not
    """
    taxid = taxid.split("-")[0]
    searchterm = "'\"{}\"[Scientific Name]'".format(taxid)
    searchterm = taxid

    species = taxid.replace(" ", "+").strip()
    search = Entrez.esearch(term = species, db = "taxonomy", retmode = "xml")
    records = Entrez.read(search)
    try:
        foundtaxid = records["IdList"][0]

        handle = Entrez.efetch(db="taxonomy", id = foundtaxid , retmode="xml")
        records = Entrez.read(handle)
        lineage = defaultdict(lambda: 'Unclassified')
        lineage["Bilateria"] = "NO"
        lineage["Deuterostomia"] = "NO"
        lineage["Protostomia"] = "NO"
        lineage["Opisthokonta"] = "NO"
        lineage["Metazoa"] = "NO"
        lineage["Organism"] = records[0]['ScientificName']
        for entry in records[0]['LineageEx']:
            if entry['Rank'] in ['superkingdom', 'kingdom', 'phylum',
                                 'class', 'order', 'family',
                                 'genus', 'species']:
                lineage[entry['Rank']] = entry['ScientificName'].split(' ')[-1]
                lineage["{}_id".format(entry['Rank'])] = int(entry['TaxId'].split(' ')[-1])
            # entry as integer
            eai = int(entry["TaxId"])
            if eai == 33213:
                lineage["Bilateria"] = "YES"
            elif eai == 33317:
                lineage["Protostomia"] = "YES"
            elif eai == 33511:
                lineage["Deuterostomia"] = "YES"
            elif eai == 33154:
                lineage["Opisthokonta"] = "YES"
            elif eai == 33208:
                lineage["Metazoa"] = "YES"
        if lineage["Metazoa"] == "YES":
            return True
        else:
            return False
    except:
        return True

rule get_possible_contams:
    """
    This finds the scaffolds that are definitely not metazoan
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        table = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.table.fixed.txt",
        json = config["tool"] + "/output/blobtools_{nom}/{nom}_blobtools.blobDB.json"
    output:
        nm = config["tool"] + "/output/cleanup_{nom}/{nom}_non_metazoan_scafs.txt",
        me = config["tool"] + "/output/cleanup_{nom}/{nom}_metazoan_scafs.txt"
    threads: 1
    run:
        # read in the summary
        df = pd.read_csv(input.table, comment = "#", delimiter = "\t")
        # which column
        thiscol = [x for x in df.columns if x.startswith("phylum.t")][0]

        nonmetaz_taxa = [
            "Actinobacteria",
            "Ascomycota",
            "Bacteria-undef",
            "Bacteroidetes",
            "Basidiomycota",
            "Candidatus Bathyarchaeota",
            "Candidatus Moranbacteria",
            "Candidatus Tectomicrobia",
            "Candidatus Kryptonia",
            "Chlamydiae",
            "Chloroflexi",
            "Chlorophyta",
            "Cyanobacteria",
            "Euryarchaeota",
            "Firmicutes",
            "Microsporidia",
            "Mucoromycota",
            "Nitrospinae",
            "Planctomycetes",
            "Proteobacteria",
            "Spirochaetes",
            "Streptophyta",
            "Thaumarchaeota",
            "Verrucomicrobia",
            "Viruses-undef"]
        metazoan_taxa = ["Annelida",
                         "Arthropoda",
                         "Brachiopoda",
                         "Chordata",
                         "Cnidaria",
                         "Ctenophora",
                         "Echinodermata",
                         "Eukaryota-undef",
                         "Hemichordata",
                         "Mollusca",
                         "Nematoda",
                         "no-hit",
                         "Porifera",
                         "Platyhelminthes",
                         "Placozoa",
                         "Priapulida",
                         "Tardigrada",
                         "undef"]

        not_seen_before = set()
        metazoa_or_not_dict = {}
        print("Checking if things are metazoan")
        print("Taxonomy\tmetazoan?")
        for thisname in df[thiscol].unique():
            edited = thisname.strip()
            are_these_metazoa = False
            if edited in nonmetaz_taxa:
                pass
            elif edited in metazoan_taxa:
                are_these_metazoa = True
            else:
                not_seen_before.add(edited)
                are_these_metazoa = is_metazoa(edited)
            metazoa_or_not_dict[edited] = are_these_metazoa
            print(edited, are_these_metazoa)
        if len(not_seen_before) > 0:
            print()
            print("We don't have these phyla in our script. Contact the authors to add them. This will make the program stop prematurely, so ask the author on github if you'd like this feature removed.")
            print(not_seen_before)
            raise IOError("see above error from stdout")
            print()

        all_seqs = set()
        non_metazoan = set()
        metazoan = set()
        for record in SeqIO.parse(input.assem, "fasta"):
            all_seqs.add(str(record.id))

        # now determine which seqs are metazoa
        for index, row in df.iterrows():
            thisseq = row["name"]
            is_metazoan = metazoa_or_not_dict[row[thiscol]]
            if is_metazoan:
                metazoan.add(thisseq)
            else:
                non_metazoan.add(thisseq)
        test_all = metazoan.union(non_metazoan)
        if not test_all == all_seqs:
            raise IOError("The sequences from the fasta don't match those from the table")

        # now make a list of metazoan or non-metazoan sequences
        with open(output.nm, "w") as f:
            for entry in non_metazoan:
                print(entry, file=f)
        with open(output.me, "w") as f:
            for entry in metazoan:
                print(entry, file=f)

rule nonmetazoan_assembly:
    """
    This finds the scaffolds that are definitely not metazoan
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        nm = config["tool"] + "/output/cleanup_{nom}/{nom}_non_metazoan_scafs.txt",
    output:
        nm = config["tool"] + "/output/assembly_{nom}/{nom}_non_metazoan.fasta"
    threads: 1
    shell:
        """
        seqtk subseq {input.assem} {input.nm} > {output.nm}
        """

rule metazoan_assembly:
    """
    This finds the scaffolds that are metazoan
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        me = config["tool"] + "/output/cleanup_{nom}/{nom}_metazoan_scafs.txt",
    output:
        me = config["tool"] + "/output/assembly_{nom}/{nom}_metazoan.fasta"
    threads: 1
    shell:
        """
        seqtk subseq {input.assem} {input.me} > {output.me}
        """
