"""
After removing haplotigs it is possible to join the ends using long reads.
npScarf does this. This software joins contigs with npScarf then outputs a QC report.
"""
from subprocess import Popen, PIPE
from functions import run_fasta_stats, parse_flagstat, parse_permap
import pandas as pd
import subprocess
import sys
import time
filepath = os.path.dirname(os.path.realpath(__file__))
fs_path=os.path.join(filepath, "../bin/fasta_stats")

configfile: "config.yaml"
maxthreads = 90

print(config["assemblies"])

rule all:
    input:
        # get mapping info before scaffolding
        expand("GAP_npscarf/before_NPS/{nom}_LR.permap.txt", nom=config["assemblies"]),
        expand("GAP_npscarf/before_NPS/{nom}_LR.flagstat", nom=config["assemblies"]),
        # now scaffold with npscarf
        expand("GAP_npscarf/npscarf/{nom}_nps/{nom}_nps.fin.fasta", nom=config["assemblies"]),
        #now collect information about the merge
        expand("GAP_npscarf/after_NPS/{nom}_LR.permap.txt", nom=config["assemblies"]),
        expand("GAP_npscarf/after_NPS/{nom}_LR.flagstat", nom=config["assemblies"]),
        "GAP_npscarf/final_results/final_results.tsv"

rule map_LR_to_assembly:
    input:
        LR = config["long_reads"]
    output:
        bam = temp("GAP_npscarf/bams/LR_to_{nom}.sorted.bam")
    params:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom],
        sort_threads = int(maxthreads/3)
    threads:
        workflow.cores
    shell:
        """
        minimap2 -t {threads} -ax map-pb {params.assembly} {input.LR} | \
            samtools view -hb -@ {params.sort_threads} - | \
            samtools sort -@ {params.sort_threads} - > {output.bam}
        """

rule generate_LR_read_length:
    input:
        LR = config["long_reads"]
    output:
        txt = "GAP_npscarf/data_output/LR_read_id_and_length.txt"
    threads:
        1
    shell:
        """
        bioawk -cfastx '{{ printf("%s\\t%s\\n", $name, length($seq)) }}' {input.LR} > {output.txt}
        """

rule flagstat_mapped_before_purge:
    input:
        bam = "GAP_npscarf/bams/LR_to_{nom}.sorted.bam"
    output:
        flagstat_info = "GAP_npscarf/before_NPS/{nom}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule percent_of_data_mapping_before:
    input:
        bam = "GAP_npscarf/bams/LR_to_{nom}.sorted.bam",
        txt = "GAP_npscarf/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "GAP_npscarf/before_NPS/{nom}_LR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.bam, input.txt, output.per_map)

rule map_LR_for_npscarf:
    input:
        LR = config["long_reads"]
    output:
        sam = temp("GAP_npscarf/bams/LR_to_{nom}.sorted.sam")
    params:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    threads: workflow.cores
    shell:
        """
        minimap2 -t {threads} -Y -ax map-pb {params.assembly} {input.LR} > {output.sam}
        """

rule npscarf_join:
    input:
        sam = "GAP_npscarf/bams/LR_to_{nom}.sorted.sam",
        LR = config["long_reads"],
    output:
        assem = "GAP_npscarf/npscarf/{nom}_nps/{nom}_nps.fin.fasta"
    params:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom],
        prefix   = lambda wildcards: "GAP_npscarf/npscarf/{0}_nps/{0}_nps".format(wildcards.nom)
    threads: maxthreads - 2
    shell:
        """
        cat {input.sam} | \
          jsa.np.npscarf -input - -format sam \
          -seq {params.assembly} -prefix {params.prefix}
        """

rule map_LR_to_merged:
    input:
        LR = config["long_reads"],
        assembly = "GAP_npscarf/npscarf/{nom}_nps/{nom}_nps.fin.fasta"
    output:
        bam = temp("GAP_npscarf/bams/merged/LR_to_{nom}_clip.sorted.bam")
    params:
         sort_threads = int(maxthreads/3)
    threads:
        workflow.cores
    shell:
        """
        minimap2 -t {threads} -ax map-pb {input.assembly} {input.LR} | \
            samtools view -hb -@ {params.sort_threads} - | \
            samtools sort -@ {params.sort_threads} - > {output.bam}
        """

rule flagstat_mapped_after_merge:
    input:
        bam = "GAP_npscarf/bams/merged/LR_to_{nom}_clip.sorted.bam"
    output:
        flagstat_info = "GAP_npscarf/after_NPS/{nom}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule per_of_data_mapping_after_merge:
    """
    measures the percentage of the data in the bam that maps
     to the reference, in % of all bp
    """
    input:
        bam = "GAP_npscarf/bams/merged/LR_to_{nom}_clip.sorted.bam",
        txt = "GAP_npscarf/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "GAP_npscarf/after_NPS/{nom}_LR.permap.txt"
    threads:
        1
    run:
        percent_of_data_mapping(input.bam, input.txt, output.per_map)

rule make_genome_stats_table:
    """
    makes a table of all the relevant genome stats
    """
    input:
        assembly = expand("GAP_npscarf/npscarf/{nom}_nps/{nom}_nps.fin.fasta", nom=config["assemblies"]),
        per_before = expand("GAP_npscarf/before_NPS/{nom}_LR.permap.txt", nom=config["assemblies"]),
        per_map = expand("GAP_npscarf/after_NPS/{nom}_LR.permap.txt", nom=config["assemblies"]),
        flagstat_before = expand("GAP_npscarf/before_NPS/{nom}_LR.flagstat", nom=config["assemblies"]),
        flagstat_info = expand("GAP_npscarf/after_NPS/{nom}_LR.flagstat", nom=config["assemblies"])
    output:
        results_table = "GAP_npscarf/final_results/final_results.tsv"
    run:
        all_samples = []
        for nom in config["assemblies"]:
            tfile = "GAP_npscarf/npscarf/{0}_nps/{0}_nps.fin.fasta".format(nom)
            this_dict = {}
            this_dict["assem_file"] = tfile
            this_dict["name"] = nom
            # get the NCBI info first
            # now get the other assembly info from fasta_stats
            fstats_dict = run_fasta_stats(tfile)
            z1 = {**this_dict, **fstats_dict}

            # BEFORE PURGE HAPLOTIGS
            # get the flagstat info
            fstat_file = "GAP_npscarf/before_NPS/{0}_LR.flagstat".format(nom)
            z1["before_flagstat_pMap"] = parse_flagstat(fstat_file)
            #get the manually calculated percent that maps (bases and reads)
            pmap_file = "GAP_npscarf/before_NPS/{0}_LR.permap.txt".format(nom)
            results = parse_permap(pmap_file)
            z1["before_manual_pBMap"] = results[0]
            z1["before_manual_pMap"] = results[1]

            # AFTER PURGE HAPLOTIGS
            # get the flagstat info
            fstat_file = "GAP_npscarf/after_NPS/{0}_LR.flagstat".format(nom)
            z1["after_flagstat_pMap"] = parse_flagstat(fstat_file)
            #get the manually calculated percent that maps (bases and reads)
            pmap_file = "GAP_npscarf/after_NPS/{0}_LR.permap.txt".format(nom)
            results = parse_permap(pmap_file)
            z1["after_manual_pBMap"] = results[0]
            z1["after_manual_pMap"] = results[1]

            # DIFFERENCE
            z1["before_minus_after_flagstat_pMap"] = z1["after_flagstat_pMap"] - z1["before_flagstat_pMap"]
            z1["before_minus_after_manual_pBMap"]  = z1["after_manual_pBMap"] - z1["before_manual_pBMap"]
            z1["before_minus_after_manual_pMap"]   = z1["after_manual_pMap"] - z1["before_manual_pMap"]
            all_samples.append(z1)
            time.sleep(1)
        # now make a df with all the results
        df = pd.DataFrame(all_samples)
        df.to_csv(output.results_table, sep='\t', index=False)
