"""
This snakefile maps all the HiC reads to a final genome assembly
  - It also makes a pretextmap file for quickly looking at the hic-maps
  - It also calculates the obs/exp matrix, and the pearson exp matrix of that
  - It maps long reads to the assembly then makes a depth histogram that can be
     viewed in HiGlass
"""
from Bio import SeqIO
import gzip
import os
import sys
minchromsize = 1000000
configfile: "config.yaml"
config["tool"] = "GAP_hic_map4"

filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
kmer_position_path=os.path.join(filepath, "../bin/kmer_positions.py")
picard_path=os.path.join(filepath, "../bin/picard.jar")
bedsort_path=os.path.join(filepath, "../dependencies/kentutils/bedSort")
bed2bw_path=os.path.join(filepath,  "../dependencies/kentutils/bedGraphToBigWig")

def rc(seq):
    """
    reverse complement the sequence
    """
    this_complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}
    return "".join(this_complement.get(base, base) for base in reversed(seq))

def nx(seq, n):
    """
    concats the sequence n times
    """
    return "".join([seq]*n)

if "minimap2arg" not in config:
    config["minimap2arg"] = "map-hifi"

###
###  KMER PARSING SECTION
###
# if kmers not specified in config, add
if "kmers" not in config:
    config["kmers"] = {}

# if telomere kmers not specified in the config file, use regular metazoan seq
if "telomere" not in config:
    config["telomere_seqs"] = ["TTAGGG"]
config["telomere_seqs"] = [x.upper() for x in config["telomere_seqs"]]

# make sure that there are no reverse complements in the telomere_seqs
#  we don't want this because we specifically define the revcomp in the
#  next step
for x in config["telomere_seqs"]:
    if rc(x) in config["telomere_seqs"]:
        raise IOError("Don't include the reverse reverse complement of the telomere sequences")
config["telomere_seqs"] = {"{}5x".format(key.upper()):
                           {"f":nx(key.upper(), 5),
                            "r":nx(rc(key.upper()), 5)}
                           for key in config["telomere_seqs"]}
## now add tracks for contiguous stretches of two, three
#for n in [2]:
#    config["telomere_seqs"].update(
#        {"{}{}x".format(key.upper(), n):
#                  {"f":nx(key.upper(), n),
#                   "r":nx(rc(key.upper()), n)}
#                  for key in config["telomere_seqs"]})

print(config["telomere_seqs"])

# make sure all the kmers are uppercase and that the set is complete
for key in config["kmers"]:
    config["kmers"][key] = list(set([x.upper() for x in config["kmers"][key]] + \
                           [reverse_complement(x.upper()) for x in config["kmers"][key]]))
print(config["kmers"])

# make this dummy LR fastq file in case we don't actually want to map any reads
if "LR" not in config:
    config["LR"] = ["{}/temp_dont_delete_me.fastq.gz".format(os.path.join(os.getcwd(),config["tool"]))]
    if not os.path.exists(config["LR"][0]):
        content = b""
        f = gzip.open(config["LR"][0], 'wb')
        f.write(content)
        f.close()

# make this dummy transcript file in case we don't actually want to map any reads
if "transcripts" not in config:
    config["transcripts"] = ["{}/temp_dont_delete_me.fastq.gz".format(os.path.join(os.getcwd(),config["tool"]))]
    if not os.path.exists(config["transcript"][0]):
        content = b""
        f = gzip.open(config["LR"][0], 'wb')
        f.write(content)
        f.close()

# now we check the LR and transcript files to make sure they are fasta or fastq
for thiskey in ["transcripts", "LR"]:
    for entry in config[thiskey]:
        good = False
        for ending in [".fa", ".fa.gz", ".fasta", ".fasta.gz", ".fastq", ".fastq.gz"]:
            if entry.endswith(ending):
                good = True
        if not good:
            raise IOError ("The LR or transcripts file {} must end with .fa, .fa.gz, .fasta, .fasta.gz, .fastq, or .fastq.gz.".format(entry))

def get_chromosome_sizes(assembly_file, minsize):
    """
    returns a set of chromosomes to keep
    """
    chroms = []
    with open(assembly_file) as handle:
        for record in SeqIO.parse(handle, "fasta"):
            if len(record.seq) >= minsize:
                chroms.append(record.id)
    return chroms

wildcard_constraints:
    datatype="[A-Za-z0-9]+",
    kmer="[A-Za-z0-9]+",
    nom="[A-Za-z0-9.]+",
    telo="[A-Za-z0-9]+",
    telodir="[fr]"

rule all:
    input:
        #stats - this is in the pair processing step. fine with HiC-v2
        expand(config["tool"] + "/output/pairsam_stats/{nom}_parsed.pairsam.stats",
               nom = config["assemblies"]),
        expand(config["tool"] + "/output/pairsam_stats/{nom}_parsed_sorted_dedupe.pairsam.stats",
               nom = config["assemblies"]),
        expand(config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.covstats",
               nom = config["assemblies"]),

        # now make a hic map
        expand(config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.gi.tsv",
               nom = config["assemblies"], binsize=config["binsize"]),
        expand(config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.best_connection.tsv",
               nom = config["assemblies"], binsize=config["binsize"]),

        #final files
        # chromsize
        expand(config["tool"] + "/output/{nom}/{nom}_chromsize.txt",
               nom = config["assemblies"]),
        # gaps
        expand(config["tool"] + "/output/{nom}/{nom}_gaps.bed",
               nom = config["assemblies"]),
        expand(config["tool"] + "/output/{nom}/{nom}_gaps.beddb",
               nom = config["assemblies"]),
        # get the diagnostic png before doing anything else
        expand(config["tool"] + "/output/{nom}/{nom}.{binsize}.cool.png",
               nom = config["assemblies"], binsize=config["binsize"]),
        # balanced cool
        expand(config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool",
               nom = config["assemblies"], binsize=config["binsize"]),
        # mcool from balanced
        expand(config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.mcool",
               nom = config["assemblies"], binsize=config["binsize"]),
        ## pretext view
        expand(config["tool"] + "/output/{nom}/{nom}.pretext", nom = config["assemblies"]),
        # plots of AB compartmentalization
        expand(config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.plot.pdf",
               nom = config["assemblies"],
               binsize = [x for x in config["binsize"] if int(x) >= 50000]),

        # read_depth_positions
        #expand(config["tool"] + "/output/{nom}/{nom}_kmer_{kmer}.bw",
        #       kmer = config["kmers"], nom = config["assemblies"]),
        #expand(config["tool"] + "/output/{nom}/{nom}_kmerInLR_{kmer}.bw",
        #       kmer = config["kmers"], nom = config["assemblies"]),
        #expand(config["tool"] + "/output/{nom}/{nom}_kmerInSR_{kmer}.bw",
        #       kmer = config["kmers"], nom = config["assemblies"]),

        expand(config["tool"] + "/output/{nom}/{nom}_{datatype}.bw",
               datatype = ["SRdepth", "TXdepth", "LRdepth"],
               nom = config["assemblies"]),


        # LR telomere bams
        #expand(config["tool"] + "/input/telomere_reads/{telo}_{telodir}_LR_readlist.txt",
        #       telo = config["telomere_seqs"], telodir = ["f", "r"]),
        #expand(config["tool"] + "/input/telo_reads/{LR}_to_{nom}_{telo}_{telodir}.bam",
        #       LR = [os.path.basename(x) for x in config["LR"]],
        #       telo = config["telomere_seqs"], telodir = ["f", "r"],
        #       nom = config["assemblies"]),
        expand(config["tool"] + "/output/{nom}/{nom}_telomereLRDepth_{telo}.bw.mv5",
               telo = config["telomere_seqs"], nom = config["assemblies"]),
        expand(config["tool"] + "/output/{nom}/{nom}_telomereSRDepth_{telo}.bw.mv5",
               telo = config["telomere_seqs"], nom = config["assemblies"]),
        expand(config["tool"] + "/output/{nom}/{nom}_telomereKmerDepth_{telo}.bw.mv5",
               telo = config["telomere_seqs"], nom = config["assemblies"]),

        ## output interaction matrix used for running the Lorentz 3DGR problem
        #expand(config["tool"] + "/output/{nom}/3DGR/{nom}_balanced.{binsize}.if.tsv",
        #       nom = config["assemblies"], binsize=config["binsize"]),

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

rule chrom_size:
    """
    make a file with the chromosome sizes
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    threads: 1
    shell:
        """
        bioawk -cfastx '{{printf("%s\\t%d\\n", $name, length($seq))}}' {input.assem} > {output.cs}
        """

rule index_ref:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        amb   = config["tool"] + "/input/assembly/{nom}_input.fasta.amb",
        ann   = config["tool"] + "/input/assembly/{nom}_input.fasta.ann",
        bwt   = config["tool"] + "/input/assembly/{nom}_input.fasta.bwt",
        pac   = config["tool"] + "/input/assembly/{nom}_input.fasta.pac",
    shell:
        """
        bwa index {input.assem}
        """

rule genome_bed:
    """
    just a bed file of the whole genome assembly to usebedtools subtract
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        bed = config["tool"] + "/output/{nom}/{nom}.bed"
    threads: 1
    shell:
        """
        bioawk -cfastx '{{printf("%s\\t0\\t%d\\n", $name, length($seq))}}' {input.assem} | \
          bedtools sort > {output.bed}
        """

rule telomere_kmer_positions_bigwig:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        kmer_counter = kmer_position_path,
        bed = config["tool"] + "/output/{nom}/{nom}.bed",
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        temppos   = temp(config["tool"] + "/output/depth/TEMP_{nom}_teloKmer_{telo}_{telodir}.nozero.bg"),
        tempzeros = temp(config["tool"] + "/output/depth/TEMP_{nom}_teloKmer_{telo}_{telodir}.zero.bg"),
        bg    = temp(config["tool"] + "/output/depth/{nom}_teloKmer_{telo}_{telodir}.bg"),
        depth = temp(config["tool"] + "/output/depth/{nom}_teloKmer_{telo}_{telodir}.s.bg"),
        bw    =      config["tool"] + "/output/depth/{nom}_teloKmer_{telo}_{telodir}.bw"
    params:
        telokmer = lambda wildcards: config["telomere_seqs"][wildcards.telo][wildcards.telodir],
        k = lambda wildcards: len(config["telomere_seqs"][wildcards.telo][wildcards.telodir])
    threads: 1
    shell:
        """
        # get the kmer positions
        cat {input.assem} | \
          python {input.kmer_counter} {params.k} {params.telokmer} | \
          bedtools merge | \
          awk '{{if (($3-$2)/{params.k} > 1) {{ \
          printf("%s\\t%d\\t%d\\t%d\\n", $1, $2, $3, ($3-$2)/{params.k} )}}}}' | \
          bedtools sort > {output.temppos}

        # get subtract for the zero positions
        bedtools subtract -a {input.bed} -b {output.temppos} | \
          awk '{{printf("%s\\t0\\n", $0)}}' > {output.tempzeros}

        # now cat and sort
        cat {output.temppos} {output.tempzeros} | \
          sort -k1,1 -k2,2n -k3,3n > {output.bg}

        # now do the steps to make a bigwig
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {output.bg} {output.depth}
        {input.bed2bw} {output.depth} {input.cs} {output.bw}
        """

rule telomere_kmer_bigwig_to_multivec:
    """
    make a multivec from multiple bigwig files based on working multivec spec
      - https://paper.dropbox.com/doc/Multivec-Spec-3IelZjzjXDo7mGy3SkGUF

    This multivec file is from looking at the kmers present in the genome

    #--row-infos-filename {input.rows} \
    """
    input:
        bw_f = config["tool"] + "/output/depth/{nom}_teloKmer_{telo}_f.bw",
        bw_r = config["tool"] + "/output/depth/{nom}_teloKmer_{telo}_r.bw",
        #rows = config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}_rows.txt",
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        multivec = config["tool"] + "/output/{nom}/{nom}_telomereKmerDepth_{telo}.bw.mv5"
    threads: 1
    shell:
        """
        clodius convert bigwigs-to-multivec {input.bw_f} {input.bw_r} \
        --chromsizes-filename {input.cs} \
        --output-file {output.multivec}
        """

rule kmer_positions_bigwig:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        kmer_counter = kmer_position_path,
        bed = config["tool"] + "/output/{nom}/{nom}.bed",
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        temppos   = temp(config["tool"] + "/output/depth/TEMP_{nom}_kmer_{kmer}.nozero.bg"),
        tempzeros = temp(config["tool"] + "/output/depth/TEMP_{nom}_kmer_{kmer}.zero.bg"),
        bg    =      config["tool"] + "/output/depth/{nom}_kmer_{kmer}.bg",
        depth = temp(config["tool"] + "/output/depth/{nom}_kmer_{kmer}.s.bg"),
        bw    =      config["tool"] + "/output/{nom}/{nom}_kmer_{kmer}.bw"
    params:
        tkmers = lambda wildcards: " ".join(config["kmers"][wildcards.kmer]),
        k = lambda wildcards: int(list(set([len(x) for x in config["kmers"][wildcards.kmer]]))[0])
    threads: 1
    shell:
        """
        # get the kmer positions
        cat {input.assem} | \
          python {input.kmer_counter} {params.k} {params.tkmers} | \
          bedtools merge | \
          awk '{{if (($3-$2)/{params.k} > 1) {{ \
          printf("%s\\t%d\\t%d\\t%d\\n", $1, $2, $3, ($3-$2)/{params.k} )}}}}' | \
          bedtools sort > {output.temppos}

        # get subtract for the zero positions
        bedtools subtract -a {input.bed} -b {output.temppos} | \
          awk '{{printf("%s\\t0\\n", $0)}}' > {output.tempzeros}

        # now cat and sort
        cat {output.temppos} {output.tempzeros} | \
          sort -k1,1 -k2,2n -k3,3n > {output.bg}

        # now do the steps to make a bigwig
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {output.bg} {output.depth}
        {input.bed2bw} {output.depth} {input.cs} {output.bw}
        """

rule telomere_lookup_longreads:
    """
    look up the reads with telomere sequences in a specific direction
    """
    input:
        long_reads = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        read_list = config["tool"] + "/input/telomere_reads/{telo}_{telodir}_LR_readlist.txt"
    threads: 1
    params:
        grep_string = lambda wildcards: config["telomere_seqs"][wildcards.telo][wildcards.telodir]
    shell:
        """
        zcat -f {input.long_reads} | \
          bioawk -cfastx '{{print($name, $seq)}}' | \
          grep '{params.grep_string}' | \
          awk '{{print($1)}}' > {output.read_list} || true
        """

rule filter_telo_bams_longreads:
    """
    we have a list of reads that have the telomere seqs we want, so
     pull them out with picard.
    """
    input:
        read_list = config["tool"] + "/input/telomere_reads/{telo}_{telodir}_LR_readlist.txt",
        bam = config["tool"] + "/input/bams_LR/LR_{LR}_to_{nom}.bam",
        picard = picard_path
    output:
        bam = config["tool"] + "/input/telo_reads/{LR}_to_{nom}_{telo}_{telodir}.bam"
    threads: 1
    shell:
        """
        {{ # try
           java -jar {input.picard} FilterSamReads \
            I={input.bam} \
            O={output.bam} \
            READ_LIST_FILE={input.read_list} \
            FILTER=includeReadList
        }} || {{ # catch
            # save log for exception
            samtools view -Hb {input.bam} > {output.bam}
        }}
        """

rule filter_telodir_mapdir:
    """
    filter the bams so that we know the mapping direction
    """
    input:
        bam = config["tool"] + "/input/telo_reads/{LR}_to_{nom}_{telo}_{telodir}.bam"
    output:
        fbam = config["tool"] + "/input/telo_reads/{LR}_to_{nom}_{telo}_{telodir}_f.bam",
        rbam = config["tool"] + "/input/telo_reads/{LR}_to_{nom}_{telo}_{telodir}_r.bam"
    threads: 1
    shell:
        """
        samtools view -hb -F 20 {input.bam} | \
          samtools sort - > {output.fbam}
        samtools view -hb -F 4 -f 16 {input.bam} | \
          samtools sort - > {output.rbam}
        """

rule bedgraph_of_telomeres:
    input:
        fbam = expand(config["tool"] + "/input/telo_reads/{LR}_to_{{nom}}_{{telo}}_f_f.bam",
                       LR = [os.path.basename(x) for x in config["LR"]]) + \
               expand(config["tool"] + "/input/telo_reads/{LR}_to_{{nom}}_{{telo}}_r_r.bam",
                       LR = [os.path.basename(x) for x in config["LR"]]),
        rbam = expand(config["tool"] + "/input/telo_reads/{LR}_to_{{nom}}_{{telo}}_f_r.bam",
                       LR = [os.path.basename(x) for x in config["LR"]]) + \
               expand(config["tool"] + "/input/telo_reads/{LR}_to_{{nom}}_{{telo}}_r_f.bam",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        bg_f  =      config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_f.bg",
        bg_r  =      config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_r.bg",
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.fbam} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.bg_f}
        samtools cat {input.rbam} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.bg_r}
        """

rule bigwig_of_telomeres:
    """
    just make bigwigs of these files
    """
    input:
        bg = config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_{telodir}.bg",
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        bg_s =      config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_{telodir}.s.bg",
        bw   =      config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_{telodir}.bw",
    threads: 1
    shell:
        """
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {input.bg} {output.bg_s}
        {input.bed2bw} {output.bg_s} {input.cs} {output.bw}
        """

rule telomere_bigwig_to_multivec:
    """
    make a multivec from multiple bigwig files based on working multivec spec
      - https://paper.dropbox.com/doc/Multivec-Spec-3IelZjzjXDo7mGy3SkGUF

    #--row-infos-filename {input.rows} \
    """
    input:
        bw_f = config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_f.bw",
        bw_r = config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_r.bw",
        #rows = config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}_rows.txt",
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        multivec = config["tool"] + "/output/{nom}/{nom}_telomereLRDepth_{telo}.bw.mv5"
    threads: 1
    shell:
        """
        clodius convert bigwigs-to-multivec {input.bw_f} {input.bw_r} \
        --chromsizes-filename {input.cs} \
        --output-file {output.multivec}
        """

# candidate for deletion - we no longer need this as we use clodius to join two bigwigs
#rule telomere_multivec:
#    """
#    makes a multivec file in the format needed for higlass
#    """
#    input:
#        bg_f  =   config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_f.bg",
#        bg_r  =   config["tool"] + "/output/depth/{nom}_telomereLRDepth_{telo}_r.bg",
#    output:
#        mv    =   config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}.bg",
#        rows  =   config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}_rows.txt",
#    params:
#        forward = lambda wildcards: config["telomere_seqs"][wildcards.telo]["f"], 
#        reverse = lambda wildcards: config["telomere_seqs"][wildcards.telo]["r"] 
#    threads: 1
#    shell:
#        """
#        bedtools unionbedg -i {input.bg_f} {input.bg_r} >> {output.mv}
#        echo -e '{params.forward}\n{params.reverse}' > {output.rows}
#        """

# candidate for deletion
#rule aggregate_telomere_multivec:
#    """
#    #--row-infos-filename {input.rows} \
#    """
#    input:
#        mv    =    config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}.bg",
#        #rows  =    config["tool"] + "/output/depth/multivec/{nom}_telomereLRDepth_{telo}_rows.txt",
#        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
#    output:
#        multivec = config["tool"] + "/output/{nom}/{nom}_telomereLRDepth_{telo}.mv5"
#    threads: 1
#    shell:
#        """
#        clodius convert bedfile-to-multivec \
#          {input.mv} \
#          --chromsizes-filename {input.cs} \
#          --starting-resolution 1 \
#          --output-file {output.multivec}
#        """

rule shortread_telomere_kmers_bws:
    """
    grep the bam files to get the read depths from telomere kmers
    """
    input:
        bams = expand(config["tool"] + "/input/bams/{{nom}}_{lib}_to_ref.sorted.bam", lib=config["libs"]),
        picard = picard_path,
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        bg   = config["tool"] + "/input/telo_SRs/SR_to_{nom}_{telo}_{telodir}.bg",
        bg_s = temp(config["tool"] + "/input/telo_SRs/SR_to_{nom}_{telo}_{telodir}.s.bg"),
        bw   = config["tool"] + "/input/telo_SRs/SR_to_{nom}_{telo}_{telodir}.bw",
    params:
        telokmer = lambda wildcards: config["telomere_seqs"][wildcards.telo][wildcards.telodir]
    threads: max(1, int(workflow.cores/4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools view -h - | \
          grep '@\\|{params.telokmer}' | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.bg}
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {output.bg} {output.bg_s}
        {input.bed2bw} {output.bg_s} {input.cs} {output.bw}
        """

rule shortread_telomere_bws_to_multivec:
    """
    make a multivec from multiple bigwig files based on working multivec spec
      - https://paper.dropbox.com/doc/Multivec-Spec-3IelZjzjXDo7mGy3SkGUF

    #--row-infos-filename {input.rows} \
    """
    input:
        bw_f = config["tool"] + "/input/telo_SRs/SR_to_{nom}_{telo}_f.bw",
        bw_r = config["tool"] + "/input/telo_SRs/SR_to_{nom}_{telo}_r.bw",
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        multivec = config["tool"] + "/output/{nom}/{nom}_telomereSRDepth_{telo}.bw.mv5"
    threads: 1
    shell:
        """
        clodius convert bigwigs-to-multivec {input.bw_f} {input.bw_r} \
        --chromsizes-filename {input.cs} \
        --output-file {output.multivec}
        """

rule kmer_lookup_longreads:
    input:
        long_reads = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        read_list = config["tool"] + "/input/kmer_reads/{kmer}_LR_readlist.txt"
    threads: 1
    params:
        grep_string = lambda wildcards: "\|".join(["".join([x]*5) for x in config["kmers"][wildcards.kmer]])
    shell:
        """
        zcat -f {input.long_reads} | \
          bioawk -cfastx '{{print($name, $seq)}}' | \
          grep '{params.grep_string}' | \
          cut -f1 > {output.read_list}
        """

rule filter_kmer_bams_longreads:
    """
    we have a list of reads that have the kmers we want, so
     pull them out with picard.
    """
    input:
        read_list = config["tool"] + "/input/kmer_reads/{kmer}_LR_readlist.txt",
        bam = config["tool"] + "/input/bams_LR/LR_{LR}_to_{nom}.bam",
        picard = picard_path
    output:
        bam = config["tool"] + "/input/kmer_reads/{LR}_to_{nom}_{kmer}.bam"
    threads: 1
    shell:
        """
        {{ # try
           java -jar {input.picard} FilterSamReads \
            I={input.bam} \
            O={output.bam} \
            READ_LIST_FILE={input.read_list} \
            FILTER=includeReadList
        }} || {{ # catch
            # save log for exception
            samtools view -Hb {input.bam} > {output.bam}
        }}
        """

rule get_coverage_of_bams_kmer_positions:
    input:
        bams = expand(config["tool"] + "/input/kmer_reads/{LR}_to_{{nom}}_{{kmer}}.bam",
                       LR = [os.path.basename(x) for x in config["LR"]]),
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        bg  =      config["tool"] + "/output/depth/{nom}_kmerInLR_{kmer}.bg",
        bgs = temp(config["tool"] + "/output/depth/{nom}_kmerInLR_{kmer}.s.bg"),
        bw    =    config["tool"] + "/output/{nom}/{nom}_kmerInLR_{kmer}.bw"
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.bg}
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {output.bg} {output.bgs}
        {input.bed2bw} {output.bgs} {input.cs} {output.bw}
        """

rule kmer_lookup_shortreads:
    input:
        left  = [config["libs"][x]["read1"] for x in config["libs"]],
        right = [config["libs"][x]["read2"] for x in config["libs"]]
    output:
        read_list = config["tool"] + "/input/kmer_reads/{kmer}_SR_readlist.txt"
    threads: 1
    params:
        grep_string = lambda wildcards: "\|".join(["".join([x]*5) for x in config["kmers"][wildcards.kmer]])
    shell:
        """
        zcat -f {input.left} {input.right} | \
          bioawk -cfastx '{{print($name, $seq)}}' | \
          grep '{params.grep_string}' | \
          cut -f1 > {output.read_list}
        """

rule filter_kmer_bams_shortreads:
    """
    We have a list of reads that have the kmers we want, so
     pull them out with picard.
    """
    input:
        read_list = config["tool"] + "/input/kmer_reads/{kmer}_SR_readlist.txt",
        bam = config["tool"] + "/input/bams/{nom}_{lib}_to_ref.sorted.bam",
        picard = picard_path
    output:
        bam = config["tool"] + "/input/kmer_reads_short/{lib}_to_{nom}_{kmer}.bam"
    threads: 1
    shell:
        """
        {{ # try
           java -jar {input.picard} FilterSamReads \
            I={input.bam} \
            O={output.bam} \
            READ_LIST_FILE={input.read_list} \
            FILTER=includeReadList
        }} || {{ # catch
            # save log for exception
            samtools view -Hb {input.bam} > {output.bam}
        }}
        """

rule get_coverage_of_bams_kmer_positions_shortreads:
    input:
        bams = expand(config["tool"] + "/input/kmer_reads_short/{lib}_to_{{nom}}_{{kmer}}.bam",
                            lib=config["libs"]),
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        bg  =      config["tool"] + "/output/depth/{nom}_kmerInSR_{kmer}.bg",
        bgs = temp(config["tool"] + "/output/depth/{nom}_kmerInSR_{kmer}.s.bg"),
        bw    =    config["tool"] + "/output/{nom}/{nom}_kmerInSR_{kmer}.bw"
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.bg}
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {output.bg} {output.bgs}
        {input.bed2bw} {output.bgs} {input.cs} {output.bw}
        """

rule bedgraph_and_bed2bigwig:
    input:
        depth   = config["tool"] + "/output/depth/{nom}_{datatype}.bg",
        bedsort = bedsort_path,
        bed2bw = bed2bw_path,
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        depth = temp(config["tool"] + "/output/depth/{nom}_{datatype}.s.bg"),
        bw    = config["tool"] + "/output/{nom}/{nom}_{datatype}.bw"
    threads: 1
    shell:
        """
        chmod u+x {input.bedsort}
        chmod u+x {input.bed2bw}
        {input.bedsort} {input.depth} {output.depth}
        {input.bed2bw} {output.depth} {input.cs} {output.bw}
        """

rule gaps_from_assembly:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
    output:
        gapbed = config["tool"] + "/output/{nom}/{nom}_gaps.bed"
    threads: 1
    run:
        # this block of code from https://www.biostars.org/p/133742/
        #import the SeqIO module from Biopython
        outhandle = open(output.gapbed, "w")
        with open(input.assem, mode="r") as fasta_handle:
            for record in SeqIO.parse(fasta_handle, "fasta"):
                start_pos=0
                counter=0
                gap=False
                gap_length = 0
                for char in record.seq:
                    if char == 'N':
                        if gap_length == 0:
                            start_pos=counter
                            gap_length = 1
                            gap = True
                        else:
                            gap_length += 1
                    else:
                        if gap:
                            print("{} {} {}".format(
                                record.id,
                                start_pos,
                                start_pos + gap_length),
                                  file = outhandle)
                            gap_length = 0
                            gap = False
                    counter += 1
        outhandle.close()

rule make_beddb:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        gapbed = config["tool"] + "/output/{nom}/{nom}_gaps.bed",
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        beddb = config["tool"] + "/output/{nom}/{nom}_gaps.beddb"
    threads: 1
    shell:
        """
        clodius aggregate bedfile \
          --chromsizes-filename {input.cs} \
          --delimiter " " {input.gapbed} \
          --assembly {input.assem} \
          --output-file {output.beddb}
        """

rule symlink_the_transcripts:
    """
    Symlinks the transcripts so they are easier to work with in the rest of the
      pipeline.
    """
    input:
        LR = config["transcripts"]
    output:
        assem = expand(config["tool"] + "/input/transcripts/{tx}",
                       tx = [os.path.basename(x) for x in config["transcripts"]])
    params:
        fileprefix = config["tool"] + "/input/transcripts/"
    threads: 1
    run:
        for thisfile in config["transcripts"]:
            fname = os.path.basename(thisfile)
            dest = "{}{}".format(params.fileprefix, fname)
            os.symlink(thisfile, dest)

rule map_transcripts_to_genome:
    """
    maps the reads to the assembly
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        transcripts = config["tool"] + "/input/transcripts/{tx}"
    output:
        bam = config["tool"] + "/input/bams_transcripts/LR_{tx}_to_{nom}.bam"
    params:
        tx_name = lambda wildcards: wildcards.tx
    threads: workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} -ax splice:hq {input.assem} \
            --split-prefix {params.tx_name} \
            {input.transcripts} | \
          samtools view -hb - | \
          samtools sort - > {output.bam}
        """

rule index_transcripts_bams:
    input:
        bam = config["tool"] + "/input/bams_transcripts/LR_{tx}_to_{nom}.bam"
    output:
        bai = config["tool"] + "/input/bams_transcripts/LR_{tx}_to_{nom}.bam.bai"
    threads: 1
    shell:
        """
        samtools index {input.bam}
        """

rule get_coverage_of_transcripts_bams:
    input:
        bams = expand(config["tool"] + "/input/bams_transcripts/LR_{tx}_to_{{nom}}.bam",
                       tx = [os.path.basename(x) for x in config["transcripts"]]),
        bais = expand(config["tool"] + "/input/bams_transcripts/LR_{tx}_to_{{nom}}.bam.bai",
                       tx = [os.path.basename(x) for x in config["transcripts"]])
    output:
        depth = config["tool"] + "/output/depth/{nom}_TXdepth.bg"
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.depth}
        """

rule symlink_the_LRs:
    """
    Symlinks the long reads so they are easier to work with in the rest of the
      pipeline.
    """
    input:
        LR = config["LR"]
    output:
        assem = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    params:
        fileprefix = config["tool"] + "/input/longreads/"
    run:
        for thisfile in config["LR"]:
            fname = os.path.basename(thisfile)
            dest = "{}{}".format(params.fileprefix, fname)
            os.symlink(thisfile, dest)

rule map_LR_to_genome:
    """
    maps the reads to the assembly
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        long_reads = config["tool"] + "/input/longreads/{LR}"
    output:
        bam = config["tool"] + "/input/bams_LR/LR_{LR}_to_{nom}.bam"
    params:
        minimaparg = config["minimap2arg"],
        LR_name = lambda wildcards: wildcards.LR
    threads: workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} -ax {params.minimaparg} {input.assem} \
            --split-prefix {params.LR_name} {input.long_reads} | \
          samtools view -hb - | \
          samtools sort - > {output.bam}
        """

rule index_LR_bams:
    input:
        bam = config["tool"] + "/input/bams_LR/LR_{LR}_to_{nom}.bam"
    output:
        bai = config["tool"] + "/input/bams_LR/LR_{LR}_to_{nom}.bam.bai"
    threads: 1
    shell:
        """
        samtools index {input.bam}
        """

rule get_coverage_of_LR_bams:
    input:
        bams = expand(config["tool"] + "/input/bams_LR/LR_{LR}_to_{{nom}}.bam",
                       LR = [os.path.basename(x) for x in config["LR"]]),
        bais = expand(config["tool"] + "/input/bams_LR/LR_{LR}_to_{{nom}}.bam.bai",
                       LR = [os.path.basename(x) for x in config["LR"]]),
    output:
        depth = config["tool"] + "/output/depth/{nom}_LRdepth.bg"
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.depth}
        """

rule get_coverage_of_SR_bams:
    input:
        bams  = expand(config["tool"] + "/input/bams/{{nom}}_{lib}_to_ref.sorted.bam",
                       lib=config["libs"])
    output:
        depth = config["tool"] + "/output/depth/{nom}_SRdepth.bg"
    threads: max(4, int(workflow.cores /4))
    shell:
        """
        samtools cat {input.bams} | \
          samtools sort -@ {threads} - | \
          bedtools genomecov -ibam stdin -bga -split > {output.depth}
        """

rule map_hic_to_ref:
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        amb   = config["tool"] + "/input/assembly/{nom}_input.fasta.amb",
        ann   = config["tool"] + "/input/assembly/{nom}_input.fasta.ann",
        bwt   = config["tool"] + "/input/assembly/{nom}_input.fasta.bwt",
        pac   = config["tool"] + "/input/assembly/{nom}_input.fasta.pac",
        left  = lambda wildcards: config["libs"][wildcards.lib]["read1"],
        right = lambda wildcards: config["libs"][wildcards.lib]["read2"]
    output:
        bam   = config["tool"] + "/input/bams/{nom}_{lib}_to_ref.sorted.bam"
    threads: workflow.cores - 1
    shell:
        """
        # DONT PIPE INTO SAMTOOLS SORT
        bwa mem -5SPM -t {threads} {input.assem} {input.left} {input.right} | \
            samtools view -hb -@ {threads} - > {output.bam}
        # DO NOT PIPE INTO SAMTOOLS SORT
        """

rule hic_make_pairsam:
    """
    # BAM to pairs: annotate reads, create a pairsam file
    """
    input:
        bam   = expand(config["tool"] + "/input/bams/{{nom}}_{lib}_to_ref.sorted.bam", lib=config["libs"]),
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        ps = config["tool"] + "/output/temp/pairsam/{nom}_parsed.pairsam.gz",
        stats = config["tool"] + "/output/pairsam_stats/{nom}_parsed.pairsam.stats"
    threads: 1
    shell:
        """
        NUMBAM=$(echo "{input.bam}" | sed 's/ /\\n/g' | wc -l)
        if [ ${{NUMBAM}} -gt 1 ]
          then
            samtools cat -h {input.bam} | samtools view -h - | \
              pairtools parse --output-stats {output.stats} \
              -c {input.cs} -o {output.ps}
          else
            samtools view -h {input.bam} | \
              pairtools parse --output-stats {output.stats} \
              -c {input.cs} -o {output.ps}
        fi
        """

rule hic_sort_pairsam:
    """
    Sorting pairs by chromosome1-chromosome2-position1-position2
    <n_proc> = number of processors to be used
    """
    input:
        ps = config["tool"] + "/output/temp/pairsam/{nom}_parsed.pairsam.gz"
    output:
        pps = config["tool"] + "/output/temp/sorted/{nom}_parsed_sorted.pairsam.gz"
    threads:
        workflow.cores - 1
    shell:
        """
        pairtools sort --nproc {threads} -o {output.pps} {input.ps} --tmpdir=./
        """

rule hic_dedup:
    """
    # NOTE: this step needs to write a lot of tmp files,
      and by default your tmp directory is in your home directory,
      which can fill up. To avoid that, do this before running the sort command:

     # Marking duplicates
     # <n_proc> = number of processors to be used
    """
    input:
        pps = config["tool"] + "/output/temp/sorted/{nom}_parsed_sorted.pairsam.gz"
    output:
        dpps = config["tool"] + "/output/temp/dedupe/{nom}_parsed_sorted_dedupe.pairsam.gz",
        stats = config["tool"] + "/output/pairsam_stats/{nom}_parsed_sorted_dedupe.pairsam.stats"
    threads: workflow.cores - 1
    params:
        nom = lambda w: w.nom
    shell:
        """
        mkdir -p tempdir{params.nom}
        export TMPDIR=tempdir{params.nom}
        pairtools dedup --nproc-in {threads} \
          --output-stats {output.stats} \
          --mark-dups -o {output.dpps} {input.pps}
        rm -rf tempdir{params.nom}
        """

rule hic_filter:
    input:
        dpps = config["tool"] + "/output/temp/dedupe/{nom}_parsed_sorted_dedupe.pairsam.gz",
    output:
        filt = config["tool"] + "/output/temp/filt/{nom}_parsed_sorted_dedupe_filt.pairsam.gz"
    threads: 1
    shell:
        """
        pairtools select  '(pair_type == "UU") or (pair_type == "UR") or (pair_type == "RU")' \
          -o {output.filt} {input.dpps}
        """

rule generate_final_pairs:
    """
    generate the final output from a pairsam to a pairs file
    """
    input:
        filt = config["tool"] + "/output/temp/filt/{nom}_parsed_sorted_dedupe_filt.pairsam.gz"
    output:
        final = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz"
    threads: 1
    shell:
        """
        pairtools split --output-pairs {output.final} {input.filt}
        """

rule generate_coverage_stats_final_pairs:
    """
    Look at the pairs file to determine to interchromosomal connectivity.
    """
    input:
        final = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz",
        chromsize = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        stats = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.covstats"
    threads: 1
    run:
        scaf_to_size = {}
        # first get the chromosome sizes
        with open(input.chromsize, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    splitd = line.split("\t")
                    scaf_to_size[splitd[0]] = int(splitd[1])
        # now parse the pairs file
        interchrom_counter = {key: 0 for key in scaf_to_size}
        intrachrom_counter = {key: 0 for key in scaf_to_size}
        import gzip
        with gzip.open(input.final, "rb") as f:
            for line in f:
                line = line.decode("utf-8").strip()
                if line and not line.startswith("#"):
                    chrom1 = line.split("\t")[1]
                    chrom2 = line.split("\t")[3]
                    if chrom1 == chrom2:
                        intrachrom_counter[chrom1] += 1
                    else:
                        interchrom_counter[chrom1] += 1
                        interchrom_counter[chrom2] += 1
        # now print the information about each scaffold
        # fields are
        # 1 - scaffold
        # 2 - intrachrom pairs count
        # 3 - interchrom pairs count
        # 4 - scaf length
        # 5 - length of all other scafs
        # 6 - intrachrom divided by scaf length
        # 7 - interchrom / ( scaf len * len of other scafs)
        with open(output.stats, "w") as f:
            for key in scaf_to_size:
                others_size = sum([scaf_to_size[x] for x in scaf_to_size if x != key])
                outstr  = key + "\t"                      # 1 - scaffold
                outstr += str(intrachrom_counter[key]) + "\t"  # 2 - intrachrom pairs count
                outstr += str(interchrom_counter[key]) + "\t"  # 3 - interchrom counter
                outstr += str(scaf_to_size[key]) + "\t"        # 4 - scaf length
                outstr += str(others_size) + "\t"              # 5 - length of all other scafs
                outstr += str(intrachrom_counter[key] / scaf_to_size[key]) + "\t" # 6 - intrachrom divided by scaf length
                outstr += "{:.20f}".format(interchrom_counter[key] / (scaf_to_size[key] * others_size))  # 7 - interchrom divided by (scaf len * len of other scafs)
                print(outstr, file = f)

rule index_pairs:
    """
    indexing output. the *.pairs.gz suffix is important to parse the file correctly!
    """
    input:
        pairs = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz"
    output:
        index = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz.px2"
    threads: 1
    shell:
        """
        pairix -f {input.pairs}
        """

rule make_bins_individual:
    """
    This makes the matrix and bins everything
    i.e. binsize: 5000 (high resolution), 500000 (lower resolution)
    """
    input:
        final = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz",
        index = config["tool"] + "/output/pairs/{nom}/{nom}.dedup.filt.pairs.gz.px2",
        cs = config["tool"] + "/output/{nom}/{nom}_chromsize.txt"
    output:
        cool = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool"
    threads: 1
    params:
        bs = lambda wildcards: wildcards.binsize,
        nom = lambda wildcards: wildcards.nom,
        outname = lambda wildcards: "{tool}/output/{nom}/{nom}.{binsize}.cool".format(
            tool=config["tool"], nom = wildcards.nom, binsize = wildcards.binsize )
    shell:
        """
        cooler cload pairix {input.cs}:{params.bs} {input.final} {params.outname}
        """

rule diagnostic_plot:
    """
    This generates a diagnostic plot of the bin distribution to pick the z-score cutoffs
    """
    input:
        cool = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool"
    output:
        png = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool.png"
    shell:
        """
        hicCorrectMatrix diagnostic_plot -m {input.cool} -o {output.png}
        """

rule cutoff_matrix:
    """
    Balances the matrix using the z-score cutoffs. Removes bins that are too dense or sparse
    """
    input:
        png = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool.png",
        cool = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool"
    output:
        balanced = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool"
    params:
        zmin = lambda wildcards: config["binsize"][int(wildcards.binsize)]["zmin"],
        zmax = lambda wildcards: config["binsize"][int(wildcards.binsize)]["zmax"],
        binsize = lambda wildcards: wildcards.binsize
    threads: 1
    shell:
        """
        if [ "{params.zmin}" -eq "0" ]; then
            echo "Set the {params.binsize} zmin to something other than 0";
            exit;
        fi
        if [ "{params.zmax}" -eq "0" ]; then
            echo "Set the {params.binsize} zmax to something other than 0";
            exit;
        fi
        hicCorrectMatrix correct -m {input.cool} \
             --filterThreshold {params.zmin} {params.zmax} \
             -o {output.balanced}
        """

#rule old_balance_matrix:
#    """
#    ## Matrix normalization/ balancing
#    #cooler balance res.<binsize>.cool
#    ## if any problems with creating output add "--force" option
#    # this only works with --cis-only. Otherwise it drops a ton of bins
#    """
#    input:
#        cool = config["tool"] + "/output/{nom}/{nom}.{binsize}.cool"
#    output:
#        cool = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool"
#    shell:
#        """
#        cp {input.cool} {output.cool}
#        cooler balance --force {output.cool}
#        """

rule zoomify_matrix:
    """
    ## Aggregation (for HiGlass view)
      - This generates a *.mcool file from the balanced cool file
    """
    input:
        cool = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool"
    output:
        mcool = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.mcool"
    threads: 1
    shell:
        """
        cooler zoomify -o {output.mcool} {input.cool}
        """

# Now we get the Hi-C interaction matrix
rule make_hic_matrix_simple:
    input:
        cool =   config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool"
    output:
        hicm = config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.gi.tsv"
    params:
        gi = config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.gi"
    threads: 1
    shell:
        """
        hicConvertFormat -m {input.cool} \
           --outFileName {params.gi} \
           --inputFormat cool \
           --outputFormat ginteractions
        """

rule best_connected_scaffold:
    """
    Find which scaffold is the best connected to each.
      - This is most useful for placing scaffolds during manual edits

    COW1    12000   14000   sca1    4000    6000    0.003780422975718399
    COW1    16000   18000   sca1    2000    4000    0.0013141025221723496
    COW1    32000   34000   sca1    0       2000    0.0012358846777278977
    """
    input:
        hicm = config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.gi.tsv",
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        hicm = config["tool"] + "/output/{nom}/text_hic_map/{nom}_balanced.{binsize}.best_connection.tsv"
    threads: 1
    run:
        scaf_sizes = {}
        # first we need to get the scaffold sizes without gaps
        for record in SeqIO.parse(input.assem, "fasta"):
            scaf_sizes[record.id] = len(str(record.seq).replace("N",
                                                "").replace("n", ""))
        contact_dict = {}
        # now we process the table
        with open(input.hicm, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = line.split("\t")
                    scaf1 = fields[0]
                    scaf2 = fields[3]
                    if scaf1 != scaf2:
                        value = float(fields[6])
                        if scaf1 not in contact_dict:
                            contact_dict[scaf1] = {}
                        if scaf2 not in contact_dict:
                            contact_dict[scaf2] = {}

                        if scaf1 not in contact_dict[scaf2]:
                            contact_dict[scaf2][scaf1] = 0
                        if scaf2 not in contact_dict[scaf1]:
                            contact_dict[scaf1][scaf2] = 0
                        contact_dict[scaf1][scaf2] += value
                        contact_dict[scaf2][scaf1] += value

        ## now we correct for basepair area
        #for i in contact_dict:
        #    for j in contact_dict[i]:
        #        bp_i = scaf_sizes[i]
        #        bp_j = scaf_sizes[j]
        #        area = bp_i * bp_j
        #        value = contact_dict[i][j]
        #        #print("{}: {} * {}: {} = {} || {}/{} = {}".format(i, bp_i, j, bp_j, area, value, area, value/area))
        #        contact_dict[i][j] = value/bp_j

        # now we print out the strongest connection
        outhandle = open(output.hicm, "w")
        for this_scaf in scaf_sizes:
            max_val = 0
            max_scaf = "NA"
            #print (this_scaf, contact_dict[this_scaf])
            #print()
            if this_scaf in contact_dict:
                for other_scaf in contact_dict[this_scaf]:
                    thisval = contact_dict[this_scaf][other_scaf]
                    if thisval > max_val:
                        max_val = thisval
                        max_scaf = other_scaf
                print("{}\t{}\t{}".format(this_scaf, max_scaf, max_val),
                                          file = outhandle)
            else:
                print("{}\t{}\t{}".format(this_scaf, "NA", "0"),
                                          file = outhandle)



# Filter out the small chromosomes for further analyses/balancing/et cet
rule only_chromosomes:
    input:
        cool = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool",
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        onlychr = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.cool"
    params:
        mc = minchromsize
        #chr_string = lambda wildcards: ' '.join(get_chromosome_sizes(
        #                      config["tool"] + "/input/assembly/{}_input.fasta".format(wildcards.nom), 
        #                      minchromsize))
    threads: 1
    shell:
        """
        CHROMS=$(bioawk -cfastx '{{if (length($seq) >= {params.mc}) {{printf("%s ", $name)}} }}' {input.assem})
        >&2 echo "keeping ${{CHROMS}}"
        hicAdjustMatrix --chromosomes ${{CHROMS}} \
             -m {input.cool} \
             --outFileName {output.onlychr} \
             --action keep
        """

# Now compute the obs-exp using lieberman-aiden
rule obs_expected_lieb:
    input:
        onlychr = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.cool"
    output:
        obsexp = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.cool"
    threads: 1
    shell:
        """
        hicTransform -m {input.onlychr} \
             --outFileName {output.obsexp} \
             --method obs_exp_lieberman
        """

# Now compute the Pearson matrix of the obs-exp. Plotting this shows A-B compartments
rule pearson_matrix:
    input:
        obsexp = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.cool"
    output:
        pearson = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.cool"
    threads: 1
    shell:
        """
        hicTransform -m {input.obsexp} \
             --outFileName {output.pearson} \
             --method pearson
        """

# Calculates the obs/exp of the matrix
rule PCA_of_matrix:
    """
    this makes a bigwig file of the PCAs of the balanced cooler files
    """
    input:
        onlychr = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.cool"
    output:
        pca1 = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.pca1.bw",
        pca2 = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.pca2.bw"
    threads: 1
    shell:
        """
        hicPCA -m {input.cool} -o {output.pca1} {output.pca2} --format bigwig
        """

rule plot_matrices:
    input:
        pearson = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.cool",
        pca1 = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.pca1.bw"
    output:
        pdf = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.ABplot.pdf"
    threads: 1
    shell:
        """
        hicPlotMatrix --colorMap RdBu -m {input.pearson} --outFileName {output.pdf} --perChr --bigwig {input.pca1}
        """

rule convert_pearson_to_gi:
    """
    converts the pearson correlation matrix to a gi file. This is easier to work with than a cooler file.
    """
    input:
        pearson = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.cool"
    output:
        gi = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.gi.tsv"
    params:
        gi = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.gi"
    threads: 1
    shell:
        """
        hicConvertFormat -m {input.pearson} \
           --outFileName {params.gi} \
           --inputFormat cool \
           --outputFormat ginteractions
        """

rule plot_matrices_custom:
    """
    This plots the AB compartments using a custom script.
    I think the results are much nicer than the package that is included
      in HiCExplorer.
    """
    input:
        gi = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.gi.tsv"
    output:
        pdf = config["tool"] + "/output/{nom}/ABcompartments/{nom}_balanced.{binsize}.onlychr.obsexplieb.pearson.plot.pdf"
    threads: 1
    run:
        import ast
        import pandas as pd
        import seaborn as sns; sns.set()
        import matplotlib
        import matplotlib.pyplot as plt
        import matplotlib.ticker as ticker
        from matplotlib.backends.backend_pdf import PdfPages
        from matplotlib.ticker import StrMethodFormatter, NullFormatter
        import numpy as np

        # set seaborn stuff
        #sns.set(rc={'text.usetex' : True})
        sns.set_style("ticks", {'font.family': ['sans-serif'],
                                    'font.sans-serif': ['Helvetica'],
                                    'grid.color': '.95'})

        # Preserve the vertical order of embedded images:
        matplotlib.rcParams['image.composite_image'] = False
        matplotlib.rcParams['pdf.fonttype'] = 42
        matplotlib.rcParams['ps.fonttype'] = 42

        # plot all the whole chromosome interactions
        filepath = input.gi

        # set up the pdf to which we will save everything
        outfile = output.pdf
        pp = PdfPages(outfile)

        dfs = []
        columns = [["source", "sstart", "sstop",
                      "target", "tstart", "tstop",
                      "counts"],
                   ["target", "tstart", "tstop",
                   "source", "sstart", "sstop",
                   "counts"]]

        for i in range(2):
            df = pd.read_csv(filepath, header=None, sep='\t')
            df.columns = columns[i]
            # Feb 2022 - I do not know why this big block is commented out
            #df["distance"] = df["tstart"] - df["sstart"]
            #expected = df.groupby(["distance"]).mean().reset_index()
            #expected["expected"] = expected["counts"]
            #expected = expected[["distance", "expected"]]
            #df = df.merge(expected, left_on='distance', right_on='distance')
            #df["log_obs_exp"] = np.log2(df["counts"]/df["expected"])
            #df["source"] = df.apply(lambda row: ''.join(i for i in row["source"] if i.isdigit()), axis =1)
            #df["target"] = df.apply(lambda row: ''.join(i for i in row["target"] if i.isdigit()), axis =1)
            #df["source"] = df.apply(lambda row: ''.join(row["source"].str.replace("Chr", "").replace("chr", "").replace("c",""), axis =1))
            #df["target"] = df.apply(lambda row: ''.join(row["target"].str.replace("Chr", "").replace("chr", "").replace("c",""), axis =1))
            #df["source"] = df.apply(lambda row: row["source"].str.replace("Chr", ""), axis =1)
            #df["target"] = df.apply(lambda row: row["target"].str.replace("Chr", ""), axis =1)   
            for deletethis in ["Chr", "chr", "c"]:
                df["source"] = df["source"].str.replace(deletethis, "")
                df["target"] = df["target"].str.replace(deletethis, "")
            dfs.append(df)

        concat = pd.concat(dfs).reset_index()
        concat = concat[[x for x in concat.columns if x != "index"]]

        #first, just read in the the dataframe to get all of the chromosomes
        chroms = list(concat["target"].unique())

        directorypath = output.pdf.replace(".pdf", "")
        if not os.path.exists(directorypath):
            os.mkdir(directorypath)

        for middle in ["dark", "light"]:
            for thischrom in chroms:
                missing_rows = []
                onechrom = concat.loc[concat["source"] == concat["target"], ]
                onechrom = onechrom.loc[onechrom["source"] == thischrom, ]
                #figure out the interval
                interval = (onechrom["sstop"] - onechrom["sstart"]).value_counts().idxmax()
                minstart = onechrom["sstart"].min()
                maxstart = onechrom["sstart"].max()
                #now go through and find missing rows
                has_these_starts_set = set(onechrom["sstart"].unique())
                should_have_all_these_starts_set = set(np.arange(minstart,maxstart+interval,interval))
                needs_these_rows = sorted(list(should_have_all_these_starts_set.difference(has_these_starts_set)))
                #manually add the rows to the missing_rows_list
                for i in range(len(needs_these_rows)):
                    for j in range(len(needs_these_rows)):
                        missing_rows.append({"source": thischrom,
                                             "sstart": needs_these_rows[i],
                                             "sstop":  needs_these_rows[i] + interval,
                                             "target": thischrom,
                                             "tstart": needs_these_rows[j],
                                             "tstop":  needs_these_rows[j] + interval,
                                             "counts": np.nan
                                            })
                tempdf = pd.DataFrame.from_dict(missing_rows)
                plot_concat = pd.concat([onechrom, tempdf]).reset_index(drop=True)
                plot_concat["tstart"] = plot_concat["tstart"]/1000000
                plot_concat["sstart"] = plot_concat["sstart"]/1000000
                result = plot_concat.pivot_table(index='tstart',
                                        columns='sstart',
                                        values='counts', dropna=False)
                #result.columns.name = None
                ax = sns.heatmap(result,
                                 cmap = sns.diverging_palette(256, 0, n=100, center=middle), 
                                 square=True,
                                 mask=result.isnull(),
                                 linewidths=0.0)
                ax.set_title("Hi-C Obs/Exp Pearson - Chr {}".format(thischrom))
                ax.set_ylabel("Position (Mb)")
                ax.set_xlabel("Position (Mb)")
                fig = ax.get_figure()
                fig.tight_layout()
                pngpath = directorypath + "/plot_of_{}.{}.{}.png".format(thischrom, interval, middle)
                print("Saving the file: {}".format(pngpath))
                fig.savefig(pngpath, dpi=300)
                pp.savefig()
                fig.clear()
        pp.close()

## Now plot the gene interactions matrix
## get the list of chromosomes that we will keep for 3DGR
#
#rule gene_interactions_matrix:
#    input:
#        cool = config["tool"] + "/output/{nom}/{nom}_balanced.{binsize}.cool"
#    output:
#        giv = config["tool"] + "/output/{nom}/3DGR/{nom}_balanced.{binsize}.gi.tsv"
#    params:
#        giv = lambda w: config["tool"] + "/output/{nom}/3DGR/{nom}_balanced.{binsize}.gi".format(
#               nom = w.nom, binsize = w.binsize)
#    threads: 1
#    shell:
#        """
#        hicConvertFormat -m {input.cool} -o {params.giv} \
#           --inputFormat cool --outputFormat ginteractions
#        """
#
#rule convert_gi_to_if:
#    """
#    This converts the gi to an interaction frequency matrix used by LorDB
#    """
#    input:
#        giv = config["tool"] + "/output/{nom}/3DGR/{nom}_balanced.{binsize}.gi.tsv",
#        gi_to_if_script = os.path.join(filepath, "gi_to_if.py")
#    output:
#        giv = config["tool"] + "/output/{nom}/3DGR/{nom}_balanced.{binsize}.if.tsv"
#    threads: 1
#    shell:
#        """
#        cat {input.giv} | python {input.gi_to_if_script} > {output.giv}
#        """

# This part makes a pretext map
rule make_pretextmap:
    input:
        bam   = expand(config["tool"] + "/input/bams/{{nom}}_{lib}_to_ref.sorted.bam", lib=config["libs"]),
    output:
        pretext = config["tool"] + "/output/{nom}/{nom}.pretext"
    threads: 1
    shell:
        """
        NUMBAM=$(echo "{input.bam}" | sed 's/ /\\n/g' | wc -l)
        if [ ${{NUMBAM}} -gt 1 ]
          then
            samtools cat -h {input.bam} | samtools view -h - | \
                PretextMap --sortby nosort -o {output.pretext}
          else
            samtools view -h {input.bam} | \
                PretextMap --sortby nosort -o {output.pretext}
        fi
        """
