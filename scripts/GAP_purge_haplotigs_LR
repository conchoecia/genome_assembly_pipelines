"""
Haplotig removal from several candidate genome assemblies.

This pipeline is stage two in diploid genome assembly for invertebrates.

It first characterizes how many reads are mapped to each genome assembly.
Then, with some user intervention it purges haplotigs.
After purging haplotigs it clips the overlapping ends of the genome assembly.
Lastly, it characterizes the resulting assemblies and makes a table for the user to look at
 to select the best assemblies.
"""

from subprocess import Popen, PIPE
import os
import pandas as pd
import subprocess
import sys
import time
import functions
filepath = os.path.dirname(os.path.realpath(__file__))
fs_path=os.path.join(filepath, "../bin/fasta_stats")

config["tool"] = "PH_pipeline"
CUTOFFS = [95, 90, 80, 70]

configfile: "config.yaml"

rule all:
    input:
        #expand("PH_pipeline/bams/LR_to_{nom}.sorted.bam", nom=config["assemblies"]),
        "PH_pipeline/data_output/LR_read_id_and_length.txt",
        expand("PH_pipeline/before_PH/{nom}_LR.flagstat", nom=config["assemblies"]),
        expand("PH_pipeline/before_PH/{nom}_LR.permap.txt", nom=config["assemblies"]),
        #get the histogram pdf
        expand("PH_pipeline/coverage/plot_histo_LR_{nom}.pdf", nom=config["assemblies"]),
        expand("PH_pipeline/PH_hist/LR_to_{nom}.sorted.bam.gencov", nom=config["assemblies"]),
        #purge
        expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta", nom=config["assemblies"], CO=CUTOFFS),
        #clip
        expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta", nom=config["assemblies"], CO=CUTOFFS),
        ##now collect information about the clip
        #expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        #expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        #expand("PH_pipeline/coverage_after_clip/plot_histo_LR_{nom}_CO{CO}.pdf", nom=config["assemblies"], CO=CUTOFFS),
        #"PH_pipeline/final_results/final_results.tsv"

rule symlink_the_LRs:
    """
    Symlinks the long reads so they are easier to work with in the rest of the
      pipeline.
    """
    input:
        LR = config["LR"]
    output:
        assem = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    params:
        fileprefix = config["tool"] + "/input/longreads/"
    run:
        for thisfile in config["LR"]:
            fname = os.path.basename(thisfile)
            dest = "{}{}".format(params.fileprefix, fname)
            os.symlink(thisfile, dest)

rule map_LR_to_genome:
    """
    maps the reads to the assembly
    """
    input:
        #assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        assem = lambda wildcards: config["assemblies"][wildcards.nom],
        LRs = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        bam = config["tool"] + "/input/bams_LR/LR_to_{nom}.bam"
    params:
        minimaparg = config["minimap2arg"]
    threads: workflow.cores - 1
    shell:
        """
        minimap2 -t {threads} -ax {params.minimaparg} {input.assem} \
          {input.LRs} | \
          samtools view -hb - | \
          samtools sort - > {output.bam}
        """

rule index_LR_bams:
    input:
        bam = config["tool"] + "/input/bams_LR/LR_to_{nom}.bam"
    output:
        bam = config["tool"] + "/input/bams_LR/LR_to_{nom}.bam.bai"
    threads: 1
    shell:
        """
        samtools index {input.bam}
        """

rule generate_LR_read_length:
    input:
        LRs = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    threads:
        1
    shell:
        """
        cat {input.LRs} | bioawk -cfastx '{{ printf("%s\\t%s\\n", $name, length($seq)) }}' > {output.txt}
        """

rule flagstat_mapped_before_purge:
    input:
        bam = "PH_pipeline/input/bams_LR/LR_to_{nom}.bam"
    output:
        flagstat_info = "PH_pipeline/before_PH/{nom}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule percent_of_data_mapping_before:
    input:
        bam = "PH_pipeline/input/bams_LR/LR_to_{nom}.bam",
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/before_PH/{nom}_LR.permap.txt"
    threads:
        1
    run:
        transcript_to_len = {}
        added = set()
        tot_mapped = 0
        num_reads = 0
        tot_size = 0
        print("reading in the thing")
        with open(input.txt, "r") as f:
            for line in f:
                if line.strip():
                    splitd = line.split()
                    transcript_to_len[splitd[0]] = int(splitd[1])
                    tot_size += int(splitd[1])
                    num_reads += 1
        print("done reading in the thing")
        command = "samtools view -F 4 {} | cut -f1".format(input.bam)
        #command = "cat temp.sam"

        #print("command is: ", command)
        #sys.exit()
        process = Popen(command, stdout=PIPE, shell=True)
        blank_counter = 0
        while True:
            line = process.stdout.readline().rstrip().decode("utf-8").strip()
            try:
                if (len(added) % 1000) == 0:
                    percent_of_reads_observed = 100 * (len(added)/num_reads)
                    percent_of_data_mapped = 100 * (tot_mapped/tot_size)
                    differential = 100 * ((percent_of_reads_observed - percent_of_data_mapped)/percent_of_reads_observed)
                    sys.stdout.write("  {0:.3f}% Done. {1:.3f}% of data mapped. {2:.3f}% has not mapped.  \r".format(
                                    percent_of_reads_observed,
                                    percent_of_data_mapped,
                                    differential
                                   ))
                    sys.stdout.flush()
            except:
                pass
            if not line.strip():
                blank_counter += 1
                if blank_counter > 99999:
                    break
            else:
                if line not in added:
                    added.add(line)
                    tot_mapped += transcript_to_len[line]

        with open(output.per_map, "w") as f:
            print("Number of bases mapped: {}".format(tot_mapped), file=f)
            print("Number of bases total: {}".format(tot_size), file=f)
            print("{0:.4}%".format(100* (tot_mapped/tot_size)), file=f)
            print("", file=f)
            print("Number of reads mapped: {}".format(len(added)), file=f)
            print("Number of reads total: {}".format(num_reads), file=f)
            print("{0:.4}%".format(100* (len(added)/num_reads)), file=f)

rule cov_histo_check:
    """
    just checks the average coverage of the bam files before running
     purge haplotigs
    """
    input:
        bam = "PH_pipeline/input/bams_LR/LR_to_{nom}.bam",
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    output:
        cov = "PH_pipeline/coverage/LR_{nom}.cov",
        his = "PH_pipeline/coverage/histo_LR_{nom}.histo",
        pdf = "PH_pipeline/coverage/plot_histo_LR_{nom}.pdf"
    threads:
        1
    shell:
        """
        bedtools genomecov -d -ibam {input.bam} \
            -g {input.assembly} > {output.cov}
        cut -f3 {output.cov} | sort | uniq -c | \
            column -t | sort -k2 -n > {output.his}
        cat {output.his} | plot_uniq_c.py -x 15 -X 400 -d
        mv read_depth_histogram.pdf {output.pdf}
        """

rule PH_hist:
    input:
        bam = "PH_pipeline/input/bams_LR/LR_to_{nom}.bam",
        assembly = lambda wildcards: config["assemblies"][wildcards.nom]
    output:
        gc  = "PH_pipeline/PH_hist/LR_to_{nom}.sorted.bam.gencov",
        pdf = "PH_pipeline/PH_hist/LR_to_{nom}.sorted.bam.histogram.png"
    params:
        gc  = lambda wildcards: "LR_to_{}.bam.gencov".format(wildcards.nom),
        pdf = lambda wildcards: "LR_to_{}.bam.histogram.png".format(wildcards.nom)
    threads:
        10
    shell:
        """
        purge_haplotigs hist -b {input.bam} \
            -g {input.assembly} -t {threads} -d 500
        mv {params.gc} {output.gc}
        mv {params.pdf} {output.pdf}
        """

rule PH_cov:
    input:
        gc = "PH_pipeline/PH_hist/LR_to_{nom}.sorted.bam.gencov"
    output:
        csv = "PH_pipeline/PH_cov/{nom}_cov_stats.csv"
    params:
        low = config["low"],
        medium = config["medium"],
        high = config["high"]
    threads:
        1
    shell:
        """
        if [[ {params.low} -eq -1 ]]; then
            exit 1
        fi
        if [[ {params.medium} -eq -1 ]]; then
            exit 1
        fi
        if [[ {params.high} -eq -1 ]]; then
            exit 1
        fi
        purge_haplotigs  cov  -i {input.gc}  \
               -l {params.low} -m {params.medium} -h {params.high} \
               -o {output.csv} -j 80  -s 80
        """

rule PH_purge:
    input:
        assembly = lambda wildcards: config["assemblies"][wildcards.nom],
        csv = "PH_pipeline/PH_cov/{nom}_cov_stats.csv",
        bam = "PH_pipeline/input/bams_LR/LR_to_{nom}.bam"
    output:
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta",
        haplotigs = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.haplotigs.fasta"
    params:
        cutoff   = lambda wildcards: wildcards.CO,
        tempdir  = lambda wildcards: "temp_{}_{}".format(wildcards.nom, wildcards.CO),
        mv_these = lambda wildcards: "PH_purge_{0}_CO{1}".format(wildcards.nom, wildcards.CO),
        mkdir1   = "PH_pipeline/PH_purge",
        mkdir2   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies".format(wildcards.nom),
        outdir   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/".format(wildcards.nom, wildcards.CO),
    threads:
        workflow.cores
    shell:
        """
        rm -rf {params.tempdir}
        mkdir {params.tempdir}
        cd {params.tempdir}
        mkdir {params.mv_these}

        purge_haplotigs purge -g {input.assembly} \
               -c ../{input.csv} -t {threads} \
               -d -b ../{input.bam} \
               -a {params.cutoff} \
               -o {params.mv_these}

        if [ ! -d ../{params.mkdir1} ]
        then
            mkdir -p ../{params.mkdir1}
        fi

        if [ ! -d ../{params.mkdir2} ]
        then
            mkdir -p ../{params.mkdir2}
        fi

        if [ ! -d ../{params.outdir} ]
        then
            mkdir -p ../{params.outdir}
        fi

        mv {params.mv_these}* ../{params.outdir}
        mv dotplots* ../{params.outdir}
        cd ..
        rm -rf {params.tempdir}
        """

rule PH_clip:
    input:
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.fasta",
        haplotigs = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}.haplotigs.fasta"
    output:
        clip = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    params:
        outdir   = lambda wildcards: "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/".format(wildcards.nom, wildcards.CO),
        mv_these = lambda wildcards: "PH_pipeline/PH_purge_{}_CO{}_clip".format(wildcards.nom, wildcards.CO)
    threads:
        workflow.cores
    shell:
        """
        mkdir -p {params.mv_these}
        purge_haplotigs clip  -p {input.assembly} \
               -h {input.haplotigs} \
               -o {params.mv_these}
        # sometimes there are no overlaps, and the directory just ends up empty.
        # in that case we need to handle the file specially.
        if [ -z "$(ls -A /path/to/dir)" ]; then
            cp {input.assembly} {output.clip}
        else
            mv {params.mv_these}* {params.outdir}
        fi
        rm -rf {params.mv_these}
        """

rule map_LR_to_purged:
    input:
        #assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        assem = lambda wildcards: config["assemblies"][wildcards.nom],
        LRs = expand(config["tool"] + "/input/longreads/{LR}",
                       LR = [os.path.basename(x) for x in config["LR"]])
    output:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam"
    threads:
        workflow.cores
    shell:
        """
        minimap2 -t {threads} -ax {params.minimaparg} {input.assem} \
          {input.LRs} | \
          samtools view -hb - | \
          samtools sort - > {output.bam}
        """

rule flagstat_mapped_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam"
    output:
        flagstat_info = "PH_pipeline/after_PH/{nom}_CO{CO}_LR.flagstat"
    threads:
        1
    shell:
        """
        samtools flagstat {input.bam} > {output.flagstat_info}
        """

rule percent_of_data_mapping_after_purge:
    input:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam",
        txt = "PH_pipeline/data_output/LR_read_id_and_length.txt"
    output:
        per_map = "PH_pipeline/after_PH/{nom}_CO{CO}_LR.permap.txt"
    threads:
        1
    run:
        transcript_to_len = {}
        added = set()
        tot_mapped = 0
        num_reads = 0
        tot_size = 0
        print("reading in the thing")
        with open(input.txt, "r") as f:
            for line in f:
                if line.strip():
                    splitd = line.split()
                    transcript_to_len[splitd[0]] = int(splitd[1])
                    tot_size += int(splitd[1])
                    num_reads += 1
        print("done reading in the thing")
        command = "samtools view -F 4 {} | cut -f1".format(input.bam)
        #command = "cat temp.sam"

        #print("command is: ", command)
        #sys.exit()
        process = Popen(command, stdout=PIPE, shell=True)
        blank_counter = 0
        while True:
            line = process.stdout.readline().rstrip().decode("utf-8").strip()
            if (len(added) % 1000) == 0:
                try:
                    percent_of_reads_observed = 100 * (len(added)/num_reads)
                    percent_of_data_mapped = 100 * (tot_mapped/tot_size)
                    differential = 100 * ((percent_of_reads_observed - percent_of_data_mapped)/percent_of_reads_observed)
                    sys.stdout.write("  {0:.3f}% Done. {1:.3f}% of data mapped. {2:.3f}% has not mapped.  \r".format(
                                    percent_of_reads_observed,
                                    percent_of_data_mapped,
                                    differential
                                   ))
                    sys.stdout.flush()
                except:
                    pass
            if not line.strip():
                blank_counter += 1
                if blank_counter > 99999:
                    break
            else:
                if line not in added:
                    added.add(line)
                    tot_mapped += transcript_to_len[line]

        with open(output.per_map, "w") as f:
            print("Number of bases mapped: {}".format(tot_mapped), file=f)
            print("Number of bases total: {}".format(tot_size), file=f)
            print("{0:.4}%".format(100* (tot_mapped/tot_size)), file=f)
            print("", file=f)
            print("Number of reads mapped: {}".format(len(added)), file=f)
            print("Number of reads total: {}".format(num_reads), file=f)
            print("{0:.4}%".format(100* (len(added)/num_reads)), file=f)

rule cov_histo_check_after_purge:
    """
    just checks the average coverage of the bam files before running
     purge haplotigs
    """
    input:
        bam = "PH_pipeline/bams/clipped/LR_to_{nom}_CO{CO}_clip.sorted.bam",
        assembly = "PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta"
    output:
        cov = "PH_pipeline/coverage_after_clip/LR_{nom}_CO{CO}.cov",
        his = "PH_pipeline/coverage_after_clip/histo_LR_{nom}_CO{CO}.histo",
        pdf = "PH_pipeline/coverage_after_clip/plot_histo_LR_{nom}_CO{CO}.pdf"
    threads:
        1
    shell:
        """
        bedtools genomecov -d -ibam {input.bam} \
            -g {input.assembly} > {output.cov}
        cut -f3 {output.cov} | sort | uniq -c | \
            column -t | sort -k2 -n > {output.his}
        cat {output.his} | plot_uniq_c.py -x 15 -X 400 -d
        mv read_depth_histogram.pdf {output.pdf}
        """

rule make_genome_stats_table:
    """
    makes a table of all the relevant genome stats
    """
    input:
        assembly = expand("PH_pipeline/PH_purge/{nom}_assemblies/PH_purge_{nom}_CO{CO}/PH_purge_{nom}_CO{CO}_clip.fasta", nom=config["assemblies"], CO=CUTOFFS),
        per_before = expand("PH_pipeline/before_PH/{nom}_LR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        per_map = expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.permap.txt", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_before = expand("PH_pipeline/before_PH/{nom}_LR.flagstat", nom=config["assemblies"], CO=CUTOFFS),
        flagstat_info = expand("PH_pipeline/after_PH/{nom}_CO{CO}_LR.flagstat", nom=config["assemblies"], CO=CUTOFFS)
    output:
        results_table = "PH_pipeline/final_results/final_results.tsv"
    run:
        all_samples = []
        for nom in config["assemblies"]:
            for CO in CUTOFFS:
                tfile = "PH_pipeline/PH_purge/{0}_assemblies/PH_purge_{0}_CO{1}/PH_purge_{0}_CO{1}_clip.fasta".format(nom, CO)
                this_dict = {}
                this_dict["assem_file"] = tfile
                this_dict["PH_CO"] = CO
                this_dict["name"] = nom
                # get the NCBI info first
                # now get the other assembly info from fasta_stats
                fstats_dict = functions.run_fasta_stats(tfile)
                z1 = {**this_dict, **fstats_dict}

                # BEFORE PURGE HAPLOTIGS
                # get the flagstat info
                fstat_file = "PH_pipeline/before_PH/{}_LR.flagstat".format(nom)
                z1["before_flagstat_pMap"] = functions.parse_flagstat(fstat_file)
                #get the manually calculated percent that maps (bases and reads)
                pmap_file = "PH_pipeline/before_PH/{}_LR.permap.txt".format(nom)
                results = functions.parse_permap(pmap_file)
                z1["before_manual_pBMap"] = results[0]
                z1["before_manual_pMap"] = results[1]

                # AFTER PURGE HAPLOTIGS
                # get the flagstat info
                fstat_file = "PH_pipeline/after_PH/{0}_CO{1}_LR.flagstat".format(nom, CO)
                z1["after_flagstat_pMap"] = functions.parse_flagstat(fstat_file)
                #get the manually calculated percent that maps (bases and reads)
                pmap_file = "PH_pipeline/after_PH/{0}_CO{1}_LR.permap.txt".format(nom, CO)
                results = functions.parse_permap(pmap_file)
                z1["after_manual_pBMap"] = results[0]
                z1["after_manual_pMap"] = results[1]

                # DIFFERENCE
                z1["before_minus_after_flagstat_pMap"] = z1["before_flagstat_pMap"] - z1["after_flagstat_pMap"]
                z1["before_minus_after_manual_pBMap"]  = z1["before_manual_pBMap"] - z1["after_manual_pBMap"]
                z1["before_minus_after_manual_pMap"]   = z1["before_manual_pMap"] - z1["after_manual_pMap"]
                all_samples.append(z1)
                time.sleep(1)
        # now make a df with all the results
        df = pd.DataFrame(all_samples)
        df.to_csv(output.results_table, sep='\t', index=False)
