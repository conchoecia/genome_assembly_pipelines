"""
This snakemake file is used for phasing a genome given a set of input data.
If some of the data types are missing, that is OK.
Uses HAPCUT2.

Script inspired by:
# author: Peter Edge
# 12/19/2016
# email: pedge@eng.ucsd.edu

Build history:
  - Updated in August 2025 to better work with SLURM and the new version of Picard.
    Now it is on snakemake 9.
  - Heavily modified by DTS in 2020-2021 dts@ucsc.edu. Written using snakemake 5.10.0

"""

import sys
from pathlib import Path

configfile: "config.yaml"
config["tool"] = "GAP_phase"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

# if Illumina data missing
if config["ILL_R1"] == "" or config["ILL_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem, exist_ok=True)
    config["ILL_R1"] = outstem + "/ill_R1.fastq.gz"
    config["ILL_R2"] = outstem + "/ill_R2.fastq.gz"
    if not os.path.exists(config["ILL_R1"]):
        os.mknod(config["ILL_R1"])
    if not os.path.exists(config["ILL_R2"]):
        os.mknod(config["ILL_R2"])

if config["CHI_R1"] == "" or config["CHI_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["CHI_R1"] = outstem + "/chi_R1.fastq.gz"
    config["CHI_R2"] = outstem + "/chi_R2.fastq.gz"
    if not os.path.exists(config["CHI_R1"]):
        os.mknod(config["CHI_R1"])
    if not os.path.exists(config["CHI_R2"]):
        os.mknod(config["CHI_R2"])

if config["HIC_R1"] == "" or config["HIC_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["HIC_R1"] = outstem + "/hic_R1.fastq.gz"
    config["HIC_R2"] = outstem + "/hic_R2.fastq.gz"
    if not os.path.exists(config["HIC_R1"]):
        os.mknod(config["HIC_R1"])
    if not os.path.exists(config["HIC_R2"]):
        os.mknod(config["HIC_R2"])

if config["LONGREAD_BAM"] == "":
    outstem = config["tool"] + "/output/bams/temp"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["longread_bam"] = outstem + "/longreads_sorted.bam"
    if not os.path.exists(config["longread_bam"]):
        os.mknod(config["longread_bam"])

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    datatype="[A-Za-z0-9|.]+",
    libtype="[A-Za-z]+"

rule all:
    input:
        expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"]),
        expand(config["tool"] + "/output/bams/temp/{libtype}_sorted.bam",
               libtype = ["ill", "chi", "hic", "longreads"]),
        expand(config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai",
               libtype = ["ill", "chi", "hic", "longreads"]),
        expand(config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
               chrom = config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
               chrom = config["CHROMS"]),

        config["tool"] + "/output/final_output/complete_assembly.hap.gz",
        config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt",
        config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        expand(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz", chrom = config["CHROMS"]),

        # also generate haplotagged bam files:
        expand(config["tool"] + "/output/final_output/{libtype}_haplotagged.bam",
               libtype = ["ill", "chi", "hic", "longreads"])

######################################################################
#  Make a softlink of the assembly
######################################################################
rule make_assem_softlink:
    input:
        ref = config["REFERENCE"],
    output:
        assem = config["tool"] + "/input/assembly/input.fasta"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        ln -s {input.ref} {output.assem}
        """

######################################################################
#   Make symlinks of the input reads
######################################################################

rule make_ill_softlink:
    """Just symlink the reads to not occupy more disk space."""
    params:
        reads1 = config["ILL_R1"],
        reads2 = config["ILL_R2"]
    output:
        R1 = config["tool"] + "/input/reads/ill_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/ill_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_chi_softlink:
    params:
        reads1 = config["CHI_R1"],
        reads2 = config["CHI_R2"]
    output:
        R1 = config["tool"] + "/input/reads/chi_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/chi_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_hic_softlink:
    params:
        reads1 = config["HIC_R1"],
        reads2 = config["HIC_R2"]
    output:
        R1 = config["tool"] + "/input/reads/hic_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/hic_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_longreads_bam_softlink:
    params:
        longreads = config["LONGREAD_BAM"]
    output:
        bam = config["tool"] + "/output/bams/temp/longreads_sorted.bam"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.longreads} {output.bam}
        """

######################################################################
#  First we deal with the input VCF file
#   - it could be that the VCF is not sorted correctly, so let's sort
#      it according to the input assembly
######################################################################

# get a picard dict of the reference sequences
rule picard_dict:
    """
    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        picard = config["PICARD"]
    output:
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        java -jar -Xmx1g {input.picard} CreateSequenceDictionary \
          -R {input.ref} \
          -O {output.seqdict}
        """

rule update_vcf_dict:
    """
    Update the header of the vcf file

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        seqdict = config["tool"] + "/input/ref/input.ref.dict",
        vcf = config["VCFfile"],
        picard = config["PICARD"]
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        java -jar -Xmx1g {input.picard} UpdateVcfSequenceDictionary \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# filter the vcf to remove entries that are not legal to hapcut2
rule filter_vcf:
    """
    In DeepVariant, it makes reference calls and annotates them as ./.
    These are not legal, so remove them.
     - ./. We remove these, since these are simply ref calls
     - 0/3 We remove these, since there are 3 alt alleles that will break HapCUT2
     - 1/3 This isn't valid because there are still multiple alleles to pick from
     - 0/2 We should probably remove this, since one is ref, and there are two options for the alt.
     - 1/2 ??
     - 0/2 ??
     - 2/3 ??
     - n/4 too many, just remove this
     - n/5 too many, just remove this
     - n/6 too many, just remove this
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 5
    shell:
        """
        awk 'BEGIN{{OFS="\t"}} /^#/ {{print; next}} $7 != "RefCall" {{print}}' {input.vcf} | \
            awk '/^#/ {{print; next}} $10 !~ /\\/3/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/4/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/5/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/6/ {{print}}' | \
            grep -v '\\./\\.' | \
            grep -v '0/2' > {output.vcf}
        """

# sort the vcf file based on the input assembly
rule sort_input_vcf:
    """
    If the input VCF isn't sorted correctly, then the downstream steps
     won't work.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf",
        picard  = config["PICARD"],
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 30
    shell:
        """
        java -jar -Xmx3g {input.picard} SortVcf \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# compress the vcf file
rule split_vcf_into_chr_zip:
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    output:
        tvcf = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        bgzip -c {input.vcf} > {output.tvcf}  #compress vcf
        """

# index the vcf file
rule index_vcf:
    input:
        tvcf = config["tool"] + "/output/vcf/orig/temp.vcf.gz"
    output:
        index = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi"),
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        tabix -p vcf {input.tvcf}  # index compressed vcf
        """

rule list_of_chroms_from_reference:
    input:
        fai = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    shell:
        """
        # save all the chromosome names into a file
        cut -f1 {input.fai} > {output.chrtxt}
        """

rule list_of_chroms_from_config:
    """
    The point of this is to get the list of chromosomes that we will keep for phasing.

    Notes:
      - Previously, this would fail sometimes, even if there were perfect matches
    """
    input:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt",
    output:
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    run:
        # first, print out the chromosomes so we are sure of what we're looking for
        print("The chromosomes we are looking for are:")
        CHROMLIST = [str(chrom) for chrom in config["CHROMS"]]
        print(CHROMLIST)

        matches = []
        outhandle = open(output.chrtxt, "w")
        with open(input.chrtxt, "r") as f:
            for line in f:
                line = str(line.strip())
                if line:
                    if line in CHROMLIST:
                        matches.append(line)
        # if there were no matches, state as much and raise an error. We should have found something
        if len(matches) == 0:
            print("No matching chromosomes found.", file=outhandle)
            raise ValueError("No matching chromosomes found.")

        # If we found matches, write them to the output file
        for line in matches:
            print(line, file=outhandle)

# split all vcf files by chromosome
rule split_vcf_into_chr_remainder:
    input:
        gz = config["tool"] + "/output/vcf/orig/temp.vcf.gz",
        index = config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi",
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    output:
        vcf  = expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"])
    params:
        outstub = config["tool"] + "/output/vcf/orig/split"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        while IFS= read -r line; do
          tabix {input.gz} $line > {params.outstub}/$line.vcf;
        done < {input.chrtxt}
        """

######################################################################
#  Now handle the read mapping
######################################################################

# index reference genome
rule index_genome:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt"
    threads: 1
    resources:
        mem_mb  = 16000,
        runtime = 120
    shell:
        'bwa index {input.ref}'

rule index_genome_faidx:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.fai"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 10
    shell:
        'samtools faidx {input.ref}'

# align HiC fastq file to reference
rule align_HiC_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/hic_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/hic_R2.fastq.gz"
    output:
        hic_bam = config["tool"] + "/output/bams/temp/hic_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} -5SPM {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.hic_bam}
        """

rule align_Chicago_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/chi_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/chi_R2.fastq.gz"
    output:
        chi_bam = config["tool"] + "/output/bams/temp/chi_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} -5SPM {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.chi_bam}
        """

rule align_Illumina_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/ill_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/ill_R2.fastq.gz"
    output:
        ill_bam = config["tool"] + "/output/bams/temp/ill_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.ill_bam}
        """

rule index_bams:
    input:
        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam"
    output:
        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
    threads: 1
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Split the chromosomes
######################################################################

# SPLIT bam files by chromosome
rule split_bams:
    """
    This is incredibly slow for any large bam file.
    Needs reworking for parallelization, even though it is tough on the disks.
    """
    input:
        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam",
        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
    output:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
    params:
        stub = config["tool"] + "/output/bams/split/"
    threads: 4
    resources:
        mem_mb=8000, runtime=60
    shell:
        """
        set -euo pipefail
        mkdir -p {params.stub}
        samtools view -@ {threads} -bh {input.bam} "{wildcards.chrom}" -o {output.bam}
        """

rule add_rg:
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        picard = config["PICARD"]
    output:
        bam = temp(config["tool"] + "/output/bams/split/{libtype}.rg.REF_{chrom}.bam")
    threads: 2
    resources:
        mem_mb  = 4000,
        runtime = 90
    shell:
        """
        java -Xmx3g -jar {input.picard} AddOrReplaceReadGroups \
          -I {input.bam} -O {output.bam} \
          --RGID rg1 --RGLB lib1 --RGPL ILLUMINA --RGPU unit1 --RGSM sample1
        """

rule mark_duplicates_in_split_bams:
    """
    This marks the duplicates for chicago, illumina, and hi-c.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   --INPUT {input.bam}
    """
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.rg.REF_{chrom}.bam",
        picard = config["PICARD"]
    output:
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
        metrics = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.metrics"
    threads: 1
    resources:
        mem_mb  = 8000,
        runtime = 90  # minutes
    shell:
        """
        java -jar -Xmx7g {input.picard} MarkDuplicates \
          --READ_NAME_REGEX null \
          --INPUT {input.bam} \
          --OUTPUT {output.bam} \
          --METRICS_FILE {output.metrics} \
          --ASSUME_SORTED true
        """

rule index_markeddup_bams:
    input:
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
    output:
        bai = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam.bai",
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Extract the hairs from the different data types
######################################################################

# convert Hi-C bam files to haplotype fragment files
rule extract_hairs:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        fai = config["tool"] + "/input/assembly/input.fasta.fai",
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
        bai = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam.bai",
        vcf = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        frag = config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{chrom}.frag"
    threads: 1
    params:
        fragtype = lambda wildcards: wildcards.libtype
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        if [ "{params.fragtype}" == "longreads" ]; then
            extractHAIRS --pacbio 1 --new_format 1 --indels 1 \
               --bam {input.bam} --ref {input.ref} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "hic" ]; then
            extractHAIRS --hic 1 --new_format 1 \
               --indels 1 --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "chi" ]; then
            extractHAIRS --maxIS 10000000 --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "ill" ]; then
            extractHAIRS --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        fi
        """

rule concatenate_HAPCUT2_evidence:
    input:
        frags = expand(config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{{chrom}}.frag",
                       libtype = ["ill", "chi", "hic", "longreads"])
    output:
        cat = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        cat {input.frags} > {output.cat}
        """

######################################################################
# Run HAPCUT2 - once per scaffold
######################################################################

# run HapCUT2 to assemble haplotypes from combined Hi-C + longread haplotype fragments
rule run_hapcut2_hic_longread:
    input:
        frag = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
        vcf  = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        hap   = config["tool"] + "/output/hapcut2_results/{chrom}.hap",
        vcf   = config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF"
    params:
        thischrom = lambda wildcards: wildcards.chrom,
        model = config["tool"] + "/output/hapcut2_results/{chrom}.htrans_model"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 120
    shell:
        """
        HAPCUT2 --fragments {input.frag} --vcf {input.vcf} \
            --output {output.hap} \
            --hic 1 \
            --htrans_data_outfile {params.model} \
            --outvcf 1
        """

######################################################################
# Once we have the HAPCUT2 output, put it in a useful format
#
# The rules cat_haps and merge_vcfs were previously in one rule.
#  I encountered some errors so split them up.
######################################################################

rule cat_haps:
    """
    This pools all of the haplotype files that we generated previously.
    """
    input:
        hap = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
                     chrom=config["CHROMS"])
    output:
        hap = temp(config["tool"] + "/output/final_output/complete_assembly.hap")
    threads: 1
    resources:
        mem_mb=1000, runtime=10
    shell:
        r"""
        mkdir -p {config[tool]}/output/final_output
        cat {input.hap} > {output.hap}
        """

rule merge_vcfs:
    """
    202508 - This file isn't used for anything
    """
    input:
        origvcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf",
        vcfs    = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF",
                         chrom=config["CHROMS"])
    output:
        merged = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.unsorted.vcf")
    threads: 2
    resources:
        mem_mb=4000, runtime=30
    shell:
        r"""
        set -euo pipefail
        mkdir -p {config[tool]}/output/final_output

        # Merge all per-chrom VCFs into one (keep all sites, even if contigs differ)
        bcftools concat -a -D -O v -o {output.merged} {input.vcfs}

        # If you need to ensure the header carries over metadata from origvcf,
        # you can reheader (optional). Usually concat keeps a sane header:
        # bcftools annotate --header-lines {input.origvcf} -O v -o {output.merged}.tmp {output.merged}
        # mv {output.merged}.tmp {output.merged}
        """

#rule sort_final_vcf:
#    """
#    This combines the vcf and hap files from hapcut2
#
#    I found a trick here to avoid grep crashing in case of a nonmatch
#    https://unix.stackexchange.com/questions/330660
#    """
#    input:
#        origvcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf",
#        vcf = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF",
#                     chrom = config["CHROMS"]),
#        hap = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
#                     chrom = config["CHROMS"]),
#    output:
#        hap = temp(config["tool"] + "/output/final_output/complete_assembly.hap"),
#        vcf = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf")
#    threads: 1
#    resources:
#        mem_mb  = 8000,
#        runtime = 60
#    shell:
#        """
#        set +e
#
#        cat {input.origvcf} | grep '##fileformat'  > temp.vcf
#        cat {input.origvcf} | grep '##filedate'    >> temp.vcf
#        cat {input.origvcf} | grep '##source'      >> temp.vcf
#        cat {input.origvcf} | grep '##reference'   >> temp.vcf
#        cat {input.origvcf} | grep '##commandline' >> temp.vcf
#        cat {input.origvcf} | grep '##contig'      >> temp.vcf
#        cat {input.origvcf} | grep '##INFO'        >> temp.vcf
#        cat {input.origvcf} | grep '##FORMAT'      >> temp.vcf
#        cat {input.origvcf} | grep '#CHROM'        >> temp.vcf
#        echo '##SAMPLE=<ID=NONE>'        >> temp.vcf
#
#        cat {input.vcf} >> temp.vcf
#        cat {input.hap} > {output.hap}
#        cat temp.vcf | awk '$1 ~ /^#/ {{print $0;next}} {{print $0 | "sort -k1,1 -k2,2n"}}' > {output.vcf}
#        rm temp.vcf
#        set -e
#        """

rule get_largest_hap_blocks:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        biggest_blocks = {}
        this_block_header = ""
        this_block_c = ""
        this_block_start = -1
        this_block_stop = -1
        with open(input.hap, "r") as f:
            for line in f:
                line = line.replace("*", "").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        # we have just found a new block
                        if not this_block_header == "":
                            # not the first, add an entry to biggest_blocks
                            span = int(this_block_stop) - int(this_block_start) + 1
                            add_this = False
                            if this_block_c not in biggest_blocks:
                                add_this = True
                            else:
                                if span > biggest_blocks[this_block_c]["span"]:
                                    add_this = True
                            if add_this:
                                biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
                        this_block_start = -1
                        this_block_stop = -1
                        this_block_header = line
                    else:
                        if this_block_start == -1:
                            this_block_start = splitd[4]
                            this_block_stop = splitd[4]
                            this_block_c     = splitd[3]
                        else:
                            this_block_stop = splitd[4]
        # final parsing at the end
        add_this = False
        if this_block_c not in biggest_blocks:
            add_this = True
        else:
            if span > biggest_blocks[this_block_c]["span"]:
                add_this = True
        if add_this:
            biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
        with open(output.txt, "w") as f:
            for key in biggest_blocks:
                print("{}\t{}\t\"{}\"".format(
                      key,
                      biggest_blocks[key]["span"],
                      biggest_blocks[key]["name"]), file = f)

rule hap_blocks_to_vcf:
    """
    This converts the haplotype blocks to a vcf
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf",
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    output:
        final_vcf = temp(config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        # first get the headers that we want
        headers = set()
        with open(input.txt, "r") as f:
            for line in f:
                line = line.split("\"")[1]
                headers.add(line)
        # now get the sites that we will keep
        #  key is chr, set of sites
        keepers = {}
        with open(input.hap, "r") as f:
            capturing = False
            for line in f:
                line=line.replace('*',"").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        if line in headers:
                            capturing = True
                        else:
                            capturing = False
                    else:
                        if capturing:
                            this_c = splitd[3]
                            this_s = int(splitd[4])
                            if this_c not in keepers:
                                keepers[this_c] = set()
                            keepers[this_c].add(this_s)
        # now get the sites that are in the phase blocks
        writeme = open(output.final_vcf, "w")
        with open(input.vcf, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    if line[0] == '#':
                        print(line, file = writeme)
                    else:
                        # not a comment
                        splitd = line.split()
                        this_c = splitd[0]
                        this_s = int(splitd[1])
                        if this_c in keepers:
                            if this_s in keepers[this_c]:
                                print(line, file = writeme)
        writeme.close()

######################################################################
#   Compress the output
######################################################################
rule compress_final_output:
    input:
        final_vcf = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf"
    output:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        # compress
        bgzip -c {input.final_vcf} > {output.final_gz}
        # index
        tabix -p vcf {output.final_gz}
        """

rule gzip_hapcut2_vcf:
    input:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        tbi = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        bgzip -c {input.vcf} > {output.vcf}
        # index
        tabix -p vcf {output.vcf}
        """

rule gzip_complete_assembly_hap_file:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap"
    output:
        gzipped = config["tool"] + "/output/final_output/complete_assembly.hap.gz"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        cat {input.hap} | gzip > {output.gzipped}
        """

########################################################################
#    Now split this into different chromosomes and make the final table
########################################################################

rule split_phased_into_individual_chromosomes:
    input:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    output:
        chroms = temp(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf"),
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz",
        index = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 60
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        bcftools filter -r {params.tchrom} {input.final_gz} > {output.chroms}
        # compress
        bgzip -c {output.chroms} > {output.chromgz}
        # index
        tabix -p vcf {output.chromgz}
        """

rule make_table_of_final_output:
    """
    This makes a table of the results from phasing. Useful for plotting.
    The columns included are:
      - chrom (the name from the fasta header)
      - chrom_length (the sequence length)
      - largest_pb_start (1-based start index for the largest phase block)
      - largest_pb_stop  (1-based stop index for the largest phase block)
      - largest_pb_length (the size of the largest phase block)
      - largest_pb_num_var (the number of variants in the largest phase block)
      - largest_pb_num_phased (the number of phased variants in the largest phase block)
    """
    input:
        txt   = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        vcfgz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        tbi   = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi",
        ref   = config["tool"] + "/input/assembly/input.fasta"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt"
    threads: 1
    resources:
        mem_mb=2000, runtime=30
    run:
        import subprocess
        from Bio import SeqIO
        from contextlib import ExitStack

        chrom_to_length = {}

        with ExitStack() as stack:
            ref_handle  = stack.enter_context(open(input.ref, "r"))
            out_handle  = stack.enter_context(open(output.txt, "w"))
            txt_handle  = stack.enter_context(open(input.txt, "r"))

            # chrom sizes from FASTA
            for record in SeqIO.parse(ref_handle, "fasta"):
                chrom_to_length[record.id] = len(record.seq)

            print("\t".join([
                "chrom","chrom_length","largest_pb_start","largest_pb_stop",
                "largest_pb_length","percent_of_chrom_in_largest_pb",
                "largest_pb_num_var","largest_pb_num_phased","percent_of_vars_phased"
            ]), file=out_handle)

            for raw in txt_handle:
                line = raw.strip()
                if not line:
                    continue

                splitd = line.split("\t")
                chrom = splitd[0]
                chrom_length = chrom_to_length[chrom]

                # Use tabix/bcftools random access to fetch first/last POS
                cmd_base   = f"bcftools view -r {chrom} -H {input.vcfgz} | cut -f2"
                start_str  = subprocess.check_output(f"{cmd_base} | head -n1", shell=True, text=True).strip()
                stop_str   = subprocess.check_output(f"{cmd_base} | tail -n1", shell=True, text=True).strip()

                largest_pb_start = int(start_str) if start_str else 0
                largest_pb_stop  = int(stop_str)  if stop_str  else 0
                largest_pb_length = max(0, largest_pb_stop - largest_pb_start + 1)

                largest_pb_num_var    = int(splitd[2].split()[4])
                largest_pb_num_phased = int(splitd[2].split()[6])

                percent_of_chrom_in_largest_pb = f"{(largest_pb_length/chrom_length)*100:.4f}" if chrom_length else "0.0000"
                percent_of_vars_phased = f"{(largest_pb_num_phased/largest_pb_num_var)*100:.2f}" if largest_pb_num_var else "0.00"

                print("\t".join(map(str, [
                    chrom,
                    chrom_length,
                    largest_pb_start,
                    largest_pb_stop,
                    largest_pb_length,
                    percent_of_chrom_in_largest_pb,
                    largest_pb_num_var,
                    largest_pb_num_phased,
                    percent_of_vars_phased
                ])), file=out_handle)

########################################################################
#    Now we use WhatsHap to haplotag the Hi-C and long reads
########################################################################

def bam_size_mb(path: str) -> int:
    """Return BAM size in MB (>=1)."""
    try:
        return max(1, int(Path(path).stat().st_size // (1024 * 1024)))
    except FileNotFoundError:
        return 1

def mem_for_bam_mb(path: str,
                   floor=4000,    # 4 GB minimum
                   ceil=64000,    # 64 GB maximum
                   pct=0.03,      # ~3% of file size
                   base=2048) -> int:  # +2 GB headroom
    """Estimate RAM in MB as base + pct * size, clamped."""
    size_mb = bam_size_mb(path)
    est = base + size_mb * pct
    return int(max(floor, min(ceil, est)))

def runtime_for_bam_min(path: str,
                        base=30,     # 30 min minimum
                        per_gb=1.5,  # ~1.5 min per GB
                        ceil=720) -> int:  # cap at 12 h
    """Estimate runtime in minutes based on BAM size."""
    size_gb = bam_size_mb(path) / 1024
    est = base + per_gb * size_gb
    return int(max(base, min(ceil, est)))

rule haplotag_reads:
    input:
        readbam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam",
        bambai  = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai",
        vcf     = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        vcf_in  = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi",
        ref     = config["tool"] + "/input/assembly/input.fasta",
        fai     = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        tagbam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam"
    threads: 4
    params:
        outputthreads = 3
    resources:
        mem_mb  = lambda wc, input: mem_for_bam_mb(input.readbam),
        runtime = lambda wc, input: runtime_for_bam_min(input.readbam)
    shell:
        """
        set -euo pipefail

        # If BAM is invalid or has zero reads, write an empty-but-valid BAM and exit.
        if ! samtools quickcheck -v {input.readbam} >/dev/null 2>&1 || \
           [ "$(samtools view -c {input.readbam})" -eq 0 ]; then
            echo "[haplotag_reads] Input has no reads; emitting header-only BAM."
            if samtools view -H {input.readbam} >/dev/null 2>&1; then
                samtools view -H {input.readbam} | samtools view -b -o {output.tagbam} -
            else
                awk 'BEGIN{{print "@HD\\tVN:1.6\\tSO:coordinate"}} {{printf "@SQ\\tSN:%s\\tLN:%s\\n",$1,$2}}' {input.fai} \
                  | samtools view -b -T {input.ref} -o {output.tagbam} -
            fi
            samtools index -@ {threads} {output.tagbam}
            exit 0
        fi

        whatshap haplotag \
            -o {output.tagbam} \
            --reference {input.ref} \
            --ignore-read-groups \
            --ploidy 2 \
            --output-threads {params.outputthreads} \
            {input.vcf} \
            {input.readbam}

        samtools index -@ {threads} {output.tagbam}
        """
