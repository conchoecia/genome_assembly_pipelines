"""
This snakemake file is used for phasing a genome given a set of input data.
If some of the data types are missing, that is OK.
Uses HAPCUT2.

Important notes for Hi-C data:
  - For Hi-C, the paired-end reads (fastq) should be aligned using BWA with the -5, -S,
    and -P options for optimal results with HapCUT2.
  - Reference: https://github.com/vibansal/HapCUT2/issues/122

Wildcard character restrictions:
  - libtype: Letters only [A-Za-z]+ (e.g., "hic", "chi", "ill", "longreads")
  - libname: Letters and numbers only [A-Za-z0-9]+ (e.g., "DpnII", "MboI", "lib1", "PacBio2")
  - datatype: Letters only [A-Za-z]+ (e.g., "hic", "chi", "ill", "longreads")
  
  Note: Underscores, hyphens, and other special characters are NOT allowed in library names.
        Use only alphanumeric characters to avoid snakemake wildcard matching issues.

Script inspired by:
# author: Peter Edge
# 12/19/2016
# email: pedge@eng.ucsd.edu

Build history:
  - Updated in November 2025 to improve robustness and resource allocation:
    * Added dynamic memory and runtime allocation based on BAM file sizes
    * Implemented piecewise memory scaling for Hi-C data (32-160 GB based on shard size)
    * Added handling for empty BAM shards in haplotagging step
    * Added checks for VCF files with no variants to avoid unnecessary processing
    * Improved merge and indexing steps with size-based runtime estimates
    * Enhanced error handling with samtools quickcheck for invalid BAM files
    * Split haplotagging by chromosome for better parallelization and resource control
  - Updated in August 2025 to better work with SLURM and the new version of Picard.
    Now it is on snakemake 9.
  - Heavily modified by DTS in 2020-2021 dts@ucsc.edu. Written using snakemake 5.10.0

"""

import sys
from pathlib import Path

configfile: "config.yaml"
config["tool"] = "GAP_phase"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# Check if using new library format
if "libraries" in config:
    # New format: config["libraries"][libtype][libname] = {bam: "/path/to/file.bam"}
    config["LIBRARY_STRUCTURE"] = "new"

    # Ensure config["libraries"] is a dict
    if not isinstance(config["libraries"], dict):
        config["libraries"] = {}

    # Build library lists for each type
    config["LIB_NAMES"] = {}
    for libtype in ["hic", "chi", "ill", "longreads"]:
        if libtype in config["libraries"] and isinstance(config["libraries"][libtype], dict):
            config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
        else:
            config["LIB_NAMES"][libtype] = []
            # Ensure the libtype key exists in libraries
            if libtype not in config["libraries"]:
                config["libraries"][libtype] = {}

else:
    # Old format: convert to new format for backward compatibility
    config["LIBRARY_STRUCTURE"] = "old"
    config["libraries"] = {"hic": {}, "chi": {}, "ill": {}, "longreads": {}}
    config["LIB_NAMES"] = {"hic": [], "chi": [], "ill": [], "longreads": []}

    # Convert old BAM paths to new format
    if "HIC_BAM" in config and config["HIC_BAM"] != "":
        config["libraries"]["hic"]["hic_lib1"] = {"bam": config["HIC_BAM"]}
        config["LIB_NAMES"]["hic"] = ["hic_lib1"]

    if "CHI_BAM" in config and config["CHI_BAM"] != "":
        config["libraries"]["chi"]["chi_lib1"] = {"bam": config["CHI_BAM"]}
        config["LIB_NAMES"]["chi"] = ["chi_lib1"]

    if "ILL_BAM" in config and config["ILL_BAM"] != "":
        config["libraries"]["ill"]["ill_lib1"] = {"bam": config["ILL_BAM"]}
        config["LIB_NAMES"]["ill"] = ["ill_lib1"]
    
    if "LONGREAD_BAM" in config and config["LONGREAD_BAM"] != "":
        config["libraries"]["longreads"]["longreads_lib1"] = {"bam": config["LONGREAD_BAM"]}
        config["LIB_NAMES"]["longreads"] = ["longreads_lib1"]

# Validate BAM file paths - check for leading/trailing whitespace
for libtype in config["libraries"]:
    for libname in config["libraries"][libtype]:
        bam_path = config["libraries"][libtype][libname]["bam"]
        if bam_path != bam_path.strip():
            raise ValueError(
                f"\n\nERROR: BAM file path has leading or trailing whitespace!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{bam_path}'\n"
                f"Fixed path should be: '{bam_path.strip()}'\n\n"
                f"Please fix your config file and remove the extra whitespace."
            )

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi", "ill", "longreads"]:
    if libtype in config["LIB_NAMES"] and len(config["LIB_NAMES"][libtype]) == 0:
        del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    datatype="[A-Za-z]+"

rule all:
    input:
        expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
               chrom = config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
               chrom = config["CHROMS"]),

        config["tool"] + "/output/final_output/complete_assembly.hap.gz",
        config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt",
        config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        expand(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz", chrom = config["CHROMS"]),

        # also generate haplotagged bam files:
        expand(config["tool"] + "/output/final_output/{libtype}_haplotagged.bam",
               libtype = config["ACTIVE_LIBTYPES"]),
        expand(config["tool"] + "/output/final_output/{libtype}_haplotagged.bam.bai",
               libtype = config["ACTIVE_LIBTYPES"])


######################################################################
#  Make a softlink of the assembly
######################################################################
rule make_assem_softlink:
    input:
        ref = config["REFERENCE"],
    output:
        assem = config["tool"] + "/input/assembly/input.fasta"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        ln -s {input.ref} {output.assem}
        """

######################################################################
#   Make symlinks of the input BAM files for each library
######################################################################

rule make_library_bam_softlink:
    """
    Create symlinks for BAM files for each library.
    All library types (Hi-C, Chicago, Illumina, long reads) now use BAM files.
    """
    output:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam"
    params:
        bam = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["bam"]
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -sf {params.bam} {output.bam}
        """

def get_add_rg_runtime(wildcards, input):
    """
    Calculate runtime for AddOrReplaceReadGroups based on input BAM size.
    
    Picard AddOrReplaceReadGroups needs to read and write the entire BAM file.
    Based on empirical data: 3.3 GB BAM takes ~60 minutes (close call)
    
    Conservative estimate: ~55 MB/min read+write throughput on network storage
    Formula: (BAM_size_GB * 1024) / 55 = minutes
    Add 50% safety buffer for cluster load and network I/O variance
    
    Minimum: 30 minutes
    Maximum: 360 minutes (6 hours)
    """
    try:
        bam_size_bytes = os.path.getsize(input.bam)
        bam_size_gb = bam_size_bytes / (1024**3)
        
        # 3.3 GB takes ~60 min, so ~55 MB/min throughput
        # Convert GB to MB: bam_size_gb * 1024
        estimated_minutes = (bam_size_gb * 1024) / 55
        
        # Add 50% safety buffer
        runtime = int(estimated_minutes * 1.5)
        
        # Minimum 30 minutes, maximum 360 minutes (6 hours)
        return max(30, min(runtime, 360))
    except Exception as e:
        print(f"Warning: Could not calculate AddRG runtime for {wildcards}: {e}")
        return 90  # Safe default: 1.5 hours

def get_merge_bam_runtime(wildcards, input):
    """
    Calculate runtime for samtools merge based on total input BAM sizes.
    
    samtools merge reads all input BAMs and writes one output BAM.
    With 4 threads, this is I/O bound on network storage.
    
    Conservative estimate: ~100 MB/min total throughput with 4 threads
    Add 50% safety buffer for cluster load
    
    Minimum: 30 minutes
    Maximum: 360 minutes (6 hours)
    """
    try:
        total_size_bytes = sum(os.path.getsize(bam) for bam in input.bams)
        total_size_gb = total_size_bytes / (1024**3)
        
        # ~100 MB/min with 4 threads
        estimated_minutes = (total_size_gb * 1024) / 100
        
        # Add 50% safety buffer
        runtime = int(estimated_minutes * 1.5)
        
        # Minimum 30 minutes, maximum 360 minutes (6 hours)
        return max(30, min(runtime, 360))
    except Exception as e:
        print(f"Warning: Could not calculate merge runtime for {wildcards}: {e}")
        return 120  # Safe default: 2 hours

def get_split_bam_index_runtime(wildcards, input):
    """
    Calculate runtime for indexing split BAM files based on file size.
    
    samtools index with 2 threads on network storage.
    Based on empirical observation: need more time for large files.
    
    Conservative estimate: ~70 MB/min effective throughput with 2 threads
    Add 50% safety buffer for cluster load
    
    Minimum: 30 minutes (for files up to ~2 GB)
    Maximum: 360 minutes (6 hours for very large chromosome splits)
    """
    try:
        bam_size_bytes = os.path.getsize(input.bam)
        bam_size_gb = bam_size_bytes / (1024**3)
        
        # ~70 MB/min with 2 threads
        estimated_minutes = (bam_size_gb * 1024) / 70
        
        # Add 50% safety buffer
        runtime = int(estimated_minutes * 1.5)
        
        # Minimum 30 minutes, maximum 360 minutes (6 hours)
        return max(30, min(runtime, 360))
    except Exception as e:
        print(f"Warning: Could not calculate split BAM index runtime for {wildcards}: {e}")
        return 90  # Safe default: 1.5 hours

def get_hapcut2_memory(wildcards, input):
    """
    Calculate memory for HapCUT2 based on fragment file size.
    
    HapCUT2 memory usage scales with:
    - Number of fragments (read depth Ã— chromosome size)
    - Complexity of Hi-C contact graph
    
    Empirical observation: Fragment files can be large with deep coverage.
    Conservative estimate: ~2 GB baseline + ~3 GB per GB of fragment file
    
    Minimum: 8 GB (for small chromosomes with low coverage)
    Maximum: 64 GB (for large chromosomes with deep Hi-C coverage)
    """
    try:
        frag_size_bytes = os.path.getsize(input.frag)
        frag_size_gb = frag_size_bytes / (1024**3)
        
        # Base memory + scaling factor
        # 2 GB baseline + 3 GB per GB of fragment file
        memory_mb = int(2000 + (frag_size_gb * 3000))
        
        # Minimum 8 GB, maximum 64 GB
        return max(8000, min(memory_mb, 64000))
    except Exception as e:
        print(f"Warning: Could not calculate HapCUT2 memory for {wildcards}: {e}")
        return 12000  # Safe default: 12 GB

def get_bam_index_runtime(wildcards):
    """
    Calculate runtime for BAM indexing based on file size.
    
    Conservative estimates for HPC with network storage:
    - Assume ~60-80 MB/sec effective throughput with 4 threads
    - Double the estimate for safety buffer on slow filesystems
    
    For a 247 GB file: ~3-4 hours estimated
    
    Note: We allocate time for full indexing even if .bai exists, because
    the shell script will detect it and copy quickly. This avoids issues
    with filesystem caching/permissions during DAG planning phase.
    
    Minimum: 15 minutes (enough for copy or small BAM index)
    """
    original_bam = config["libraries"][wildcards.libtype][wildcards.libname]["bam"]
    
    # Get BAM file size for runtime calculation
    try:
        bam_size_bytes = os.path.getsize(original_bam)
        bam_size_gb = bam_size_bytes / (1024**3)
        
        # Conservative estimate: ~70 MB/sec = ~0.07 GB/sec with 4 threads on HPC
        # Time in seconds = size_gb / 0.07, convert to minutes
        estimated_minutes = (bam_size_gb / 0.07) / 60
        
        # Double it for safety on network filesystems and cluster load
        runtime = int(estimated_minutes * 2.0)
        
        # Minimum 15 minutes, maximum 720 minutes (12 hours)
        return max(15, min(runtime, 720))
    except:
        # If we can't get size, use a safe default
        return 180

rule index_library_bam:
    """
    Index the symlinked BAM files for each library.
    First tries to copy existing .bai from original location, otherwise indexes from scratch.
    Runtime scales with BAM file size.
    """
    input:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam"
    output:
        bai = config["tool"] + "/input/bams/{libtype}.{libname}.bam.bai"
    params:
        original_bam = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["bam"],
        original_bai = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["bam"] + ".bai"
    threads: 4
    resources:
        mem_mb = 4000,
        runtime = get_bam_index_runtime
    shell:
        """
        set -euo pipefail
        
        # Debug: show what we're looking for
        echo "Original BAM: {params.original_bam}"
        echo "Looking for index: {params.original_bai}"
        
        # Try to copy existing index first
        if [ -f "{params.original_bai}" ]; then
            echo "Found existing index at {params.original_bai}, copying..."
            cp "{params.original_bai}" {output.bai}
            echo "Index copied successfully"
        else
            echo "No existing index found at {params.original_bai}"
            echo "Indexing BAM (this may take a while for large files)..."
            samtools index -@ {threads} {input.bam}
            echo "Indexing complete"
        fi
        """

######################################################################
#  First we deal with the input VCF file
#   - it could be that the VCF is not sorted correctly, so let's sort
#      it according to the input assembly
######################################################################

# get a picard dict of the reference sequences
rule picard_dict:
    """
    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        picard = config["PICARD"]
    output:
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        java -jar -Xmx1g {input.picard} CreateSequenceDictionary \
          -R {input.ref} \
          -O {output.seqdict}
        """

rule update_vcf_dict:
    """
    Update the header of the vcf file

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        seqdict = config["tool"] + "/input/ref/input.ref.dict",
        vcf = config["VCFfile"],
        picard = config["PICARD"]
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        java -jar -Xmx1g {input.picard} UpdateVcfSequenceDictionary \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# filter the vcf to remove entries that are not legal to hapcut2
rule filter_vcf:
    """
    In DeepVariant, it makes reference calls and annotates them as ./.
    These are not legal, so remove them.
     - ./. We remove these, since these are simply ref calls
     - 0/3 We remove these, since there are 3 alt alleles that will break HapCUT2
     - 1/3 This isn't valid because there are still multiple alleles to pick from
     - 0/2 We should probably remove this, since one is ref, and there are two options for the alt.
     - 1/2 ??
     - 0/2 ??
     - 2/3 ??
     - n/4 too many, just remove this
     - n/5 too many, just remove this
     - n/6 too many, just remove this
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 5
    shell:
        """
        awk 'BEGIN{{OFS="\t"}} /^#/ {{print; next}} $7 != "RefCall" {{print}}' {input.vcf} | \
            awk '/^#/ {{print; next}} $10 !~ /\\/3/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/4/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/5/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/6/ {{print}}' | \
            grep -v '\\./\\.' | \
            grep -v '0/2' > {output.vcf}
        """

# sort the vcf file based on the input assembly
rule sort_input_vcf:
    """
    If the input VCF isn't sorted correctly, then the downstream steps
     won't work.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf",
        picard  = config["PICARD"],
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 30
    shell:
        """
        java -jar -Xmx3g {input.picard} SortVcf \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# compress the vcf file
rule split_vcf_into_chr_zip:
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    output:
        tvcf = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        bgzip -c {input.vcf} > {output.tvcf}  #compress vcf
        """

# index the vcf file
rule index_vcf:
    input:
        tvcf = config["tool"] + "/output/vcf/orig/temp.vcf.gz"
    output:
        index = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi"),
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        tabix -p vcf {input.tvcf}  # index compressed vcf
        """

rule list_of_chroms_from_reference:
    input:
        fai = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    shell:
        """
        # save all the chromosome names into a file
        cut -f1 {input.fai} > {output.chrtxt}
        """

rule list_of_chroms_from_config:
    """
    The point of this is to get the list of chromosomes that we will keep for phasing.

    Notes:
      - Previously, this would fail sometimes, even if there were perfect matches
    """
    input:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt",
    output:
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    run:
        # first, print out the chromosomes so we are sure of what we're looking for
        print("The chromosomes we are looking for are:")
        CHROMLIST = [str(chrom) for chrom in config["CHROMS"]]
        print(CHROMLIST)

        matches = []
        outhandle = open(output.chrtxt, "w")
        with open(input.chrtxt, "r") as f:
            for line in f:
                line = str(line.strip())
                if line:
                    if line in CHROMLIST:
                        matches.append(line)
        # if there were no matches, state as much and raise an error. We should have found something
        if len(matches) == 0:
            print("No matching chromosomes found.", file=outhandle)
            raise ValueError("No matching chromosomes found.")

        # If we found matches, write them to the output file
        for line in matches:
            print(line, file=outhandle)

# split all vcf files by chromosome
rule split_vcf_into_chr_remainder:
    input:
        gz = config["tool"] + "/output/vcf/orig/temp.vcf.gz",
        index = config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi",
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    output:
        vcf  = expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"])
    params:
        outstub = config["tool"] + "/output/vcf/orig/split"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        while IFS= read -r line; do
          tabix {input.gz} $line > {params.outstub}/$line.vcf;
        done < {input.chrtxt}
        """

######################################################################
#  Now handle the read mapping
######################################################################

def get_bwa_index_runtime(wildcards, input):
    """
    Calculate appropriate runtime for BWA indexing based on genome size.
    Reads the .fai file to get total genome size in bases.
    
    The .fai file has format: chrom_name, length, offset, bases_per_line, bytes_per_line
    We sum the second column to get total genome size.
    
    Runtime scaling (empirically determined):
    - ~30 minutes per Gb for genomes < 1 Gb
    - ~45 minutes per Gb for genomes 1-3 Gb  
    - ~60 minutes per Gb for genomes > 3 Gb
    
    Minimum: 30 minutes
    """
    try:
        fai_file = input.fai
        total_bases = 0
        
        with open(fai_file, 'r') as f:
            for line in f:
                fields = line.strip().split('\t')
                if len(fields) >= 2:
                    total_bases += int(fields[1])
        
        # Convert to gigabases
        genome_size_gb = total_bases / 1_000_000_000
        
        # Scale runtime based on genome size (in minutes)
        if genome_size_gb < 1.0:
            runtime = max(30, int(genome_size_gb * 30))  # 30 min/Gb
        elif genome_size_gb < 3.0:
            runtime = int(genome_size_gb * 45)  # 45 min/Gb
        else:
            runtime = int(genome_size_gb * 60)  # 60 min/Gb
        
        # Add 20% buffer for safety
        runtime = int(runtime * 1.2)
        
        return runtime
    except Exception as e:
        # If anything fails, return a safe default
        print(f"Warning: Could not calculate BWA index runtime from fai file: {e}")
        return 180  # 3 hours default

# index reference genome
rule index_genome:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        fai = config["tool"] + "/input/assembly/input.fasta.fai",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt"
    threads: 1
    resources:
        mem_mb  = 16000,
        runtime = lambda wildcards, input: get_bwa_index_runtime(wildcards, input)
    shell:
        'bwa index {input.ref}'

rule index_genome_faidx:
    """
    Create fai index for the reference genome.
    First tries to copy existing .fai from original location, otherwise indexes from scratch.
    """
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        fai = config["tool"] + "/input/assembly/input.fasta.fai"
    params:
        original_ref = lambda wildcards: config["REFERENCE"]
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 10
    shell:
        """
        # Try to copy existing fai index first
        if [ -f "{params.original_ref}.fai" ]; then
            echo "Found existing fai index, copying..."
            cp "{params.original_ref}.fai" {output.fai}
        else
            echo "No existing fai index found, creating with samtools faidx..."
            samtools faidx {input.ref}
        fi
        """

######################################################################
# Split each library BAM by chromosome (before merging)
######################################################################

def get_split_bam_runtime(wildcards):
    """
    Calculate runtime for splitting BAM by chromosome based on file size.
    Large BAMs need more time to seek through and extract.
    
    Scaling: Small files get 60 min, large files (>100 GB) get 180 min
    """
    original_bam = config["libraries"][wildcards.libtype][wildcards.libname]["bam"]

    try:
        bam_size_bytes = os.path.getsize(original_bam)
        bam_size_gb = bam_size_bytes / (1024**3)

        # Scale from 60 to 180 minutes based on size
        # < 50 GB: 60 min
        # 50-100 GB: 90-120 min
        # > 100 GB: 180 min
        if bam_size_gb < 50:
            runtime = 60
        elif bam_size_gb < 100:
            runtime = 90 + int((bam_size_gb - 50) * 0.6)  # scale from 90 to 120
        else:
            runtime = 180  # 3 hours for large files

        return runtime
    except:
        return 120  # 2 hours default

rule split_library_by_chromosome:
    """
    Split each individual library BAM by chromosome.
    This is done before merging so we can add library-specific read groups.
    Requires the BAM index file to efficiently extract reads by chromosome.
    
    Runtime scales with BAM file size (large files take longer to seek through).
    """
    input:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam",
        bai = config["tool"] + "/input/bams/{libtype}.{libname}.bam.bai"
    output:
        bam = temp(config["tool"] + "/output/bams/split_per_library/{libtype}/{libtype}.{libname}.REF_{chrom}.bam")
    threads: 4
    resources:
        mem_mb=8000, 
        runtime=get_split_bam_runtime
    shell:
        """
        set -euo pipefail
        samtools view -@ {threads} -bh {input.bam} "{wildcards.chrom}" -o {output.bam}
        """

rule add_library_rg_to_split:
    """
    Add library-specific read groups to each chromosome split.
    This is where library tracking is implemented.

    Tags added:
      - LB: Library name (e.g., DpnII, MboI, PacBio_run1)
      - ID: Read group ID (libtype.libname.chrom)
      - SM: Sample name (from config or default to 'sample1')
      - PL: Platform (LONG for all read types - could be PacBio, ONT, Hi-C, etc.)
    
    TODO: For performance improvement, implement local disk I/O:
      1. Copy input BAM to $TMPDIR or /scratch/local
      2. Run AddOrReplaceReadGroups on local copy
      3. Copy output back to final location
      This reduces network I/O latency for large BAM files.
    """
    input:
        bam =      config["tool"] + "/output/bams/split_per_library/{libtype}/{libtype}.{libname}.REF_{chrom}.bam",
        picard = config["PICARD"]
    output:
        bam = temp(config["tool"] + "/output/bams/split_per_library/{libtype}/bam.rg.{libtype}.{libname}.REF_{chrom}")
    params:
        libname = lambda wildcards: wildcards.libname,
        rgid = lambda wildcards: f"{wildcards.libtype}.{wildcards.libname}.{wildcards.chrom}",
        sample = lambda wildcards: config.get("SAMPLE_NAME", "sample1"),
        platform = lambda wildcards: str.upper(wildcards.libtype) if wildcards.libtype in ["ill", "chi", "hic"] else "LONG"
    threads: 2
    resources:
        mem_mb  = 4000,
        runtime = lambda wildcards, input: get_add_rg_runtime(wildcards, input)
    shell:
        """
        java -Xmx3g -jar {input.picard} AddOrReplaceReadGroups \
          -I {input.bam} -O {output.bam} \
          --RGID {params.rgid} \
          --RGLB {params.libname} \
          --RGPL {params.platform} \
          --RGPU unit1 \
          --RGSM {params.sample}
        """

def get_library_splits_for_chrom(wildcards):
    """Get all library BAM files for a given library type and chromosome"""
    libtype = wildcards.libtype
    chrom = wildcards.chrom
    bams = []
    
    for libname in config["LIB_NAMES"][libtype]:
        bams.append(config["tool"] + f"/output/bams/split_per_library/{libtype}/bam.rg.{libtype}.{libname}.REF_{chrom}")
    
    return bams

rule merge_libraries_per_chromosome:
    """
    Merge all libraries of the same type for a specific chromosome.
    Library information is preserved in the LB tag of each read.
    If only one library exists, this will just create a symlink.
    """
    input:
        bams = get_library_splits_for_chrom
    output:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
    threads: 4
    resources:
        mem_mb  = 8000,
        runtime = lambda wildcards, input: get_merge_bam_runtime(wildcards, input)
    run:
        if len(input.bams) == 1:
            # Only one library, just copy it (safer than symlink with temp files)
            shell("cp {input.bams[0]} {output.bam}")
        else:
            # Multiple libraries, merge them (preserves read groups)
            shell("samtools merge -@ {threads} {output.bam} {input.bams}")

# mark for deletion
#rule index_bams:
#    input:
#        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam"
#    output:
#        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
#    threads: 1
#    resources:
#        mem_mb  = 8000,
#        runtime = 120
#    shell:
#        """
#        samtools index {input.bam}
#        """

######################################################################
# Index and validate split BAMs
######################################################################

rule index_split_bam:
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
    output:
        bai = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam.bai"
    threads: 2
    resources:
        mem_mb=8000,
        runtime=lambda wildcards, input: get_split_bam_index_runtime(wildcards, input)
    shell:
        """
        set -euo pipefail
        # Try to index; if the shard is empty and samtools refuses, create a placeholder
        # so downstream rules depending on the .bai can still proceed.
        samtools index -@ {threads} {input.bam} {output.bai} || touch {output.bai}
        """

# we don't need this anymore since we added RGs in merge_libraries_per_chromosome
#rule pass_through_rg:
#    """
#    Pass-through rule for compatibility with downstream rules.
#    Read groups are already set in merge_libraries_per_chromosome,
#    so we just create a symlink.
#    """
#    input:
#        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
#    output:
#        bam = temp(config["tool"] + "/output/bams/split/{libtype}.rg.REF_{chrom}.bam")
#    threads: 1
#    resources:
#        mem_mb  = 1000,
#        runtime = 5
#    shell:
#        """
#        ln -sf $(realpath {input.bam}) {output.bam}
#        """

rule rm_duplicates_in_split_bams:
    """
    This removes duplicates for chicago, illumina, and hi-c using sambamba.

    Sambamba markdup is faster than Picard and removes duplicates by default with -r flag.
    Uses multiple threads for better performance.
    """
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        sambamba = os.path.join(filepath, "../bin/sambamba")
    output:
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam"
    threads: 4
    resources:
        mem_mb  = 8000,
        runtime = 60  # minutes - sambamba is faster than Picard
    shell:
        """
        {input.sambamba} markdup -r -t {threads} {input.bam} {output.bam}
        """

rule index_rmdup_bams:
    input:
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam",
    output:
        bai = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam.bai",
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Extract the hairs from the different data types
######################################################################

# convert Hi-C bam files to haplotype fragment files
rule extract_hairs:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        fai = config["tool"] + "/input/assembly/input.fasta.fai",
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam",
        bai = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam.bai",
        vcf = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        frag = config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{chrom}.frag"
    threads: 1
    params:
        fragtype = lambda wildcards: wildcards.libtype
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        if [ "{params.fragtype}" == "longreads" ]; then
            extractHAIRS --pacbio 1 --new_format 1 --indels 1 \
               --bam {input.bam} --ref {input.ref} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "hic" ]; then
            extractHAIRS --hic 1 --new_format 1 \
               --indels 1 --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "chi" ]; then
            extractHAIRS --maxIS 10000000 --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "ill" ]; then
            extractHAIRS --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        fi
        """

rule concatenate_HAPCUT2_evidence:
    input:
        frags = expand(config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{{chrom}}.frag",
                       libtype = config["ACTIVE_LIBTYPES"])
    output:
        cat = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        cat {input.frags} > {output.cat}
        """

######################################################################
# Run HAPCUT2 - once per scaffold
######################################################################

# run HapCUT2 to assemble haplotypes from combined Hi-C + longread haplotype fragments
rule run_hapcut2_hic_longread:
    input:
        frag = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
        vcf  = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        hap   = config["tool"] + "/output/hapcut2_results/{chrom}.hap",
        vcf   = config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF"
    params:
        thischrom = lambda wildcards: wildcards.chrom,
        model = config["tool"] + "/output/hapcut2_results/{chrom}.htrans_model"
    threads: 1
    resources:
        mem_mb  = lambda wildcards, input: get_hapcut2_memory(wildcards, input),
        runtime = 120
    shell:
        """
        HAPCUT2 --fragments {input.frag} --vcf {input.vcf} \
            --output {output.hap} \
            --hic 1 \
            --htrans_data_outfile {params.model} \
            --outvcf 1
        """

######################################################################
# Once we have the HAPCUT2 output, put it in a useful format
#
# The rules cat_haps and merge_vcfs were previously in one rule.
#  I encountered some errors so split them up.
######################################################################

rule cat_haps:
    """
    This pools all of the haplotype files that we generated previously.
    """
    input:
        hap = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
                     chrom=config["CHROMS"])
    output:
        hap = temp(config["tool"] + "/output/final_output/complete_assembly.hap")
    threads: 1
    resources:
        mem_mb=1000, runtime=10
    shell:
        r"""
        mkdir -p {config[tool]}/output/final_output
        cat {input.hap} > {output.hap}
        """

rule merge_vcfs:
    """
    202508 - This file isn't used for anything
    """
    input:
        origvcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf",
        vcfs    = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF",
                         chrom=config["CHROMS"])
    output:
        merged = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.unsorted.vcf")
    threads: 2
    resources:
        mem_mb=4000, runtime=30
    shell:
        r"""
        set -euo pipefail
        mkdir -p {config[tool]}/output/final_output

        # Merge all per-chrom VCFs into one (keep all sites, even if contigs differ)
        bcftools concat -a -D -O v -o {output.merged} {input.vcfs}

        # If you need to ensure the header carries over metadata from origvcf,
        # you can reheader (optional). Usually concat keeps a sane header:
        # bcftools annotate --header-lines {input.origvcf} -O v -o {output.merged}.tmp {output.merged}
        # mv {output.merged}.tmp {output.merged}
        """

rule sort_merged_vcf:
    """
    Sort the merged phased VCF by chromosome and position.
    This completes the workflow that was split from the old sort_final_vcf rule.
    """
    input:
        unsorted = config["tool"] + "/output/final_output/complete_assembly.hap.phased.unsorted.vcf"
    output:
        sorted_vcf = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf")
    threads: 2
    resources:
        mem_mb=4000, runtime=30
    shell:
        """
        bcftools sort -O v -o {output.sorted_vcf} {input.unsorted}
        """

rule get_largest_hap_blocks:
    """
    These are the "most variants phased" (MVP) haplotype blocks.
    This term was first used in Selvaraj et al., Nature Biotechnology (2013)
    This is likely the block that covers most of the chromosome, as it contains most of the variants
      that were phased.
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        biggest_blocks = {}
        this_block_header = ""
        this_block_c = ""
        this_block_start = -1
        this_block_stop = -1
        with open(input.hap, "r") as f:
            for line in f:
                line = line.replace("*", "").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        # we have just found a new block
                        if not this_block_header == "":
                            # not the first, add an entry to biggest_blocks
                            span = int(this_block_stop) - int(this_block_start) + 1
                            add_this = False
                            if this_block_c not in biggest_blocks:
                                add_this = True
                            else:
                                if span > biggest_blocks[this_block_c]["span"]:
                                    add_this = True
                            if add_this:
                                biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
                        this_block_start = -1
                        this_block_stop = -1
                        this_block_header = line
                    else:
                        if this_block_start == -1:
                            this_block_start = splitd[4]
                            this_block_stop = splitd[4]
                            this_block_c     = splitd[3]
                        else:
                            this_block_stop = splitd[4]
        # final parsing at the end
        add_this = False
        if this_block_c not in biggest_blocks:
            add_this = True
        else:
            if span > biggest_blocks[this_block_c]["span"]:
                add_this = True
        if add_this:
            biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
        with open(output.txt, "w") as f:
            for key in biggest_blocks:
                print("{}\t{}\t\"{}\"".format(
                      key,
                      biggest_blocks[key]["span"],
                      biggest_blocks[key]["name"]), file = f)

rule hap_blocks_to_vcf:
    """
    This converts the haplotype blocks to a vcf
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf",
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    output:
        final_vcf = temp(config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        # first get the headers that we want
        headers = set()
        with open(input.txt, "r") as f:
            for line in f:
                line = line.split("\"")[1]
                headers.add(line)
        # now get the sites that we will keep
        #  key is chr, set of sites
        keepers = {}
        with open(input.hap, "r") as f:
            capturing = False
            for line in f:
                line=line.replace('*',"").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        if line in headers:
                            capturing = True
                        else:
                            capturing = False
                    else:
                        if capturing:
                            this_c = splitd[3]
                            this_s = int(splitd[4])
                            if this_c not in keepers:
                                keepers[this_c] = set()
                            keepers[this_c].add(this_s)
        # now get the sites that are in the phase blocks
        writeme = open(output.final_vcf, "w")
        with open(input.vcf, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    if line[0] == '#':
                        print(line, file = writeme)
                    else:
                        # not a comment
                        splitd = line.split()
                        this_c = splitd[0]
                        this_s = int(splitd[1])
                        if this_c in keepers:
                            if this_s in keepers[this_c]:
                                print(line, file = writeme)
        writeme.close()

######################################################################
#   Compress the output
######################################################################
rule compress_final_output:
    input:
        final_vcf = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf"
    output:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        # compress
        bgzip -c {input.final_vcf} > {output.final_gz}
        # index
        tabix -p vcf {output.final_gz}
        """

rule gzip_hapcut2_vcf:
    input:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        tbi = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        bgzip -c {input.vcf} > {output.vcf}
        # index
        tabix -p vcf {output.vcf}
        """

rule gzip_complete_assembly_hap_file:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap"
    output:
        gzipped = config["tool"] + "/output/final_output/complete_assembly.hap.gz"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        cat {input.hap} | gzip > {output.gzipped}
        """

########################################################################
#    Now split this into different chromosomes and make the final table
########################################################################


rule make_table_of_final_output:
    """
    This makes a table of the results from phasing. Useful for plotting.
    The columns included are:
      - chrom (the name from the fasta header)
      - chrom_length (the sequence length)
      - largest_pb_start (1-based start index for the largest phase block)
      - largest_pb_stop  (1-based stop index for the largest phase block)
      - largest_pb_length (the size of the largest phase block)
      - largest_pb_num_var (the number of variants in the largest phase block)
      - largest_pb_num_phased (the number of phased variants in the largest phase block)
    """
    input:
        txt   = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        vcfgz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        tbi   = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi",
        ref   = config["tool"] + "/input/assembly/input.fasta"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt"
    threads: 1
    resources:
        mem_mb=2000, runtime=30
    run:
        import subprocess
        from Bio import SeqIO
        from contextlib import ExitStack

        chrom_to_length = {}

        with ExitStack() as stack:
            ref_handle  = stack.enter_context(open(input.ref, "r"))
            out_handle  = stack.enter_context(open(output.txt, "w"))
            txt_handle  = stack.enter_context(open(input.txt, "r"))

            # chrom sizes from FASTA
            for record in SeqIO.parse(ref_handle, "fasta"):
                chrom_to_length[record.id] = len(record.seq)

            print("\t".join([
                "chrom","chrom_length","largest_pb_start","largest_pb_stop",
                "largest_pb_length","percent_of_chrom_in_largest_pb",
                "largest_pb_num_var","largest_pb_num_phased","percent_of_vars_phased"
            ]), file=out_handle)

            for raw in txt_handle:
                line = raw.strip()
                if not line:
                    continue

                splitd = line.split("\t")
                chrom = splitd[0]
                chrom_length = chrom_to_length[chrom]

                # Use tabix/bcftools random access to fetch first/last POS
                cmd_base   = f"bcftools view -r {chrom} -H {input.vcfgz} | cut -f2"
                start_str  = subprocess.check_output(f"{cmd_base} | head -n1", shell=True, text=True).strip()
                stop_str   = subprocess.check_output(f"{cmd_base} | tail -n1", shell=True, text=True).strip()

                largest_pb_start = int(start_str) if start_str else 0
                largest_pb_stop  = int(stop_str)  if stop_str  else 0
                largest_pb_length = max(0, largest_pb_stop - largest_pb_start + 1)

                largest_pb_num_var    = int(splitd[2].split()[4])
                largest_pb_num_phased = int(splitd[2].split()[6])

                percent_of_chrom_in_largest_pb = f"{(largest_pb_length/chrom_length)*100:.4f}" if chrom_length else "0.0000"
                percent_of_vars_phased = f"{(largest_pb_num_phased/largest_pb_num_var)*100:.2f}" if largest_pb_num_var else "0.00"

                print("\t".join(map(str, [
                    chrom,
                    chrom_length,
                    largest_pb_start,
                    largest_pb_stop,
                    largest_pb_length,
                    percent_of_chrom_in_largest_pb,
                    largest_pb_num_var,
                    largest_pb_num_phased,
                    percent_of_vars_phased
                ])), file=out_handle)

########################################################################
#    Now we use WhatsHap to haplotag the Hi-C and long reads
########################################################################

rule split_phased_into_individual_chromosomes:
    input:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    output:
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 20
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        bcftools view -r {wildcards.chrom} -Oz -o {output.chromgz} {input.final_gz}
        """

rule index_vcf_individual_chromosomes:
    input:
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz"
    output:
        index   = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 20
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        tabix -p vcf {input.chromgz}
        """

def bam_size_mb(path: str) -> int:
    try:
        return max(1, int(Path(path).stat().st_size // (1024*1024)))
    except FileNotFoundError:
        return 1

def mem_for_bam_mb(path: str, floor=8000, ceil=65536, pct=0.10, base=1024) -> int:
    """Generic scaler: base + pct * size (MB), clamped."""
    mb = bam_size_mb(path)
    return int(max(floor, min(ceil, base + pct * mb)))

def mem_for_hic_piecewise_mb(path: str) -> int:
    """Conservative RAM for Hi-C shards based on your observed failures."""
    gb = bam_size_mb(path) / 1024
    if gb <= 4:   return 32000   # 32 GB
    if gb <= 8:   return 40000   # 40 GB
    if gb <= 10:  return 64000   # 64 GB  (40 GB failed >8 GB)
    if gb <= 14:  return 80000   # 80 GB
    if gb <= 18:  return 96000   # 96 GB
    if gb <= 22:  return 128000  # 128 GB (future ~20 GB shards)
    return 160000                # safety cap

def mem_for_libtype_mb(libtype: str, path: str) -> int:
    """Hi-C uses piecewise; long reads get a modest scaler; others lighter."""
    lt = (libtype or "").lower()
    if lt == "hic":
        return mem_for_hic_piecewise_mb(path)
    if lt in {"hifi","pb","pacbio","ont","nanopore","clr","ul"}:
        # Long reads: slightly higher floor and slope than Illumina
        return mem_for_bam_mb(path, floor=12000, ceil=98304, pct=0.15, base=1024)
    # Illumina/other: lighter defaults
    return mem_for_bam_mb(path, floor=8000, ceil=65536, pct=0.10, base=512)

def runtime_for_bam_min(path: str, base=20, per_gb=1.0, ceil=600) -> int:
    size_gb = bam_size_mb(path) / 1024
    return int(max(base, min(ceil, base + per_gb * size_gb)))

rule haplotag_reads_chrom:
    input:
        bam    = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        bai    = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam.bai",
        vcfgz  = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz",
        vcfin  = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi",
        # index may not exist for empty shard; keep optional in logic
        ref    = config["tool"] + "/input/assembly/input.fasta",
        fai    = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        bam = config["tool"] + "/output/final_output/haplotagged_by_chrom/{libtype}.REF_{chrom}.haplotagged.bam",
        bai = config["tool"] + "/output/final_output/haplotagged_by_chrom/{libtype}.REF_{chrom}.haplotagged.bam.bai"
    threads: 1
    resources:
        # Bigger floor for Hi-C, modest floor for others
        mem_mb  = lambda wc, input: mem_for_libtype_mb(wc.libtype, input.bam),
        runtime = lambda wc, input: runtime_for_bam_min(input.bam, base=45, per_gb=10, ceil=2160),
        slurm_cpus_per_task = lambda wc, threads: threads,
    shell:
        """
        set -euo pipefail

        # Empty or invalid BAM shard? Emit header-only BAM.
        if ! samtools quickcheck -v {input.bam} >/dev/null 2>&1 || \
           [ "$(samtools view -c {input.bam})" -eq 0 ]; then
            echo "[haplotag_reads_chrom] {wildcards.chrom}: empty BAM shard; writing header-only BAM."
            samtools view -H {input.bam} | samtools view -b -o {output.bam} -
            samtools index -@ {threads} {output.bam}
            exit 0
        fi

        # No variants in this VCF shard? Just pass the BAM through unchanged.
        if [ "$(bcftools view -H {input.vcfgz} | wc -c)" -eq 0 ]; then
            echo "[haplotag_reads_chrom] {wildcards.chrom}: no variants; copying BAM shard."
            cp -p {input.bam} {output.bam}
            if [ -f {input.bai} ]; then cp -p {input.bai} {output.bai}; else samtools index -@ {threads} {output.bam}; fi
            exit 0
        fi

        # Otherwise, haplotag normally.
        whatshap haplotag \
            -o {output.bam} \
            --reference {input.ref} \
            --ignore-read-groups \
            --ploidy 2 \
            --output-threads {threads} \
            {input.vcfgz} \
            {input.bam}

        samtools index -@ {threads} {output.bam}
        """


def total_bam_size_mb(paths) -> int:
    tot = 0
    for p in paths:
        try: tot += int(Path(p).stat().st_size // (1024*1024))
        except FileNotFoundError: pass
    return max(1, tot)

def runtime_for_merge_minutes(libtype: str, paths,
                              hic_base=45, hic_per_gb=1.2, hic_ceil=2160,
                              pb_base=45,  pb_per_gb=0.8,  pb_ceil=720) -> int:
    gb = total_bam_size_mb(paths) / 1024
    lt = (libtype or "").lower()
    if lt == "hic":
        return int(min(hic_ceil, max(hic_base, hic_base + hic_per_gb * gb)))
    # PacBio / long reads default
    return int(min(pb_ceil, max(pb_base, pb_base + pb_per_gb * gb)))

rule merge_haplotagged:
    input:
        bams = expand(config["tool"] + "/output/final_output/haplotagged_by_chrom/{{libtype}}.REF_{chrom}.haplotagged.bam",
               chrom=config["CHROMS"])
    output:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam"
    threads: 8
    resources:
        mem_mb  = 8000,  # merge is light on RAM
        runtime = lambda wc, input: runtime_for_merge_minutes(wc.libtype, input.bams)
    shell:
        """
        samtools merge -@ {threads} -o {output.bam} {input}
        """

rule index_haplotagged:
    input:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam"
    output:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam.bai"
    threads: 1
    resources:
        mem_mb=8000,
        runtime = lambda wc, input: runtime_for_bam_min(input.bam, base=15, per_gb=1.5, ceil=600)
    shell:
        """
        samtools index -@ {threads} {input.bam}
        """