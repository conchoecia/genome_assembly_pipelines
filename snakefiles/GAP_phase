"""
This snakemake file is used for phasing a genome given a set of input data.
If some of the data types are missing, that is OK.
Uses HAPCUT2.

Script inspired by:
# author: Peter Edge
# 12/19/2016
# email: pedge@eng.ucsd.edu

Build history:
  - Updated in August 2025 to better work with SLURM and the new version of Picard.
    Now it is on snakemake 9.
  - Heavily modified by DTS in 2020-2021 dts@ucsc.edu. Written using snakemake 5.10.0

"""

import sys

configfile: "config.yaml"
config["tool"] = "GAP_phase"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

# if Illumina data missing
if config["ILL_R1"] == "" or config["ILL_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem, exist_ok=True)
    config["ILL_R1"] = outstem + "/ill_R1.fastq.gz"
    config["ILL_R2"] = outstem + "/ill_R2.fastq.gz"
    if not os.path.exists(config["ILL_R1"]):
        os.mknod(config["ILL_R1"])
    if not os.path.exists(config["ILL_R2"]):
        os.mknod(config["ILL_R2"])

if config["CHI_R1"] == "" or config["CHI_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["CHI_R1"] = outstem + "/chi_R1.fastq.gz"
    config["CHI_R2"] = outstem + "/chi_R2.fastq.gz"
    if not os.path.exists(config["CHI_R1"]):
        os.mknod(config["CHI_R1"])
    if not os.path.exists(config["CHI_R2"]):
        os.mknod(config["CHI_R2"])

if config["HIC_R1"] == "" or config["HIC_R2"] == "":
    outstem = config["tool"] + "/input/reads"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["HIC_R1"] = outstem + "/hic_R1.fastq.gz"
    config["HIC_R2"] = outstem + "/hic_R2.fastq.gz"
    if not os.path.exists(config["HIC_R1"]):
        os.mknod(config["HIC_R1"])
    if not os.path.exists(config["HIC_R2"]):
        os.mknod(config["HIC_R2"])

if config["LONGREAD_BAM"] == "":
    outstem = config["tool"] + "/output/bams/temp"
    if not os.path.exists(outstem):
        os.makedirs(outstem)
    config["longread_bam"] = outstem + "/longreads_sorted.bam"
    if not os.path.exists(config["longread_bam"]):
        os.mknod(config["longread_bam"])

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    datatype="[A-Za-z0-9|.]+",
    libtype="[A-Za-z]+"

rule all:
    input:
        expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"]),
        expand(config["tool"] + "/output/bams/temp/{libtype}_sorted.bam",
               libtype = ["ill", "chi", "hic", "longreads"]),
        expand(config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai",
               libtype = ["ill", "chi", "hic", "longreads"]),
        expand(config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
               chrom = config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
               chrom = config["CHROMS"]),

        config["tool"] + "/output/final_output/complete_assembly.hap.gz",
        config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt",
        config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        expand(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz", chrom = config["CHROMS"])

######################################################################
#  Make a softlink of the assembly
######################################################################
rule make_assem_softlink:
    input:
        ref = config["REFERENCE"],
    output:
        assem = config["tool"] + "/input/assembly/input.fasta"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        ln -s {input.ref} {output.assem}
        """

######################################################################
#   Make symlinks of the input reads
######################################################################

rule make_ill_softlink:
    """Just symlink the reads to not occupy more disk space."""
    params:
        reads1 = config["ILL_R1"],
        reads2 = config["ILL_R2"]
    output:
        R1 = config["tool"] + "/input/reads/ill_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/ill_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_chi_softlink:
    params:
        reads1 = config["CHI_R1"],
        reads2 = config["CHI_R2"]
    output:
        R1 = config["tool"] + "/input/reads/chi_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/chi_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_hic_softlink:
    params:
        reads1 = config["HIC_R1"],
        reads2 = config["HIC_R2"]
    output:
        R1 = config["tool"] + "/input/reads/hic_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/hic_R2.fastq.gz"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.reads1} {output.R1}
        ln -s {params.reads2} {output.R2}
        """

rule make_longreads_bam_softlink:
    params:
        longreads = config["LONGREAD_BAM"]
    output:
        bam = config["tool"] + "/output/bams/temp/longreads_sorted.bam"
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -s {params.longreads} {output.bam}
        """

######################################################################
#  First we deal with the input VCF file
#   - it could be that the VCF is not sorted correctly, so let's sort
#      it according to the input assembly
######################################################################

# get a picard dict of the reference sequences
rule picard_dict:
    """
    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        picard = config["PICARD"]
    output:
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        java -jar -Xmx1g {input.picard} CreateSequenceDictionary \
          -R {input.ref} \
          -O {output.seqdict}
        """

rule update_vcf_dict:
    """
    Update the header of the vcf file

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        seqdict = config["tool"] + "/input/ref/input.ref.dict",
        vcf = config["VCFfile"],
        picard = config["PICARD"]
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        java -jar -Xmx1g {input.picard} UpdateVcfSequenceDictionary \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# filter the vcf to remove entries that are not legal to hapcut2
rule filter_vcf:
    """
    In DeepVariant, it makes reference calls and annotates them as ./.
    These are not legal, so remove them.
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 5
    shell:
        """
        awk 'BEGIN{{OFS="\t"}} /^#/ {{print; next}} $7 != "RefCall" {{print}}' {input.vcf} | \
            grep -v '\\./\\.' > {output.vcf}
        """

# sort the vcf file based on the input assembly
rule sort_input_vcf:
    """
    If the input VCF isn't sorted correctly, then the downstream steps
     won't work.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf",
        picard  = config["PICARD"],
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 30
    shell:
        """
        java -jar -Xmx3g {input.picard} SortVcf \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# compress the vcf file
rule split_vcf_into_chr_zip:
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    output:
        tvcf = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        bgzip -c {input.vcf} > {output.tvcf}  #compress vcf
        """

# index the vcf file
rule index_vcf:
    input:
        tvcf = config["tool"] + "/output/vcf/orig/temp.vcf.gz"
    output:
        index = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi"),
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        tabix -p vcf {input.tvcf}  # index compressed vcf
        """

rule list_of_chroms_from_reference:
    input:
        fai = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    shell:
        """
        # save all the chromosome names into a file
        cut -f1 {input.fai} > {output.chrtxt}
        """

rule list_of_chroms_from_config:
    """
    The point of this is to get the list of chromosomes that we will keep for phasing.

    Notes:
      - Previously, this would fail sometimes, even if there were perfect matches
    """
    input:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt",
    output:
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    run:
        # first, print out the chromosomes so we are sure of what we're looking for
        print("The chromosomes we are looking for are:")
        CHROMLIST = [str(chrom) for chrom in config["CHROMS"]]
        print(CHROMLIST)

        matches = []
        outhandle = open(output.chrtxt, "w")
        with open(input.chrtxt, "r") as f:
            for line in f:
                line = str(line.strip())
                if line:
                    if line in CHROMLIST:
                        matches.append(line)
        # if there were no matches, state as much and raise an error. We should have found something
        if len(matches) == 0:
            print("No matching chromosomes found.", file=outhandle)
            raise ValueError("No matching chromosomes found.")

        # If we found matches, write them to the output file
        for line in matches:
            print(line, file=outhandle)

# split all vcf files by chromosome
rule split_vcf_into_chr_remainder:
    input:
        gz = config["tool"] + "/output/vcf/orig/temp.vcf.gz",
        index = config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi",
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    output:
        vcf  = expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"])
    params:
        outstub = config["tool"] + "/output/vcf/orig/split"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        while IFS= read -r line; do
          tabix {input.gz} $line > {params.outstub}/$line.vcf;
        done < {input.chrtxt}
        """

######################################################################
#  Now handle the read mapping
######################################################################

# index reference genome
rule index_genome:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt"
    threads: 1
    resources:
        mem_mb  = 16000,
        runtime = 120
    shell:
        'bwa index {input.ref}'

rule index_genome_faidx:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.fai"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 10
    shell:
        'samtools faidx {input.ref}'

# align HiC fastq file to reference
rule align_HiC_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/hic_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/hic_R2.fastq.gz"
    output:
        hic_bam = config["tool"] + "/output/bams/temp/hic_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} -5SPM {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.hic_bam}
        """

rule align_Chicago_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/chi_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/chi_R2.fastq.gz"
    output:
        chi_bam = config["tool"] + "/output/bams/temp/chi_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} -5SPM {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.chi_bam}
        """

rule align_Illumina_fastq:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        R1 = config["tool"] + "/input/reads/ill_R1.fastq.gz",
        R2 = config["tool"] + "/input/reads/ill_R2.fastq.gz"
    output:
        ill_bam = config["tool"] + "/output/bams/temp/ill_sorted.bam"
    resources:
        mem_mb  = 48000,
        runtime = 1440
    shell:
        """
        bwa mem -t {threads} {input.ref} {input.R1} {input.R2} | \
          samtools view -hb -@ {threads} | \
          samtools sort -@ {threads} - > {output.ill_bam}
        """

rule index_bams:
    input:
        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam"
    output:
        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
    threads: 1
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Split the chromosomes
######################################################################

# SPLIT bam files by chromosome
rule split_bams:
    input:
        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam",
        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
    output:
        bams = expand(config["tool"] + "/output/bams/split/{{libtype}}.REF_{chrom}.bam",
               chrom=config["CHROMS"])
    params:
        stub = config["tool"] + "/output/bams/split/{libtype}"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 60
    shell:
        """
        bamtools split -in {input.bam} -reference -stub {params.stub}

        for thisfile in {output.bams}; do
            if [ ! -f ${{thisfile}} ]; then
                samtools view -Hb {input.bam} > ${{thisfile}}
            fi
        done
        """

rule mark_duplicates_in_split_bams:
    """
    This marks the duplicates for chicago, illumina, and hi-c.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   --INPUT {input.bam}
    """
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        picard = config["PICARD"]
    output:
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
        metrics = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.metrics"
    threads: 1
    resources:
        mem_mb  = 8000,
        runtime = 30  # minutes
    shell:
        """
        java -jar {input.picard} MarkDuplicates \
          --READ_NAME_REGEX null \
          --INPUT {input.bam} \
          --OUTPUT {output.bam} \
          --METRICS_FILE {output.metrics} \
          --ASSUME_SORTED true
        """

rule index_markeddup_bams:
    input:
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
    output:
        bai = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam.bai",
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Extract the hairs from the different data types
######################################################################

# convert Hi-C bam files to haplotype fragment files
rule extract_hairs:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        fai = config["tool"] + "/input/assembly/input.fasta.fai",
        bam = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam",
        bai = config["tool"] + "/output/bams/split_markeddups/{libtype}.REF_{chrom}.markeddups.bam.bai",
        vcf = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        frag = config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{chrom}.frag"
    threads: 1
    params:
        fragtype = lambda wildcards: wildcards.libtype
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        if [ "{params.fragtype}" == "longreads" ]; then
            extractHAIRS --pacbio 1 --new_format 1 --indels 1 \
               --bam {input.bam} --ref {input.ref} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "hic" ]; then
            extractHAIRS --hic 1 --new_format 1 \
               --indels 1 --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "chi" ]; then
            extractHAIRS --maxIS 10000000 --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "ill" ]; then
            extractHAIRS --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        fi
        """

rule concatenate_HAPCUT2_evidence:
    input:
        frags = expand(config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{{chrom}}.frag",
                       libtype = ["ill", "chi", "hic", "longreads"])
    output:
        cat = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        cat {input.frags} > {output.cat}
        """

######################################################################
# Run HAPCUT2 - once per scaffold
######################################################################

# run HapCUT2 to assemble haplotypes from combined Hi-C + longread haplotype fragments
rule run_hapcut2_hic_longread:
    input:
        frag = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
        vcf  = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        hap   = config["tool"] + "/output/hapcut2_results/{chrom}.hap",
        vcf   = config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF"
    params:
        thischrom = lambda wildcards: wildcards.chrom,
        model = config["tool"] + "/output/hapcut2_results/{chrom}.htrans_model"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 120
    shell:
        """
        HAPCUT2 --fragments {input.frag} --vcf {input.vcf} \
            --output {output.hap} \
            --hic 1 \
            --htrans_data_outfile {params.model} \
            --outvcf 1
        """

######################################################################
# Once we have the HAPCUT2 output, put it in a useful format
######################################################################

rule sort_final_vcf:
    """
    This combines the vcf and hap files from hapcut2

    I found a trick here to avoid grep crashing in case of a nonmatch
    https://unix.stackexchange.com/questions/330660
    """
    input:
        origvcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf",
        vcf = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF",
                     chrom = config["CHROMS"]),
        hap = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
                     chrom = config["CHROMS"]),
    output:
        hap = temp(config["tool"] + "/output/final_output/complete_assembly.hap"),
        vcf = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf")
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 60
    shell:
        """
        set +e

        cat {input.origvcf} | grep '##fileformat'  > temp.vcf
        cat {input.origvcf} | grep '##filedate'    >> temp.vcf
        cat {input.origvcf} | grep '##source'      >> temp.vcf
        cat {input.origvcf} | grep '##reference'   >> temp.vcf
        cat {input.origvcf} | grep '##commandline' >> temp.vcf
        cat {input.origvcf} | grep '##contig'      >> temp.vcf
        cat {input.origvcf} | grep '##INFO'        >> temp.vcf
        cat {input.origvcf} | grep '##FORMAT'      >> temp.vcf
        cat {input.origvcf} | grep '#CHROM'        >> temp.vcf
        echo '##SAMPLE=<ID=NONE>'        >> temp.vcf

        cat {input.vcf} >> temp.vcf
        cat {input.hap} > {output.hap}
        cat temp.vcf | awk '$1 ~ /^#/ {{print $0;next}} {{print $0 | "sort -k1,1 -k2,2n"}}' > {output.vcf}
        rm temp.vcf
        set -e
        """

rule get_largest_hap_blocks:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        biggest_blocks = {}
        this_block_header = ""
        this_block_c = ""
        this_block_start = -1
        this_block_stop = -1
        with open(input.hap, "r") as f:
            for line in f:
                line = line.replace("*", "").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        # we have just found a new block
                        if not this_block_header == "":
                            # not the first, add an entry to biggest_blocks
                            span = int(this_block_stop) - int(this_block_start) + 1
                            add_this = False
                            if this_block_c not in biggest_blocks:
                                add_this = True
                            else:
                                if span > biggest_blocks[this_block_c]["span"]:
                                    add_this = True
                            if add_this:
                                biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
                        this_block_start = -1
                        this_block_stop = -1
                        this_block_header = line
                    else:
                        if this_block_start == -1:
                            this_block_start = splitd[4]
                            this_block_stop = splitd[4]
                            this_block_c     = splitd[3]
                        else:
                            this_block_stop = splitd[4]
        # final parsing at the end
        add_this = False
        if this_block_c not in biggest_blocks:
            add_this = True
        else:
            if span > biggest_blocks[this_block_c]["span"]:
                add_this = True
        if add_this:
            biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
        with open(output.txt, "w") as f:
            for key in biggest_blocks:
                print("{}\t{}\t\"{}\"".format(
                      key,
                      biggest_blocks[key]["span"],
                      biggest_blocks[key]["name"]), file = f)

rule hap_blocks_to_vcf:
    """
    This converts the haplotype blocks to a vcf
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf",
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    output:
        final_vcf = temp(config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        # first get the headers that we want
        headers = set()
        with open(input.txt, "r") as f:
            for line in f:
                line = line.split("\"")[1]
                headers.add(line)
        # now get the sites that we will keep
        #  key is chr, set of sites
        keepers = {}
        with open(input.hap, "r") as f:
            capturing = False
            for line in f:
                line=line.replace('*',"").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        if line in headers:
                            capturing = True
                        else:
                            capturing = False
                    else:
                        if capturing:
                            this_c = splitd[3]
                            this_s = int(splitd[4])
                            if this_c not in keepers:
                                keepers[this_c] = set()
                            keepers[this_c].add(this_s)
        # now get the sites that are in the phase blocks
        writeme = open(output.final_vcf, "w")
        with open(input.vcf, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    if line[0] == '#':
                        print(line, file = writeme)
                    else:
                        # not a comment
                        splitd = line.split()
                        this_c = splitd[0]
                        this_s = int(splitd[1])
                        if this_c in keepers:
                            if this_s in keepers[this_c]:
                                print(line, file = writeme)
        writeme.close()

######################################################################
#   Compress the output
######################################################################
rule compress_final_output:
    input:
        final_vcf = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf"
    output:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        # compress
        bgzip -c {input.final_vcf} > {output.final_gz}
        # index
        tabix -p vcf {output.final_gz}
        """

rule gzip_hapcut2_vcf:
    input:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        tbi = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        bgzip -c {input.vcf} > {output.vcf}
        # index
        tabix -p vcf {output.vcf}
        """

rule gzip_complete_assembly_hap_file:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap"
    output:
        gzipped = config["tool"] + "/output/final_output/complete_assembly.hap.gz"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        gzip {input.hap}
        """

########################################################################
#    Now split this into different chromosomes and make the final table
########################################################################

rule split_phased_into_individual_chromosomes:
    input:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    output:
        chroms = temp(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf"),
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz",
        index = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 60
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        bcftools filter -r {params.tchrom} {input.final_gz} > {output.chroms}
        # compress
        bgzip -c {output.chroms} > {output.chromgz}
        # index
        tabix -p vcf {output.chromgz}
        """

rule make_table_of_final_output:
    """
    This makes a table of the results from phasing. Useful for plotting.
    The columns included are:
      - chrom (the name from the fasta header)
      - chrom_length (the sequence length)
      - largest_pb_start (1-based start index for the largest phase block)
      - largest_pb_stop  (1-based stop index for the largest phase block)
      - largest_pb_length (the size of the largest phase block)
      - largest_pb_num_var (the number of variants in the largest phase block)
      - largest_pb_num_phased (the number of phased variants in the largest phase block)
    """
    input:
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        gzi = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        chroms = expand(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz", chrom = config["CHROMS"]),
        ref = config["tool"] + "/input/assembly/input.fasta"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        # get chrom sizes
        from Bio import SeqIO
        chrom_to_length = {}
        with open(input.ref, "rU") as handle:
            for record in SeqIO.parse(handle, "fasta"):
                chrom_to_length[record.id] = len(record.seq)

        outtxt = open(output.txt, "w")
        print("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(
               "chrom",
               "chrom_length",
               "largest_pb_start",
               "largest_pb_stop",
               "largest_pb_length",
               "percent_of_chrom_in_largest_pb",
               "largest_pb_num_var",
               "largest_pb_num_phased",
               "percent_of_vars_phased"),
               file=outtxt)
        with open(input.txt, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    splitd = line.split("\t")
                    chrom = splitd[0]
                    chrom_length = chrom_to_length[chrom]
                    gzvcf = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{}.vcf.gz".format(chrom)
                    find_start_command = "zcat {} | grep '{}' | grep -v '#' | head -1 | cut -f2".format(gzvcf, chrom)
                    find_stop_command = "zcat {} | grep '{}' | grep -v '#' | tail -1 | cut -f2".format(gzvcf, chrom)
                    out = subprocess.Popen(find_start_command,
                                           shell=True,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.STDOUT)
                    stdout,stderr = out.communicate()
                    largest_pb_start = int(stdout.decode("utf-8").strip().replace("\n",""))
                    out = subprocess.Popen(find_stop_command,
                                           shell=True,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.STDOUT)
                    stdout,stderr = out.communicate()
                    largest_pb_stop = int(stdout.decode("utf-8").strip().replace("\n",""))
                    #largest_pb_length_hapcut = int(splitd[2].split()[8])
                    largest_pb_length = largest_pb_stop - largest_pb_start + 1
                    largest_pb_num_var    = int(splitd[2].split()[4])
                    largest_pb_num_phased = int(splitd[2].split()[6])
                    percent_of_chrom_in_largest_pb = "{0:.4f}".format((largest_pb_length/chrom_length)*100)
                    percent_of_vars_phased = "{0:.2f}".format((largest_pb_num_phased/largest_pb_num_var)*100)
                    print("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}".format(
                           chrom,
                           chrom_length,
                           largest_pb_start,
                           largest_pb_stop,
                           largest_pb_length,
                           percent_of_chrom_in_largest_pb,
                           largest_pb_num_var,
                           largest_pb_num_phased,
                           percent_of_vars_phased),
                           file=outtxt)
        outtxt.close()
