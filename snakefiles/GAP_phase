"""
This snakemake file is used for phasing a genome given a set of input data.
If some of the data types are missing, that is OK.
Uses HAPCUT2.

Important notes for Hi-C data:
  - For Hi-C, the paired-end reads (fastq) should be aligned using BWA with the -5, -S,
    and -P options for optimal results with HapCUT2.
  - Reference: https://github.com/vibansal/HapCUT2/issues/122

Wildcard character restrictions:
  - libtype: Letters only [A-Za-z]+ (e.g., "hic", "chi", "ill", "longreads")
  - libname: Letters and numbers only [A-Za-z0-9]+ (e.g., "DpnII", "MboI", "lib1", "PacBio2")
  - datatype: Letters only [A-Za-z]+ (e.g., "hic", "chi", "ill", "longreads")
  
  Note: Underscores, hyphens, and other special characters are NOT allowed in library names.
        Use only alphanumeric characters to avoid snakemake wildcard matching issues.

Script inspired by:
# author: Peter Edge
# 12/19/2016
# email: pedge@eng.ucsd.edu

Build history:
  - Updated in November 2025 to improve robustness and resource allocation:
    * Added dynamic memory and runtime allocation based on BAM file sizes
    * Implemented piecewise memory scaling for Hi-C data (32-160 GB based on shard size)
    * Added handling for empty BAM shards in haplotagging step
    * Added checks for VCF files with no variants to avoid unnecessary processing
    * Improved merge and indexing steps with size-based runtime estimates
    * Enhanced error handling with samtools quickcheck for invalid BAM files
    * Split haplotagging by chromosome for better parallelization and resource control
  - Updated in August 2025 to better work with SLURM and the new version of Picard.
    Now it is on snakemake 9.
  - Heavily modified by DTS in 2020-2021 dts@ucsc.edu. Written using snakemake 5.10.0

"""

import sys
from pathlib import Path

configfile: "config.yaml"
config["tool"] = "GAP_phase"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# Check if using new library format
if "libraries" in config:
    # New format: config["libraries"][libtype][libname] = {bam: "/path/to/file.bam"}
    config["LIBRARY_STRUCTURE"] = "new"
    
    # Ensure config["libraries"] is a dict
    if not isinstance(config["libraries"], dict):
        config["libraries"] = {}
    
    # Build library lists for each type
    config["LIB_NAMES"] = {}
    for libtype in ["hic", "chi", "ill", "longreads"]:
        if libtype in config["libraries"] and isinstance(config["libraries"][libtype], dict):
            config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
        else:
            config["LIB_NAMES"][libtype] = []
            # Ensure the libtype key exists in libraries
            if libtype not in config["libraries"]:
                config["libraries"][libtype] = {}
    
else:
    # Old format: convert to new format for backward compatibility
    config["LIBRARY_STRUCTURE"] = "old"
    config["libraries"] = {"hic": {}, "chi": {}, "ill": {}, "longreads": {}}
    config["LIB_NAMES"] = {"hic": [], "chi": [], "ill": [], "longreads": []}
    
    # Convert old BAM paths to new format
    if "HIC_BAM" in config and config["HIC_BAM"] != "":
        config["libraries"]["hic"]["hic_lib1"] = {"bam": config["HIC_BAM"]}
        config["LIB_NAMES"]["hic"] = ["hic_lib1"]
    
    if "CHI_BAM" in config and config["CHI_BAM"] != "":
        config["libraries"]["chi"]["chi_lib1"] = {"bam": config["CHI_BAM"]}
        config["LIB_NAMES"]["chi"] = ["chi_lib1"]
    
    if "ILL_BAM" in config and config["ILL_BAM"] != "":
        config["libraries"]["ill"]["ill_lib1"] = {"bam": config["ILL_BAM"]}
        config["LIB_NAMES"]["ill"] = ["ill_lib1"]
    
    if "LONGREAD_BAM" in config and config["LONGREAD_BAM"] != "":
        config["libraries"]["longreads"]["longreads_lib1"] = {"bam": config["LONGREAD_BAM"]}
        config["LIB_NAMES"]["longreads"] = ["longreads_lib1"]

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi", "ill", "longreads"]:
    if len(config["LIB_NAMES"][libtype]) == 0:
        del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    datatype="[A-Za-z]+"

rule all:
    input:
        expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
               chrom = config["CHROMS"]),
        expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
               chrom = config["CHROMS"]),

        config["tool"] + "/output/final_output/complete_assembly.hap.gz",
        config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt",
        config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        expand(config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz", chrom = config["CHROMS"]),

        # also generate haplotagged bam files:
        expand(config["tool"] + "/output/final_output/{libtype}_haplotagged.bam",
               libtype = config["ACTIVE_LIBTYPES"]),
        expand(config["tool"] + "/output/final_output/{libtype}_haplotagged.bam.bai",
               libtype = config["ACTIVE_LIBTYPES"])


######################################################################
#  Make a softlink of the assembly
######################################################################
rule make_assem_softlink:
    input:
        ref = config["REFERENCE"],
    output:
        assem = config["tool"] + "/input/assembly/input.fasta"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        ln -s {input.ref} {output.assem}
        """

######################################################################
#   Make symlinks of the input BAM files for each library
######################################################################

rule make_library_bam_softlink:
    """
    Create symlinks for BAM files for each library.
    All library types (Hi-C, Chicago, Illumina, long reads) now use BAM files.
    """
    output:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam"
    params:
        bam = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["bam"]
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        ln -sf {params.bam} {output.bam}
        """

rule make_library_bai_softlink:
    """
    Create symlinks for BAM index (.bai) files for each library.
    The index file must exist alongside the BAM file, or this rule will fail.
    """
    input:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam"
    output:
        bai = config["tool"] + "/input/bams/{libtype}.{libname}.bam.bai"
    params:
        original_bam = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["bam"]
    resources:
        mem_mb = 1000,
        runtime = 2
    shell:
        """
        # Check if the .bai file exists next to the original BAM
        if [ -f "{params.original_bam}.bai" ]; then
            ln -sf {params.original_bam}.bai {output.bai}
        else
            echo "ERROR: Index file {params.original_bam}.bai not found!"
            echo "Please create an index for {params.original_bam} using: samtools index {params.original_bam}"
            exit 1
        fi
        """

######################################################################
#  First we deal with the input VCF file
#   - it could be that the VCF is not sorted correctly, so let's sort
#      it according to the input assembly
######################################################################

# get a picard dict of the reference sequences
rule picard_dict:
    """
    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        picard = config["PICARD"]
    output:
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        java -jar -Xmx1g {input.picard} CreateSequenceDictionary \
          -R {input.ref} \
          -O {output.seqdict}
        """

rule update_vcf_dict:
    """
    Update the header of the vcf file

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        seqdict = config["tool"] + "/input/ref/input.ref.dict",
        vcf = config["VCFfile"],
        picard = config["PICARD"]
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        java -jar -Xmx1g {input.picard} UpdateVcfSequenceDictionary \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# filter the vcf to remove entries that are not legal to hapcut2
rule filter_vcf:
    """
    In DeepVariant, it makes reference calls and annotates them as ./.
    These are not legal, so remove them.
     - ./. We remove these, since these are simply ref calls
     - 0/3 We remove these, since there are 3 alt alleles that will break HapCUT2
     - 1/3 This isn't valid because there are still multiple alleles to pick from
     - 0/2 We should probably remove this, since one is ref, and there are two options for the alt.
     - 1/2 ??
     - 0/2 ??
     - 2/3 ??
     - n/4 too many, just remove this
     - n/5 too many, just remove this
     - n/6 too many, just remove this
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.vcf"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 5
    shell:
        """
        awk 'BEGIN{{OFS="\t"}} /^#/ {{print; next}} $7 != "RefCall" {{print}}' {input.vcf} | \
            awk '/^#/ {{print; next}} $10 !~ /\\/3/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/4/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/5/ {{print}}' | \
            awk '/^#/ {{print; next}} $10 !~ /\\/6/ {{print}}' | \
            grep -v '\\./\\.' | \
            grep -v '0/2' > {output.vcf}
        """

# sort the vcf file based on the input assembly
rule sort_input_vcf:
    """
    If the input VCF isn't sorted correctly, then the downstream steps
     won't work.

    The latest version of picard uses a different format for the command line args.
      Used to be like this: INPUT={input.bam}
      Now it's like this:   -INPUT {input.bam}
    """
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.header_updated.filtered.vcf",
        picard  = config["PICARD"],
        seqdict = config["tool"] + "/input/ref/input.ref.dict"
    output:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 30
    shell:
        """
        java -jar -Xmx3g {input.picard} SortVcf \
          -I {input.vcf} \
          -O {output.vcf} \
          -SD {input.seqdict}
        """

# compress the vcf file
rule split_vcf_into_chr_zip:
    input:
        vcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf"
    output:
        tvcf = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        bgzip -c {input.vcf} > {output.tvcf}  #compress vcf
        """

# index the vcf file
rule index_vcf:
    input:
        tvcf = config["tool"] + "/output/vcf/orig/temp.vcf.gz"
    output:
        index = temp(config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi"),
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 10
    shell:
        """
        tabix -p vcf {input.tvcf}  # index compressed vcf
        """

rule list_of_chroms_from_reference:
    input:
        fai = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    shell:
        """
        # save all the chromosome names into a file
        cut -f1 {input.fai} > {output.chrtxt}
        """

rule list_of_chroms_from_config:
    """
    The point of this is to get the list of chromosomes that we will keep for phasing.

    Notes:
      - Previously, this would fail sometimes, even if there were perfect matches
    """
    input:
        chrtxt = config["tool"] + "/output/vcf/chromosomes.txt",
    output:
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 5
    run:
        # first, print out the chromosomes so we are sure of what we're looking for
        print("The chromosomes we are looking for are:")
        CHROMLIST = [str(chrom) for chrom in config["CHROMS"]]
        print(CHROMLIST)

        matches = []
        outhandle = open(output.chrtxt, "w")
        with open(input.chrtxt, "r") as f:
            for line in f:
                line = str(line.strip())
                if line:
                    if line in CHROMLIST:
                        matches.append(line)
        # if there were no matches, state as much and raise an error. We should have found something
        if len(matches) == 0:
            print("No matching chromosomes found.", file=outhandle)
            raise ValueError("No matching chromosomes found.")

        # If we found matches, write them to the output file
        for line in matches:
            print(line, file=outhandle)

# split all vcf files by chromosome
rule split_vcf_into_chr_remainder:
    input:
        gz = config["tool"] + "/output/vcf/orig/temp.vcf.gz",
        index = config["tool"] + "/output/vcf/orig/temp.vcf.gz.tbi",
        chrtxt = config["tool"] + "/output/vcf/chroms_filt.txt"
    output:
        vcf  = expand(config["tool"] + "/output/vcf/orig/split/{chrom}.vcf",
                    chrom=config["CHROMS"])
    params:
        outstub = config["tool"] + "/output/vcf/orig/split"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        while IFS= read -r line; do
          tabix {input.gz} $line > {params.outstub}/$line.vcf;
        done < {input.chrtxt}
        """

######################################################################
#  Now handle the read mapping
######################################################################

# index reference genome
rule index_genome:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt"
    threads: 1
    resources:
        mem_mb  = 16000,
        runtime = 120
    shell:
        'bwa index {input.ref}'

rule index_genome_faidx:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
    output:
        bwt = config["tool"] + "/input/assembly/input.fasta.fai"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 10
    shell:
        'samtools faidx {input.ref}'

######################################################################
# Split each library BAM by chromosome (before merging)
######################################################################

rule split_library_by_chromosome:
    """
    Split each individual library BAM by chromosome.
    This is done before merging so we can add library-specific read groups.
    Requires the BAM index file to efficiently extract reads by chromosome.
    """
    input:
        bam = config["tool"] + "/input/bams/{libtype}.{libname}.bam",
        bai = config["tool"] + "/input/bams/{libtype}.{libname}.bam.bai"
    output:
        bam = temp(config["tool"] + "/output/bams/split_per_library/{libtype}/{libtype}.{libname}.REF_{chrom}.bam")
    threads: 4
    resources:
        mem_mb=8000, runtime=60
    shell:
        """
        set -euo pipefail
        samtools view -@ {threads} -bh {input.bam} "{wildcards.chrom}" -o {output.bam}
        """

rule add_library_rg_to_split:
    """
    Add library-specific read groups to each chromosome split.
    This is where library tracking is implemented.

    Tags added:
      - LB: Library name (e.g., DpnII, MboI, PacBio_run1)
      - ID: Read group ID (libtype.libname.chrom)
      - SM: Sample name (from config or default to 'sample1')
      - PL: Platform (LONG for all read types - could be PacBio, ONT, Hi-C, etc.)
    """
    input:
        bam =      config["tool"] + "/output/bams/split_per_library/{libtype}/{libtype}.{libname}.REF_{chrom}.bam",
        picard = config["PICARD"]
    output:
        bam = temp(config["tool"] + "/output/bams/split_per_library/{libtype}/bam.rg.{libtype}.{libname}.REF_{chrom}")
    params:
        libname = lambda wildcards: wildcards.libname,
        rgid = lambda wildcards: f"{wildcards.libtype}.{wildcards.libname}.{wildcards.chrom}",
        sample = lambda wildcards: config.get("SAMPLE_NAME", "sample1")
    threads: 2
    resources:
        mem_mb  = 4000,
        runtime = 60
    shell:
        """
        java -Xmx3g -jar {input.picard} AddOrReplaceReadGroups \
          -I {input.bam} -O {output.bam} \
          --RGID {params.rgid} \
          --RGLB {params.libname} \
          --RGPL LONG \
          --RGSM {params.sample}
        """

def get_library_splits_for_chrom(wildcards):
    """Get all library BAM files for a given library type and chromosome"""
    libtype = wildcards.libtype
    chrom = wildcards.chrom
    bams = []
    
    for libname in config["LIB_NAMES"][libtype]:
        bams.append(config["tool"] + f"/output/bams/split_per_library/{libtype}/bam.rg.{libtype}.{libname}.REF_{chrom}")
    
    return bams

rule merge_libraries_per_chromosome:
    """
    Merge all libraries of the same type for a specific chromosome.
    Library information is preserved in the LB tag of each read.
    If only one library exists, this will just create a symlink.
    """
    input:
        bams = get_library_splits_for_chrom
    output:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
    threads: 4
    resources:
        mem_mb  = 8000,
        runtime = 60
    run:
        if len(input.bams) == 1:
            # Only one library, just create a symlink
            shell("ln -sf $(realpath {input.bams[0]}) {output.bam}")
        else:
            # Multiple libraries, merge them (preserves read groups)
            shell("samtools merge -@ {threads} {output.bam} {input.bams}")

# mark for deletion
#rule index_bams:
#    input:
#        bam = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam"
#    output:
#        bai = config["tool"] + "/output/bams/temp/{libtype}_sorted.bam.bai"
#    threads: 1
#    resources:
#        mem_mb  = 8000,
#        runtime = 120
#    shell:
#        """
#        samtools index {input.bam}
#        """

######################################################################
# Index and validate split BAMs
######################################################################

rule index_split_bam:
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
    output:
        bai = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam.bai"
    threads: 2
    resources:
        mem_mb=8000, runtime=15
    shell:
        """
        set -euo pipefail
        # Try to index; if the shard is empty and samtools refuses, create a placeholder
        # so downstream rules depending on the .bai can still proceed.
        samtools index -@ {threads} {input.bam} {output.bai} || touch {output.bai}
        """

# we don't need this anymore since we added RGs in merge_libraries_per_chromosome
#rule pass_through_rg:
#    """
#    Pass-through rule for compatibility with downstream rules.
#    Read groups are already set in merge_libraries_per_chromosome,
#    so we just create a symlink.
#    """
#    input:
#        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam"
#    output:
#        bam = temp(config["tool"] + "/output/bams/split/{libtype}.rg.REF_{chrom}.bam")
#    threads: 1
#    resources:
#        mem_mb  = 1000,
#        runtime = 5
#    shell:
#        """
#        ln -sf $(realpath {input.bam}) {output.bam}
#        """

rule rm_duplicates_in_split_bams:
    """
    This removes duplicates for chicago, illumina, and hi-c using sambamba.

    Sambamba markdup is faster than Picard and removes duplicates by default with -r flag.
    Uses multiple threads for better performance.
    """
    input:
        bam = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        sambamba = os.path.join(filepath, "../bin/sambamba")
    output:
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam"
    threads: 4
    resources:
        mem_mb  = 8000,
        runtime = 60  # minutes - sambamba is faster than Picard
    shell:
        """
        {input.sambamba} markdup -r -t {threads} {input.bam} {output.bam}
        """

rule index_rmdup_bams:
    input:
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam",
    output:
        bai = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam.bai",
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 45
    shell:
        """
        samtools index {input.bam}
        """

######################################################################
# Extract the hairs from the different data types
######################################################################

# convert Hi-C bam files to haplotype fragment files
rule extract_hairs:
    input:
        ref = config["tool"] + "/input/assembly/input.fasta",
        bwt = config["tool"] + "/input/assembly/input.fasta.bwt",
        fai = config["tool"] + "/input/assembly/input.fasta.fai",
        bam = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam",
        bai = config["tool"] + "/output/bams/split_rmdups/{libtype}.REF_{chrom}.rmdups.bam.bai",
        vcf = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        frag = config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{chrom}.frag"
    threads: 1
    params:
        fragtype = lambda wildcards: wildcards.libtype
    resources:
        mem_mb  = 8000,
        runtime = 120
    shell:
        """
        if [ "{params.fragtype}" == "longreads" ]; then
            extractHAIRS --pacbio 1 --new_format 1 --indels 1 \
               --bam {input.bam} --ref {input.ref} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "hic" ]; then
            extractHAIRS --hic 1 --new_format 1 \
               --indels 1 --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "chi" ]; then
            extractHAIRS --maxIS 10000000 --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        elif [ "{params.fragtype}" == "ill" ]; then
            extractHAIRS --new_format 1 \
               --indels 1 --ref {input.ref} \
               --bam {input.bam} --VCF {input.vcf} > {output.frag}
        fi
        """

rule concatenate_HAPCUT2_evidence:
    input:
        frags = expand(config["tool"] + "/output/hapcut2_evidence/per-datatype/{libtype}.{{chrom}}.frag",
                       libtype = config["ACTIVE_LIBTYPES"])
    output:
        cat = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 15
    shell:
        """
        cat {input.frags} > {output.cat}
        """

######################################################################
# Run HAPCUT2 - once per scaffold
######################################################################

# run HapCUT2 to assemble haplotypes from combined Hi-C + longread haplotype fragments
rule run_hapcut2_hic_longread:
    input:
        frag = config["tool"] + "/output/hapcut2_evidence/concatenated/{chrom}.frag",
        vcf  = config["tool"] + "/output/vcf/orig/split/{chrom}.vcf"
    output:
        hap   = config["tool"] + "/output/hapcut2_results/{chrom}.hap",
        vcf   = config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF"
    params:
        thischrom = lambda wildcards: wildcards.chrom,
        model = config["tool"] + "/output/hapcut2_results/{chrom}.htrans_model"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 120
    shell:
        """
        HAPCUT2 --fragments {input.frag} --vcf {input.vcf} \
            --output {output.hap} \
            --hic 1 \
            --htrans_data_outfile {params.model} \
            --outvcf 1
        """

######################################################################
# Once we have the HAPCUT2 output, put it in a useful format
#
# The rules cat_haps and merge_vcfs were previously in one rule.
#  I encountered some errors so split them up.
######################################################################

rule cat_haps:
    """
    This pools all of the haplotype files that we generated previously.
    """
    input:
        hap = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap",
                     chrom=config["CHROMS"])
    output:
        hap = temp(config["tool"] + "/output/final_output/complete_assembly.hap")
    threads: 1
    resources:
        mem_mb=1000, runtime=10
    shell:
        r"""
        mkdir -p {config[tool]}/output/final_output
        cat {input.hap} > {output.hap}
        """

rule merge_vcfs:
    """
    202508 - This file isn't used for anything
    """
    input:
        origvcf = config["tool"] + "/output/vcf/orig_sorted/input.sorted.vcf",
        vcfs    = expand(config["tool"] + "/output/hapcut2_results/{chrom}.hap.phased.VCF",
                         chrom=config["CHROMS"])
    output:
        merged = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.unsorted.vcf")
    threads: 2
    resources:
        mem_mb=4000, runtime=30
    shell:
        r"""
        set -euo pipefail
        mkdir -p {config[tool]}/output/final_output

        # Merge all per-chrom VCFs into one (keep all sites, even if contigs differ)
        bcftools concat -a -D -O v -o {output.merged} {input.vcfs}

        # If you need to ensure the header carries over metadata from origvcf,
        # you can reheader (optional). Usually concat keeps a sane header:
        # bcftools annotate --header-lines {input.origvcf} -O v -o {output.merged}.tmp {output.merged}
        # mv {output.merged}.tmp {output.merged}
        """

rule sort_merged_vcf:
    """
    Sort the merged phased VCF by chromosome and position.
    This completes the workflow that was split from the old sort_final_vcf rule.
    """
    input:
        unsorted = config["tool"] + "/output/final_output/complete_assembly.hap.phased.unsorted.vcf"
    output:
        sorted_vcf = temp(config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf")
    threads: 2
    resources:
        mem_mb=4000, runtime=30
    shell:
        """
        bcftools sort -O v -o {output.sorted_vcf} {input.unsorted}
        """

rule get_largest_hap_blocks:
    """
    These are the "most variants phased" (MVP) haplotype blocks.
    This term was first used in Selvaraj et al., Nature Biotechnology (2013)
    This is likely the block that covers most of the chromosome, as it contains most of the variants
      that were phased.
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        biggest_blocks = {}
        this_block_header = ""
        this_block_c = ""
        this_block_start = -1
        this_block_stop = -1
        with open(input.hap, "r") as f:
            for line in f:
                line = line.replace("*", "").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        # we have just found a new block
                        if not this_block_header == "":
                            # not the first, add an entry to biggest_blocks
                            span = int(this_block_stop) - int(this_block_start) + 1
                            add_this = False
                            if this_block_c not in biggest_blocks:
                                add_this = True
                            else:
                                if span > biggest_blocks[this_block_c]["span"]:
                                    add_this = True
                            if add_this:
                                biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
                        this_block_start = -1
                        this_block_stop = -1
                        this_block_header = line
                    else:
                        if this_block_start == -1:
                            this_block_start = splitd[4]
                            this_block_stop = splitd[4]
                            this_block_c     = splitd[3]
                        else:
                            this_block_stop = splitd[4]
        # final parsing at the end
        add_this = False
        if this_block_c not in biggest_blocks:
            add_this = True
        else:
            if span > biggest_blocks[this_block_c]["span"]:
                add_this = True
        if add_this:
            biggest_blocks[this_block_c] = {"name": this_block_header, "span": span}
        with open(output.txt, "w") as f:
            for key in biggest_blocks:
                print("{}\t{}\t\"{}\"".format(
                      key,
                      biggest_blocks[key]["span"],
                      biggest_blocks[key]["name"]), file = f)

rule hap_blocks_to_vcf:
    """
    This converts the haplotype blocks to a vcf
    """
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap",
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf",
        txt = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt"
    output:
        final_vcf = temp(config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf")
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    run:
        # first get the headers that we want
        headers = set()
        with open(input.txt, "r") as f:
            for line in f:
                line = line.split("\"")[1]
                headers.add(line)
        # now get the sites that we will keep
        #  key is chr, set of sites
        keepers = {}
        with open(input.hap, "r") as f:
            capturing = False
            for line in f:
                line=line.replace('*',"").strip()
                if line:
                    splitd = line.split()
                    if splitd[0] == "BLOCK:":
                        if line in headers:
                            capturing = True
                        else:
                            capturing = False
                    else:
                        if capturing:
                            this_c = splitd[3]
                            this_s = int(splitd[4])
                            if this_c not in keepers:
                                keepers[this_c] = set()
                            keepers[this_c].add(this_s)
        # now get the sites that are in the phase blocks
        writeme = open(output.final_vcf, "w")
        with open(input.vcf, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    if line[0] == '#':
                        print(line, file = writeme)
                    else:
                        # not a comment
                        splitd = line.split()
                        this_c = splitd[0]
                        this_s = int(splitd[1])
                        if this_c in keepers:
                            if this_s in keepers[this_c]:
                                print(line, file = writeme)
        writeme.close()

######################################################################
#   Compress the output
######################################################################
rule compress_final_output:
    input:
        final_vcf = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf"
    output:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        # compress
        bgzip -c {input.final_vcf} > {output.final_gz}
        # index
        tabix -p vcf {output.final_gz}
        """

rule gzip_hapcut2_vcf:
    input:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf"
    output:
        vcf = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz",
        tbi = config["tool"] + "/output/final_output/complete_assembly.hap.phased.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        bgzip -c {input.vcf} > {output.vcf}
        # index
        tabix -p vcf {output.vcf}
        """

rule gzip_complete_assembly_hap_file:
    input:
        hap = config["tool"] + "/output/final_output/complete_assembly.hap"
    output:
        gzipped = config["tool"] + "/output/final_output/complete_assembly.hap.gz"
    threads: 1
    resources:
        mem_mb  = 2000,
        runtime = 30
    shell:
        """
        cat {input.hap} | gzip > {output.gzipped}
        """

########################################################################
#    Now split this into different chromosomes and make the final table
########################################################################


rule make_table_of_final_output:
    """
    This makes a table of the results from phasing. Useful for plotting.
    The columns included are:
      - chrom (the name from the fasta header)
      - chrom_length (the sequence length)
      - largest_pb_start (1-based start index for the largest phase block)
      - largest_pb_stop  (1-based stop index for the largest phase block)
      - largest_pb_length (the size of the largest phase block)
      - largest_pb_num_var (the number of variants in the largest phase block)
      - largest_pb_num_phased (the number of phased variants in the largest phase block)
    """
    input:
        txt   = config["tool"] + "/output/final_output/hapcut_largest_blocks_per_sca.txt",
        vcfgz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        tbi   = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi",
        ref   = config["tool"] + "/input/assembly/input.fasta"
    output:
        txt = config["tool"] + "/output/final_output/hapcut_largest_block_final_table.txt"
    threads: 1
    resources:
        mem_mb=2000, runtime=30
    run:
        import subprocess
        from Bio import SeqIO
        from contextlib import ExitStack

        chrom_to_length = {}

        with ExitStack() as stack:
            ref_handle  = stack.enter_context(open(input.ref, "r"))
            out_handle  = stack.enter_context(open(output.txt, "w"))
            txt_handle  = stack.enter_context(open(input.txt, "r"))

            # chrom sizes from FASTA
            for record in SeqIO.parse(ref_handle, "fasta"):
                chrom_to_length[record.id] = len(record.seq)

            print("\t".join([
                "chrom","chrom_length","largest_pb_start","largest_pb_stop",
                "largest_pb_length","percent_of_chrom_in_largest_pb",
                "largest_pb_num_var","largest_pb_num_phased","percent_of_vars_phased"
            ]), file=out_handle)

            for raw in txt_handle:
                line = raw.strip()
                if not line:
                    continue

                splitd = line.split("\t")
                chrom = splitd[0]
                chrom_length = chrom_to_length[chrom]

                # Use tabix/bcftools random access to fetch first/last POS
                cmd_base   = f"bcftools view -r {chrom} -H {input.vcfgz} | cut -f2"
                start_str  = subprocess.check_output(f"{cmd_base} | head -n1", shell=True, text=True).strip()
                stop_str   = subprocess.check_output(f"{cmd_base} | tail -n1", shell=True, text=True).strip()

                largest_pb_start = int(start_str) if start_str else 0
                largest_pb_stop  = int(stop_str)  if stop_str  else 0
                largest_pb_length = max(0, largest_pb_stop - largest_pb_start + 1)

                largest_pb_num_var    = int(splitd[2].split()[4])
                largest_pb_num_phased = int(splitd[2].split()[6])

                percent_of_chrom_in_largest_pb = f"{(largest_pb_length/chrom_length)*100:.4f}" if chrom_length else "0.0000"
                percent_of_vars_phased = f"{(largest_pb_num_phased/largest_pb_num_var)*100:.2f}" if largest_pb_num_var else "0.00"

                print("\t".join(map(str, [
                    chrom,
                    chrom_length,
                    largest_pb_start,
                    largest_pb_stop,
                    largest_pb_length,
                    percent_of_chrom_in_largest_pb,
                    largest_pb_num_var,
                    largest_pb_num_phased,
                    percent_of_vars_phased
                ])), file=out_handle)

########################################################################
#    Now we use WhatsHap to haplotag the Hi-C and long reads
########################################################################

rule split_phased_into_individual_chromosomes:
    input:
        final_gz = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz",
        final_in = config["tool"] + "/output/final_output/largest_blocks.hap.phased.vcf.gz.tbi"
    output:
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 20
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        bcftools view -r {wildcards.chrom} -Oz -o {output.chromgz} {input.final_gz}
        """

rule index_vcf_individual_chromosomes:
    input:
        chromgz = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz"
    output:
        index   = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi"
    threads: 1
    resources:
        mem_mb  = 4000,
        runtime = 20
    params:
        tchrom = lambda w: w.chrom
    shell:
        """
        tabix -p vcf {input.chromgz}
        """

def bam_size_mb(path: str) -> int:
    try:
        return max(1, int(Path(path).stat().st_size // (1024*1024)))
    except FileNotFoundError:
        return 1

def mem_for_bam_mb(path: str, floor=8000, ceil=65536, pct=0.10, base=1024) -> int:
    """Generic scaler: base + pct * size (MB), clamped."""
    mb = bam_size_mb(path)
    return int(max(floor, min(ceil, base + pct * mb)))

def mem_for_hic_piecewise_mb(path: str) -> int:
    """Conservative RAM for Hi-C shards based on your observed failures."""
    gb = bam_size_mb(path) / 1024
    if gb <= 4:   return 32000   # 32 GB
    if gb <= 8:   return 40000   # 40 GB
    if gb <= 10:  return 64000   # 64 GB  (40 GB failed >8 GB)
    if gb <= 14:  return 80000   # 80 GB
    if gb <= 18:  return 96000   # 96 GB
    if gb <= 22:  return 128000  # 128 GB (future ~20 GB shards)
    return 160000                # safety cap

def mem_for_libtype_mb(libtype: str, path: str) -> int:
    """Hi-C uses piecewise; long reads get a modest scaler; others lighter."""
    lt = (libtype or "").lower()
    if lt == "hic":
        return mem_for_hic_piecewise_mb(path)
    if lt in {"hifi","pb","pacbio","ont","nanopore","clr","ul"}:
        # Long reads: slightly higher floor and slope than Illumina
        return mem_for_bam_mb(path, floor=12000, ceil=98304, pct=0.15, base=1024)
    # Illumina/other: lighter defaults
    return mem_for_bam_mb(path, floor=8000, ceil=65536, pct=0.10, base=512)

def runtime_for_bam_min(path: str, base=20, per_gb=1.0, ceil=600) -> int:
    size_gb = bam_size_mb(path) / 1024
    return int(max(base, min(ceil, base + per_gb * size_gb)))

rule haplotag_reads_chrom:
    input:
        bam    = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam",
        bai    = config["tool"] + "/output/bams/split/{libtype}.REF_{chrom}.bam.bai",
        vcfgz  = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz",
        vcfin  = config["tool"] + "/output/final_output/phased_by_chromosome/largest_blocks.hap.phased.{chrom}.vcf.gz.tbi",
        # index may not exist for empty shard; keep optional in logic
        ref    = config["tool"] + "/input/assembly/input.fasta",
        fai    = config["tool"] + "/input/assembly/input.fasta.fai"
    output:
        bam = config["tool"] + "/output/final_output/haplotagged_by_chrom/{libtype}.REF_{chrom}.haplotagged.bam",
        bai = config["tool"] + "/output/final_output/haplotagged_by_chrom/{libtype}.REF_{chrom}.haplotagged.bam.bai"
    threads: 1
    resources:
        # Bigger floor for Hi-C, modest floor for others
        mem_mb  = lambda wc, input: mem_for_libtype_mb(wc.libtype, input.bam),
        runtime = lambda wc, input: runtime_for_bam_min(input.bam, base=45, per_gb=10, ceil=2160),
        slurm_cpus_per_task = lambda wc, threads: threads,
    shell:
        """
        set -euo pipefail

        # Empty or invalid BAM shard? Emit header-only BAM.
        if ! samtools quickcheck -v {input.bam} >/dev/null 2>&1 || \
           [ "$(samtools view -c {input.bam})" -eq 0 ]; then
            echo "[haplotag_reads_chrom] {wildcards.chrom}: empty BAM shard; writing header-only BAM."
            samtools view -H {input.bam} | samtools view -b -o {output.bam} -
            samtools index -@ {threads} {output.bam}
            exit 0
        fi

        # No variants in this VCF shard? Just pass the BAM through unchanged.
        if [ "$(bcftools view -H {input.vcfgz} | wc -c)" -eq 0 ]; then
            echo "[haplotag_reads_chrom] {wildcards.chrom}: no variants; copying BAM shard."
            cp -p {input.bam} {output.bam}
            if [ -f {input.bai} ]; then cp -p {input.bai} {output.bai}; else samtools index -@ {threads} {output.bam}; fi
            exit 0
        fi

        # Otherwise, haplotag normally.
        whatshap haplotag \
            -o {output.bam} \
            --reference {input.ref} \
            --ignore-read-groups \
            --ploidy 2 \
            --output-threads {threads} \
            {input.vcfgz} \
            {input.bam}

        samtools index -@ {threads} {output.bam}
        """


def total_bam_size_mb(paths) -> int:
    tot = 0
    for p in paths:
        try: tot += int(Path(p).stat().st_size // (1024*1024))
        except FileNotFoundError: pass
    return max(1, tot)

def runtime_for_merge_minutes(libtype: str, paths,
                              hic_base=45, hic_per_gb=1.2, hic_ceil=2160,
                              pb_base=45,  pb_per_gb=0.8,  pb_ceil=720) -> int:
    gb = total_bam_size_mb(paths) / 1024
    lt = (libtype or "").lower()
    if lt == "hic":
        return int(min(hic_ceil, max(hic_base, hic_base + hic_per_gb * gb)))
    # PacBio / long reads default
    return int(min(pb_ceil, max(pb_base, pb_base + pb_per_gb * gb)))

rule merge_haplotagged:
    input:
        bams = expand(config["tool"] + "/output/final_output/haplotagged_by_chrom/{{libtype}}.REF_{chrom}.haplotagged.bam",
               chrom=config["CHROMS"])
    output:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam"
    threads: 8
    resources:
        mem_mb  = 8000,  # merge is light on RAM
        runtime = lambda wc, input: runtime_for_merge_minutes(wc.libtype, input.bams)
    shell:
        """
        samtools merge -@ {threads} -o {output.bam} {input}
        """

rule index_haplotagged:
    input:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam"
    output:
        bam = config["tool"] + "/output/final_output/{libtype}_haplotagged.bam.bai"
    threads: 1
    resources:
        mem_mb=8000,
        runtime = lambda wc, input: runtime_for_bam_min(input.bam, base=15, per_gb=1.5, ceil=600)
    shell:
        """
        samtools index -@ {threads} {input.bam}
        """