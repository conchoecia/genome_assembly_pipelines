"""
This performs a whole_genome_alignment using minimap2.
 It does this by breaking up the query genome into 10-mb chunks to more quickly
 align them then if they had been left whole.
"""
import subprocess
from Bio import SeqIO

configfile: "config.yaml"
config["tool"] = "GAP_dgenies_prep"

for x in ["qname", "query", "rname", "reference"]:
    if x not in config:
        raise IOError( "you must specify {} in the config".format(x))


# get the path to the dgenies script
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
dgenies_path=os.path.join(filepath, "../bin/dgenies_index.py")

rule all:
    input:
        expand(config["tool"] + "/output/paf/{qname}_to_{rname}.paf",
               qname = config["queries"], rname = config["references"]),
        expand(config["tool"] + "/output/paf/{assem}.idx",
               assem = [config["queries"], config["references"]])

rule split_query_fasta:
    """
    this splits the assembly into smaller pieces to make the mapping faster
    """
    input:
        assem = lambda wildcards: config["queries"][wildcards.qname]
    output:
        assem = config["tool"] + "/input/fragmented/{qname}_fragmented.fasta"
    threads: 1
    run:
        outhandle = open(output.assem, "w")
        stepsize = 1000000
        for record in SeqIO.parse(input.assem, "fasta"):
            i = 0
            done = False
            record.id = "{}_.-._chunk{}".format(record.id, i)
            while not done:
                SeqIO.write(record[i*stepsize:(i*stepsize)+stepsize], outhandle, 'fasta')
                i += 1
                record.id = record.id.split("_.-._")[0] + "_.-._chunk" + str(i)
                if (i*stepsize)-1 >= len(record.seq):
                    done = True
        outhandle.close()

rule minimap2:
    """
    runs minimap2 using the fragmented genome as the "reads" and the
     reference normally
    """
    input:
        reads = config["tool"] + "/input/fragmented/{qname}_fragmented.fasta",
        reference = lambda wildcards: config["references"][wildcards.rname]
    output:
        paf = config["tool"] + "/output/paf_raw/{qname}_to_{rname}.raw.paf"
    threads: workflow.cores
    shell:
        """
        minimap2 -t {threads} {input.reference} {input.reads} > {output.paf}
        """

rule cleanup_paf:
    """
    This cleas up the paf file because the coordinates are in chunks
    """
    input:
        assem  = lambda wildcards: config["queries"][wildcards.qname],
        paf    = config["tool"] + "/output/paf_raw/{qname}_to_{rname}.raw.paf"
    output:
        paf    = config["tool"] + "/output/paf/{qname}_to_{rname}.paf"
    run:
        # first we get the scaffold size
        scaf_to_size = {}
        for record in SeqIO.parse(input.assem, "fasta"):
            scaf_to_size[record.id] = len(record.seq)

        # now we fix the coordinates of the query sequences
        outhandle = open(output.paf, "w")
        #fields for paf are
        #Col 	Type 	Description
        #1 	string 	Query sequence name
        #2 	int 	Query sequence length
        #3 	int 	Query start (0-based; BED-like; closed)
        #4 	int 	Query end (0-based; BED-like; open)
        #5 	char 	Relative strand: "+" or "-"
        #6 	string 	Target sequence name
        #7 	int 	Target sequence length
        #8 	int 	Target start on original strand (0-based)
        #9 	int 	Target end on original strand (0-based)
        #10 	int 	Number of residue matches
        #11 	int 	Alignment block length
        #12 	int 	Mapping quality (0-255; 255 for missing)
        with open(input.paf, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = line.split("\t")
                    multiplier = int(fields[0].split("_.-._chunk")[1])
                    scaf       = fields[0].split("_.-._chunk")[0]
                    fields[0] = scaf
                    fields[1] = str(scaf_to_size[scaf])
                    fields[2] = str(int(fields[2]) + (1000000 * multiplier))
                    fields[3] = str(int(fields[3]) + (1000000 * multiplier))
                    print("\t".join(fields), file = outhandle)
        outhandle.close()

rule index_assemblies:
    input:
        query     = lambda wildcards: config["queries"][wildcards.qname],
        refer     = lambda wildcards: config["references"][wildcards.rname],
        dgenies   = dgenies_path
    output:
        query = config["tool"] + "/output/paf/{qname}.idx",
        refer = config["tool"] + "/output/paf/{rname}.idx"
    threads: 1
    params:
        qname = lambda wildcards: wildcards.qname,
        rname = lambda wildcards: wildcards.rname,
    shell:
        """
        python {input.dgenies} -i {input.query} -n params.qname -o {output.query}
        python {input.dgenies} -i {input.refer} -n params.rname -o {output.refer}
        """
