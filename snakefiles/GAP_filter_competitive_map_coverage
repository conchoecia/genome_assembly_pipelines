"""
This snakefile performs competitive long-read mapping and coverage analysis
to classify scaffolds based on their alignment to chromosome-scale scaffolds.

The goal of the analysis is to identify unplaced contigs that either are redundant
with the chromosome-scale scaffolds, or contigs that have few to no reads mapping to them.
These are both reasons to exclude these contigs from the assembly.

Pipeline steps:
  1. Competitively maps HiFi and ONT reads to genome (-F 2304 filters secondary/supplementary)
  2. Splits assembly into chromosome-scale and non-chromosome scaffolds
  3. Maps non-chromosome scaffolds to chromosome scaffolds
  4. Calculates mapping statistics and long-read coverage
  5. Generates summary table and interactive visualization
"""

from Bio import SeqIO
import gzip
import os
import sys

configfile: "config.yaml"
config["tool"] = "GAP_filter_competitive_map_coverage"

# Validate config file has required fields
required_fields = ["assemblies", "chromosomes", "long_reads"]
for field in required_fields:
    if field not in config:
        raise IOError(f"Config file must contain '{field}' field")

# Check assembly names don't have special characters
for x in config["assemblies"]:
    if "_" in x:
        raise IOError(f"Assembly names must not contain underscores: {x}. Use only [A-Za-z0-9]+")

# Validate long_reads structure
if not isinstance(config["long_reads"], dict):
    raise IOError("'long_reads' must be a dictionary with structure: {name: {file: path, type: map-hifi/map-ont}}")

for lr_name, lr_info in config["long_reads"].items():
    if "file" not in lr_info or "type" not in lr_info:
        raise IOError(f"Long read entry '{lr_name}' must have 'file' and 'type' fields")
    if lr_info["type"] not in ["map-hifi", "map-ont", "map-pb"]:
        raise IOError(f"Long read type must be 'map-hifi', 'map-ont', or 'map-pb'. Got: {lr_info['type']}")
    if not os.path.exists(lr_info["file"]):
        print(f"Warning: Long read file does not exist: {lr_info['file']}")

# Set default values
if "mapping_threads" not in config:
    config["mapping_threads"] = 32
if "sort_threads" not in config:
    config["sort_threads"] = 8
if "min_mapq" not in config:
    config["min_mapq"] = 0
if "generate_html" not in config:
    config["generate_html"] = True
if "generate_static" not in config:
    config["generate_static"] = False
if "filter_redundant" not in config:
    config["filter_redundant"] = False
if "filter_min_coverage" not in config:
    config["filter_min_coverage"] = 95.0
if "filter_min_mapq" not in config:
    config["filter_min_mapq"] = 30
if "trim_redundant_regions" not in config:
    config["trim_redundant_regions"] = False
if "trim_min_block_size" not in config:
    config["trim_min_block_size"] = 10000
if "trim_min_mapq" not in config:
    config["trim_min_mapq"] = 20
if "trim_min_keep_piece" not in config:
    config["trim_min_keep_piece"] = 5000
if "trim_max_redundant_pct" not in config:
    config["trim_max_redundant_pct"] = 100.0
if "trim_aggressive_min_block_size" not in config:
    config["trim_aggressive_min_block_size"] = 50000
if "trim_aggressive_min_mapq" not in config:
    config["trim_aggressive_min_mapq"] = 8

# Get paths to scripts
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
parser_script = os.path.join(filepath, "../scripts/parse_contig_mapping_stats.py")
report_script = os.path.join(filepath, "../scripts/generate_contig_classification_report.py")
coverage_script = os.path.join(filepath, "../scripts/calculate_contig_coverage.py")
chr_coverage_script = os.path.join(filepath, "../scripts/calculate_chr_coverage_stats_fast.py")
binned_coverage_script = os.path.join(filepath, "../scripts/calculate_binned_contig_coverage.py")
filter_script = os.path.join(filepath, "../scripts/filter_redundant_contigs.py")
trim_script = os.path.join(filepath, "../scripts/trim_redundant_regions.py")
combine_script = os.path.join(filepath, "../scripts/combine_chr_and_retained_contigs.py")

wildcard_constraints:
    nom="[A-Za-z0-9.]+",
    lr_name="[A-Za-z0-9._-]+"

rule all:
    input:
        # Summary table with mapping and coverage stats
        expand(config["tool"] + "/output/{nom}/{nom}_contig_classification_summary.csv",
               nom = config["assemblies"]),
        # Interactive HTML report (if enabled)
        expand(config["tool"] + "/output/{nom}/{nom}_contig_classification_report.html",
               nom = config["assemblies"]) if config["generate_html"] else [],
        # Static plots (if enabled)
        expand(config["tool"] + "/output/{nom}/{nom}_contig_classification_plot.png",
               nom = config["assemblies"]) if config["generate_static"] else [],
        # Filtered assembly (if filtering enabled)
        expand(config["tool"] + "/output/{nom}/{nom}_filtered.fasta",
               nom = config["assemblies"]) if config["filter_redundant"] else [],
        # Trimmed assembly (if trimming enabled)
        expand(config["tool"] + "/output/{nom}/{nom}_trimmed.fasta",
               nom = config["assemblies"]) if config["trim_redundant_regions"] else [],
        # Combined assembly: chromosome scaffolds + retained contigs (if filtering/trimming enabled)
        expand(config["tool"] + "/output/{nom}/{nom}_combined.fasta",
               nom = config["assemblies"]) if (config["filter_redundant"] or config["trim_redundant_regions"]) else []

# Softlink input assembly
rule softlink_assembly:
    """
    Create softlink to input assembly
    """
    input:
        assem = lambda wildcards: config["assemblies"][wildcards.nom]
    output:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    threads: 1
    resources:
        mem_mb=1000,
        runtime=2
    run:
        os.makedirs(os.path.dirname(output.assem), exist_ok=True)
        if os.path.exists(output.assem):
            os.remove(output.assem)
        os.symlink(os.path.abspath(input.assem), output.assem)

# Softlink long read files
rule softlink_long_reads:
    """
    Create softlink to long read files
    """
    output:
        lr = config["tool"] + "/input/longreads/{lr_name}"
    threads: 1
    resources:
        mem_mb=1000,
        runtime=2
    run:
        os.makedirs(os.path.dirname(output.lr), exist_ok=True)
        lr_file = config["long_reads"][wildcards.lr_name]["file"]
        if os.path.exists(output.lr):
            os.remove(output.lr)
        os.symlink(os.path.abspath(lr_file), output.lr)

rule competitive_map_LR_to_genome:
    """
    Maps long reads to assembly with competitive mapping
    Uses -F 2304 to filter out secondary (256) and supplementary (2048) alignments
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        long_reads = config["tool"] + "/input/longreads/{lr_name}"
    output:
        bam = config["tool"] + "/output/bams_LR/{nom}/LR_{lr_name}_to_{nom}.bam"
    params:
        minimaparg = lambda wildcards: config["long_reads"][wildcards.lr_name]["type"],
        lr_name = lambda wildcards: wildcards.lr_name
    threads: config["mapping_threads"]
    resources:
        mem_mb=lambda wildcards, input: max(16000, int(os.path.getsize(input.assem) / 1000000 * 4)),
        runtime=lambda wildcards, input: max(60, int(os.path.getsize(input.long_reads) / 1000000000 * 30))
    shell:
        """
        minimap2 -t {threads} -ax {params.minimaparg} {input.assem} \
            --split-prefix {params.lr_name} {input.long_reads} | \
          samtools view -F 2304 -hb - | \
          samtools sort -@ {threads} - > {output.bam}
        """

rule index_LR_bams:
    """
    Index the long read BAM files
    """
    input:
        bam = config["tool"] + "/output/bams_LR/{nom}/LR_{lr_name}_to_{nom}.bam"
    output:
        bai = config["tool"] + "/output/bams_LR/{nom}/LR_{lr_name}_to_{nom}.bam.bai"
    threads: 1
    resources:
        mem_mb=2000,
        runtime=10
    shell:
        """
        samtools index {input.bam}
        """

rule split_assembly_by_chromosomes:
    """
    Split the assembly into chromosome-scale and non-chromosome-scale scaffolds
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta"
    output:
        chr_fasta = config["tool"] + "/output/{nom}/{nom}_chr_scaffolds.fasta",
        non_chr_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_scaffolds.fasta",
        chr_list = config["tool"] + "/output/{nom}/{nom}_chr_list.txt",
        non_chr_list = config["tool"] + "/output/{nom}/{nom}_non_chr_list.txt"
    params:
        chromosomes = config["chromosomes"]
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    run:
        chr_set = set(params.chromosomes)
        chr_records = []
        non_chr_records = []
        
        with open(input.assem) as handle:
            for record in SeqIO.parse(handle, "fasta"):
                if record.id in chr_set:
                    chr_records.append(record)
                else:
                    non_chr_records.append(record)
        
        # Write chromosome scaffolds
        with open(output.chr_fasta, "w") as out_handle:
            SeqIO.write(chr_records, out_handle, "fasta")
        
        # Write non-chromosome scaffolds
        with open(output.non_chr_fasta, "w") as out_handle:
            SeqIO.write(non_chr_records, out_handle, "fasta")
        
        # Write lists
        with open(output.chr_list, "w") as out_handle:
            for record in chr_records:
                out_handle.write(f"{record.id}\n")
        
        with open(output.non_chr_list, "w") as out_handle:
            for record in non_chr_records:
                out_handle.write(f"{record.id}\n")

rule split_scaffolds_into_contigs:
    """
    Split non-chromosome scaffolds into contigs at gap regions (10+ consecutive N's)
    and trim N's from contig ends
    """
    input:
        non_chr_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_scaffolds.fasta"
    output:
        contigs_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_contigs.fasta",
        contig_map = config["tool"] + "/output/{nom}/{nom}_contig_to_scaffold_map.tsv"
    params:
        min_gap_length = 10
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    run:
        import re
        
        contig_records = []
        contig_mapping = []
        
        with open(input.non_chr_fasta) as handle:
            for scaffold in SeqIO.parse(handle, "fasta"):
                scaffold_seq = str(scaffold.seq)
                scaffold_name = scaffold.id
                
                # Split at regions with 10+ consecutive N's (case insensitive)
                # Use regex to find gaps
                gap_pattern = re.compile(r'[Nn]{' + str(params.min_gap_length) + r',}')
                
                # Find all gap positions
                splits = []
                last_end = 0
                for match in gap_pattern.finditer(scaffold_seq):
                    if last_end < match.start():
                        splits.append((last_end, match.start()))
                    last_end = match.end()
                
                # Add final segment if it exists
                if last_end < len(scaffold_seq):
                    splits.append((last_end, len(scaffold_seq)))
                
                # If no gaps found, treat whole scaffold as one contig
                if not splits:
                    splits = [(0, len(scaffold_seq))]
                
                # Create contig records
                for contig_idx, (start, end) in enumerate(splits, 1):
                    contig_seq = scaffold_seq[start:end]
                    
                    # Trim N's from both ends
                    contig_seq_stripped = contig_seq.strip('Nn')
                    
                    # Skip if contig is too short after trimming
                    if len(contig_seq_stripped) < 100:
                        continue
                    
                    # Create contig name: scaffold_name_contig_X
                    if len(splits) == 1:
                        # If only one contig, keep original name
                        contig_name = scaffold_name
                    else:
                        contig_name = f"{scaffold_name}_contig_{contig_idx}"
                    
                    # Create SeqRecord for contig
                    from Bio.Seq import Seq
                    contig_record = SeqIO.SeqRecord(
                        Seq(contig_seq_stripped),
                        id=contig_name,
                        description=f"from {scaffold_name}"
                    )
                    contig_records.append(contig_record)
                    
                    # Track contig to scaffold mapping
                    contig_mapping.append({
                        'contig_name': contig_name,
                        'scaffold_name': scaffold_name,
                        'contig_index': contig_idx,
                        'contig_length': len(contig_seq_stripped),
                        'scaffold_start': start,
                        'scaffold_end': end
                    })
        
        # Write contigs to fasta
        with open(output.contigs_fasta, "w") as out_handle:
            SeqIO.write(contig_records, out_handle, "fasta")
        
        # Write mapping file
        with open(output.contig_map, "w") as out_handle:
            out_handle.write("contig_name\tscaffold_name\tcontig_index\tcontig_length\tscaffold_start\tscaffold_end\n")
            for mapping in contig_mapping:
                out_handle.write(f"{mapping['contig_name']}\t{mapping['scaffold_name']}\t"
                               f"{mapping['contig_index']}\t{mapping['contig_length']}\t"
                               f"{mapping['scaffold_start']}\t{mapping['scaffold_end']}\n")

rule map_non_chr_to_chr_scaffolds:
    """
    Map non-chromosome contigs to chromosome scaffolds using minimap2 asm5 preset
    Keep raw sorted BAM without filtering
    """
    input:
        chr_fasta = config["tool"] + "/output/{nom}/{nom}_chr_scaffolds.fasta",
        contigs_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_contigs.fasta"
    output:
        bam = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam"
    threads: config["mapping_threads"]
    resources:
        mem_mb=16000,
        runtime=60
    shell:
        """
        minimap2 -t {threads} -ax asm5 {input.chr_fasta} {input.contigs_fasta} | \
          samtools view -hb - | \
          samtools sort -@ {threads} - > {output.bam}
        """

rule index_scaffold_mapping_bam:
    """
    Index the scaffold-to-scaffold mapping BAM
    """
    input:
        bam = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam"
    output:
        bai = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam.bai"
    threads: 1
    resources:
        mem_mb=2000,
        runtime=10
    shell:
        """
        samtools index {input.bam}
        """

rule calculate_contig_mapping_stats:
    """
    Calculate how much each non-chr contig maps to each chr scaffold
    """
    input:
        bam = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam",
        bai = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam.bai",
        contigs_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_contigs.fasta",
        contig_map = config["tool"] + "/output/{nom}/{nom}_contig_to_scaffold_map.tsv",
        chr_list = config["tool"] + "/output/{nom}/{nom}_chr_list.txt"
    output:
        stats = config["tool"] + "/output/{nom}/{nom}_contig_mapping_stats.tsv"
    params:
        min_mapq = config["min_mapq"]
    threads: 1
    resources:
        mem_mb=8000,
        runtime=30
    shell:
        """
        python {parser_script} \
            --bam {input.bam} \
            --non_chr_fasta {input.contigs_fasta} \
            --contig_map {input.contig_map} \
            --chr_list {input.chr_list} \
            --min_mapq {params.min_mapq} \
            --output {output.stats}
        """


rule calculate_LR_coverage:
    """
    Calculate long-read coverage for all contigs
    Merge all LR BAMs and calculate per-contig mean coverage using regions
    """
    input:
        assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
        contig_map = config["tool"] + "/output/{nom}/{nom}_contig_to_scaffold_map.tsv",
        bams = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam",
                      lr_name = config["long_reads"].keys()),
        bais = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam.bai",
                      lr_name = config["long_reads"].keys())
    output:
        coverage = config["tool"] + "/output/{nom}/{nom}_LR_coverage.tsv"
    threads: config["sort_threads"]
    resources:
        mem_mb=16000,
        runtime=120
    shell:
        """
        # Merge all BAMs and sort
        merged_bam=$(mktemp).bam
        samtools merge -@ {threads} $merged_bam {input.bams}
        samtools index -@ {threads} $merged_bam
        
        # Calculate coverage per contig using Python script
        python {coverage_script} \
            --bam $merged_bam \
            --contig_map {input.contig_map} \
            --output {output.coverage} \
            --threads {threads}
        
        # Clean up
        rm -f $merged_bam $merged_bam.bai
        """

rule calculate_chr_coverage_stats:
    """
    Calculate coverage statistics for chromosome-scale scaffolds
    This provides expected coverage distribution for comparison
    """
    input:
        chr_list = config["tool"] + "/output/{nom}/{nom}_chr_list.txt",
        bams = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam",
                      lr_name = config["long_reads"].keys()),
        bais = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam.bai",
                      lr_name = config["long_reads"].keys())
    output:
        json = config["tool"] + "/output/{nom}/{nom}_chr_coverage_stats.json",
        tsv = config["tool"] + "/output/{nom}/{nom}_chr_coverage_stats.tsv"
    threads: config["sort_threads"]
    resources:
        mem_mb=16000,
        runtime=120
    shell:
        """
        # Smart BAM handling: only merge if multiple BAMs
        bam_count=$(echo {input.bams} | wc -w)
        
        if [ $bam_count -eq 1 ]; then
            # Single BAM - use directly
            working_bam={input.bams}
            cleanup_bam=""
        else
            # Multiple BAMs - merge
            working_bam=$(mktemp).bam
            cleanup_bam=$working_bam
            echo "Merging $bam_count BAM files..."
            samtools merge -@ {threads} $working_bam {input.bams}
            samtools index -@ {threads} $working_bam
        fi
        
        # Calculate chromosome coverage statistics
        python {chr_coverage_script} \
            --bam $working_bam \
            --chr_list {input.chr_list} \
            --output_json {output.json} \
            --output_tsv {output.tsv}
        
        # Clean up merged BAM if created
        if [ -n "$cleanup_bam" ]; then
            rm -f $cleanup_bam $cleanup_bam.bai
        fi
        """

rule calculate_binned_contig_coverage:
    """
    Calculate binned coverage (1kb bins) for contigs
    This enables coverage track visualization in dotplots
    """
    input:
        contig_map = config["tool"] + "/output/{nom}/{nom}_contig_to_scaffold_map.tsv",
        bams = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam",
                      lr_name = config["long_reads"].keys()),
        bais = expand(config["tool"] + "/output/bams_LR/{{nom}}/LR_{lr_name}_to_{{nom}}.bam.bai",
                      lr_name = config["long_reads"].keys())
    output:
        json = config["tool"] + "/output/{nom}/{nom}_binned_contig_coverage.json"
    params:
        bin_size = 1000
    threads: config["sort_threads"]
    resources:
        mem_mb=16000,
        runtime=120
    shell:
        """
        # Smart BAM handling: only merge if multiple BAMs
        bam_count=$(echo {input.bams} | wc -w)
        
        if [ $bam_count -eq 1 ]; then
            # Single BAM - use directly
            working_bam={input.bams}
            cleanup_bam=""
        else
            # Multiple BAMs - merge
            working_bam=$(mktemp).bam
            cleanup_bam=$working_bam
            echo "Merging $bam_count BAM files..."
            samtools merge -@ {threads} $working_bam {input.bams}
            samtools index -@ {threads} $working_bam
        fi
        
        # Calculate binned coverage
        python {binned_coverage_script} \
            --bam $working_bam \
            --contig_map {input.contig_map} \
            --output_json {output.json} \
            --bin_size {params.bin_size}
        
        # Clean up merged BAM if created
        if [ -n "$cleanup_bam" ]; then
            rm -f $cleanup_bam $cleanup_bam.bai
        fi
        """

rule generate_summary_table:
    """
    Combine mapping stats and coverage into a single summary table
    """
    input:
        mapping_stats = config["tool"] + "/output/{nom}/{nom}_contig_mapping_stats.tsv",
        coverage = config["tool"] + "/output/{nom}/{nom}_LR_coverage.tsv",
        non_chr_list = config["tool"] + "/output/{nom}/{nom}_non_chr_list.txt"
    output:
        summary = config["tool"] + "/output/{nom}/{nom}_contig_classification_summary.csv"
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    run:
        import pandas as pd
        
        # Read mapping stats
        mapping_df = pd.read_csv(input.mapping_stats, sep='\t')
        
        # Read coverage stats
        coverage_df = pd.read_csv(input.coverage, sep='\t')
        
        # Merge the two dataframes on contig_name
        summary_df = pd.merge(mapping_df, coverage_df, on='contig_name', how='left')
        
        # Fill NaN values with 0 for contigs without coverage
        summary_df['mean_coverage'] = summary_df['mean_coverage'].fillna(0)
        
        # Save as CSV
        summary_df.to_csv(output.summary, index=False)

rule generate_html_report:
    """
    Generate interactive HTML report with plotly
    """
    input:
        summary = config["tool"] + "/output/{nom}/{nom}_contig_classification_summary.csv",
        removed_list = config["tool"] + "/output/{nom}/{nom}_removed_contigs.tsv",
        details_json = config["tool"] + "/output/{nom}/{nom}_trimming_details.json",
        chr_coverage = config["tool"] + "/output/{nom}/{nom}_chr_coverage_stats.json",
        binned_coverage = config["tool"] + "/output/{nom}/{nom}_binned_contig_coverage.json"
    output:
        html = config["tool"] + "/output/{nom}/{nom}_contig_classification_report.html"
    params:
        generate_html = config["generate_html"]
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    shell:
        """
        python {report_script} \
            --input {input.summary} \
            --output_html {output.html} \
            --title "{wildcards.nom} Contig Classification" \
            --removed_contigs {input.removed_list} \
            --trimming_details {input.details_json} \
            --chr_coverage {input.chr_coverage} \
            --binned_coverage {input.binned_coverage}
        """

rule generate_static_plot:
    """
    Generate static PNG/PDF plot
    """
    input:
        summary = config["tool"] + "/output/{nom}/{nom}_contig_classification_summary.csv"
    output:
        png = config["tool"] + "/output/{nom}/{nom}_contig_classification_plot.png"
    params:
        generate_static = config["generate_static"]
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    shell:
        """
        python {report_script} \
            --input {input.summary} \
            --output_static {output.png} \
            --title "{wildcards.nom} Contig Classification"
        """

rule filter_redundant_contigs:
    """
    Filter out redundant contigs based on alignment coverage and mapping quality
    """
    input:
        contigs_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_contigs.fasta",
        summary = config["tool"] + "/output/{nom}/{nom}_contig_classification_summary.csv"
    output:
        filtered_fasta = config["tool"] + "/output/{nom}/{nom}_filtered.fasta",
        removed_list = config["tool"] + "/output/{nom}/{nom}_removed_contigs.tsv"
    params:
        min_coverage = config["filter_min_coverage"],
        min_mapq = config["filter_min_mapq"]
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    shell:
        """
        python {filter_script} \
            --input_fasta {input.contigs_fasta} \
            --summary_csv {input.summary} \
            --output_fasta {output.filtered_fasta} \
            --removed_list {output.removed_list} \
            --min_coverage {params.min_coverage} \
            --min_mapq {params.min_mapq}
        """

rule trim_redundant_regions:
    """
    Trim redundant regions from contigs by breaking at high-quality alignment blocks
    Keeps non-redundant pieces and removes highly redundant swiss-cheese contigs
    """
    input:
        contigs_fasta = config["tool"] + "/output/{nom}/{nom}_non_chr_contigs.fasta",
        bam = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam",
        bai = config["tool"] + "/output/{nom}/{nom}_non_chr_to_chr.bam.bai"
    output:
        trimmed_fasta = config["tool"] + "/output/{nom}/{nom}_trimmed.fasta",
        report = config["tool"] + "/output/{nom}/{nom}_trimming_report.txt",
        details_json = config["tool"] + "/output/{nom}/{nom}_trimming_details.json"
    params:
        min_block_size = config["trim_min_block_size"],
        min_mapq = config["trim_min_mapq"],
        min_keep_piece = config["trim_min_keep_piece"],
        max_redundant_pct = config["trim_max_redundant_pct"],
        aggressive_min_block_size = config["trim_aggressive_min_block_size"],
        aggressive_min_mapq = config["trim_aggressive_min_mapq"]
    threads: 1
    resources:
        mem_mb=8000,
        runtime=30
    shell:
        """
        python {trim_script} \
            --input_fasta {input.contigs_fasta} \
            --bam {input.bam} \
            --output_fasta {output.trimmed_fasta} \
            --report {output.report} \
            --details_json {output.details_json} \
            --min_block_size {params.min_block_size} \
            --min_mapq {params.min_mapq} \
            --min_keep_piece {params.min_keep_piece} \
            --max_redundant_pct {params.max_redundant_pct} \
            --aggressive_min_block_size {params.aggressive_min_block_size} \
            --aggressive_min_mapq {params.aggressive_min_mapq}
        """

rule combine_chr_and_retained_contigs:
    """
    Combine chromosome-scale scaffolds with retained contigs in original order.
    This creates the final curated assembly output.
    
    Uses trimmed.fasta if trimming is enabled, otherwise uses filtered.fasta.
    """
    input:
        original_assembly = config["tool"] + "/input/assembly/{nom}_input.fasta",
        chr_list = config["tool"] + "/output/{nom}/{nom}_chr_list.txt",
        contig_map = config["tool"] + "/output/{nom}/{nom}_contig_to_scaffold_map.tsv",
        retained_contigs = lambda wildcards: (
            config["tool"] + f"/output/{wildcards.nom}/{wildcards.nom}_trimmed.fasta"
            if config["trim_redundant_regions"]
            else config["tool"] + f"/output/{wildcards.nom}/{wildcards.nom}_filtered.fasta"
        )
    output:
        combined_fasta = config["tool"] + "/output/{nom}/{nom}_combined.fasta"
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10
    shell:
        """
        python {combine_script} \
            --input_assembly {input.original_assembly} \
            --chr_list {input.chr_list} \
            --retained_contigs {input.retained_contigs} \
            --contig_map {input.contig_map} \
            --output_fasta {output.combined_fasta}
        """