"""
Uses miniprot to annotate a genome with proteomes from closely-related species.

This works by mapping the provided proteins to a genome, then picking the best protein
  for each location. This is useful for getting a reduced representation of a proteome
  in genome coordinates. This is not useful for keeping all of the proteins/isoforms in
  a genome, as there are many filtering steps.

Steps:
1. Check that the assembly is legal (no duplicate headers)
2. Filter the protein fasta file
    - Remove proteins with duplicate headers.
      - If there are two proteins with the same header ID, like
        "protein_1" and "protein_1", then only the first one is kept.
    - Remove proteins with duplicate sequences.
      - If there are two proteins with the same sequence, like
        "ACDFGHIKLM" and "ACDFGHIKLM", then only the first one is kept.
3. Run miniprot to map the proteins to the genome.
    - Uses the --outn 1 flag to only keep the best alignment for each protein.
4. Make a .chrom file from the miniprot output, filters the alignments.
    - Performs a few steps to get "the best" alignment for each locus.
      - Sorts the alignments by chrom, start, stop, and then by the number of
        mismatches (np), then by the number of matches (ms), and finally by the
        alignment score (AS).
      - Drops exact duplicates. This is definted as exactly the same alignment
        properties for two different proteins.
      - Drops things that have the same end, keeping the one with the best score.
      - Drops things that have the same beginning, keeping the one with the best score.
5. Make a .pep file from the filtered miniprot output.
    - This is a fasta file of the proteins that were kept in the .chrom file.
6. Check that the .pep file is legal (no duplicate headers). Also check that
    the .pep file contains all of the proteins in the .chrom file, and that all of
    the scaffolds in the .chrom file are in the assembly.
"""
from Bio import SeqIO
import os

configfile: "config.yaml"
config["tool"] = "GAP_annotate_miniprot"


snakefile_path = os.path.dirname(os.path.realpath(workflow.snakefile))
miniprot_path = os.path.join(snakefile_path, "../dependencies/miniprot/miniprot")

# Make this break if one fasta is empty
for thisassem in config["assemblies"]:
    bases = 0
    for record in SeqIO.parse(config["assemblies"][thisassem], "fasta"):
        bases += len(record.seq)
    if bases == 0:
        raise IOError("there are no bases in {}".format(thisassem))


from Bio import SeqIO
import sys

rule all:
    input:
        expand(config["tool"] + "/output/miniprots/{assem}_and_{prot}_mapped.gff",
               assem = config["assemblies"], prot = config["proteins"]),
        expand(config["tool"] + "/output/{assem}.chrom",
               assem = config["assemblies"]),
        expand(config["tool"] + "/output/{assem}.pep",
               assem = config["assemblies"]),
        expand(config["tool"] + "/output/{assem}_sample_similarity.pdf",
               assem = config["assemblies"]),
        expand(config["tool"] + "/output/checks/{assem}.check",
               assem = config["assemblies"])

rule check_assembly_legality:
    """
    Checks that each header occurs only once.
    Write the header names to a file.
    """
    input:
        assem = lambda wildcards: config["assemblies"][wildcards.assem]
    output:
        check = config["tool"] + "/input/checks/{assem}.fasta.pass"
    threads: 1
    run:
        seen = set()
        with open(config["assemblies"][wildcards.assem], "r") as f:
            for record in SeqIO.parse(f, "fasta"):
                if record.id in seen:
                    raise IOError("the header {} occurs more than once in {}".format(record.id, config["assemblies"][wildcards.assem]))
                seen.add(record.id)
        with open(output.check, "w") as o:
            for entry in seen:
                o.write(entry + "\n")

# make softlink of the input files
rule softlink_assembly:
    """
    make a softlink of the assembly
    """
    input:
        check = config["tool"] + "/input/checks/{assem}.fasta.pass",
        assem = lambda wildcards: config["assemblies"][wildcards.assem]
    output:
        assem = config["tool"] + "/input/assembly/{assem}.fasta"
    threads:
        1
    shell:
        """
        ln -s {input.assem} {output.assem}
        """

rule filter_proteins:
    """
    Filter the protein fasta file to only include the first instance of each protein
    based on its header and sequence.
    """
    input:
        proteins = [config["proteins"][x] for x in config["proteins"]]
    output:
        filtprot = [config["tool"] + "/input/proteins/{}_filt.fasta".format(x) \
                    for x in config["proteins"]]
    threads: 1
    run:
        seen_header = set()
        seen_sequence = set()
        for thisprot in config["proteins"]:
            # iterate through the proteins
            with open(config["proteins"][thisprot], "r") as f:
                outfile = config["tool"] + "/input/proteins/{}_filt.fasta".format(thisprot)
                with open(outfile, "w") as o:
                    for record in SeqIO.parse(f, "fasta"):
                        # iterate through the records
                        if str(record.id) not in seen_header and \
                           str(record.seq) not in seen_sequence:
                            # if we haven't seen this header or sequence before
                            # then write it out
                            SeqIO.write(record, o, "fasta")
                            seen_header.add(str(record.id))
                            seen_sequence.add(str(record.seq))

rule miniprot:
    """
    Just runs miniprot for one file
    """
    input:
        query = config["tool"] + "/input/proteins/{prot}_filt.fasta",
        assem = config["tool"] + "/input/assembly/{assem}.fasta",
        miniprot = miniprot_path
    output:
        query = config["tool"] + "/output/miniprots/{assem}_and_{prot}_mapped.gff"
    threads: workflow.cores - 1
    shell:
        """
        {input.miniprot} --outn 1 -t {threads} --gff {input.assem} {input.query} > {output.query}
        """

def flag_nested(df):
    """
    returns a dataframe of proteins that are not nested,
      and a list of things that are nested
    """
    df = df.sort_values(
        by        = ["chrom", "start", "stop", "np",  "ms",  "AS"],
        ascending = [True,    True,    True,   False, False, False])
    df.reset_index(drop=True, inplace = True)
    nested = set()
    prevsize = -1
    done = False
    while not done:
        drop_these_rows = set()
        prevstart = 0
        prevstop = 0
        for index, row in df.iterrows():
            thisstart = row["start"]
            thisstop  = row["stop"]
            # check if they are nested
            if thisstart >= prevstart and thisstop <= prevstop:
                # this is nested, so remove
                nested.add(row["protein"])
                drop_these_rows.add(index)
            prevstart = thisstart
            prevstop = thisstop
        df = df.loc[~df.index.isin(drop_these_rows), ]
        print("df size: ", len(df))
        if len(df) == prevsize:
            done = True
        prevsize = len(df)
    return df, list(nested)

rule sp_to_similarity:
    """
    makes a plot showing protein similarity.
    """
    input:
        query = expand(config["tool"] + "/output/miniprots/{{assem}}_and_{prot}_mapped.gff",
                       prot = config["proteins"])
    output:
        pdf = config["tool"] + "/output/{assem}_sample_similarity.pdf"
    params:
        assem = lambda wildcards: wildcards.assem
    threads: 1
    run:
        sp_to_scores = {entry: [] for entry in config["proteins"]}
        for entry in config["proteins"]:
            gff = "{}/output/miniprots/{}_and_{}_mapped.gff".format(
                config["tool"], params.assem, entry)
            with open(gff, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("##PAF"):
                        #sp_to_scores[entry].append(int(line.split("\t")[15].replace("np:i:","")))
                        sp_to_scores[entry].append(int(line.split("\t")[13].replace("AS:i:","")))


        print({x:sum(sp_to_scores[x])/len(sp_to_scores[x]) for x in sp_to_scores})
        import seaborn as sns, matplotlib.pyplot as plt, operator as op

        # sort keys and values together
        sorted_keys, sorted_vals = zip(*sorted(sp_to_scores.items(), key=op.itemgetter(1)))

        # almost verbatim from question
        sns.set(context='notebook', style='whitegrid')
        bp =  sns.boxplot(data=sorted_vals)
        bp.set_yscale("log")

        # category labels
        plt.xticks(plt.xticks()[0], sorted_keys, rotation = 45)

        fig = bp.get_figure()
        plt.xlabel("Groups")
        plt.ylabel("Pos hits")
        plt.tight_layout()
        fig.savefig(output.pdf)

rule make_a_chrom_file:
    """
    cat the tblastn results to make a .chrom file. Used in the odp pipeline.
    """
    input:
        query = expand(config["tool"] + "/output/miniprots/{{assem}}_and_{prot}_mapped.gff",
                       prot = config["proteins"])
    output:
        chrom = config["tool"] + "/output/{assem}.chrom"
    threads: 1
    run:
        import pandas as pd

        entries = []
        seen_prot = set()
        for thisfile in input.query:
            with open(thisfile, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("##PAF"):
                        fields = line.split("\t")
                        protein = fields[1]
                        if protein not in seen_prot:
                            chrom   = fields[6]
                            direc   = fields[5]
                            start   = int(fields[8])
                            stop    = int(fields[9])
                            AS      = int(fields[13].replace("AS:i:",""))
                            AS      = int(fields[13].replace("AS:i:",""))
                            ms      = int(fields[14].replace("ms:i:",""))
                            np      = int(fields[15].replace("np:i:",""))
                            entries.append({"protein":protein, "chrom":chrom, "direc":direc, "start":start, "stop":stop, "AS":AS, "ms": ms, "np":np})
                            seen_prot.add(protein)
        df = pd.DataFrame.from_dict(entries)

        # drop exact duplicates
        df = df.sort_values(by =        ["chrom", "start", "direc", "stop",  "np",  "ms",  "AS"],
                            ascending = [True,    True,    True,    True,    False, False, False])
        #print(df.loc[df.duplicated(subset=["chrom", "direc", "stop", "start"], keep = False)])
        #sys.exit()
        df = df.drop_duplicates(subset=["chrom", "start", "direc", "stop"])

        # drop things that have the same end, keep the one with the best score
        df = df.sort_values(by =        ["chrom", "stop", "direc", "np",  "ms",  "AS"],
                            ascending = [True,    True,    True,    False, False, False])
        #print(df.loc[df.duplicated(subset=["chrom", "direc", "stop"], keep = False)])
        df = df.drop_duplicates(subset=["chrom", "direc", "stop"])

        # drop things that have the same beginning, keep the one with the best score
        print(df)
        df = df.sort_values(by =        ["chrom", "start", "direc", "np",  "ms",  "AS"],
                            ascending = [True,    True,    True,    False, False, False])
        #print(df.loc[df.duplicated(subset=["chrom", "direc", "start"], keep = False)])
        df = df.drop_duplicates(subset=["chrom", "direc", "start"])

        df = df.reset_index(drop=True)
        df = df[["protein", "chrom", "direc", "start", "stop"]]
        df.to_csv(output.chrom, sep = "\t", header = False, index = None)


rule make_pep_files:
    input:
        chrom = config["tool"] + "/output/{assem}.chrom",
        query = [config["proteins"][x] for x in config["proteins"]]
    output:
        pep   = config["tool"] + "/output/{assem}.pep"
    threads: 1
    run:
        prot_dict = {}
        for thispep in input.query:
            with open(thispep) as handle:
                for record in SeqIO.parse(handle, "fasta"):
                    prot_dict[record.id] = record
        outhandle = open(output.pep, "w")
        with open(input.chrom, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    prot = line.split("\t")[0]
                    SeqIO.write(prot_dict[prot], outhandle, "fasta")
        outhandle.close()

rule check_final_files:
    """
    Check that each sequence header occurs only once in the final protein file

    Check that each protein  in the .chrom occurs in the pep file
    Check that each scaffold in the .chrom occurs in the pep file
    """
    input:
        check = config["tool"] + "/input/checks/{assem}.fasta.pass",
        chrom = config["tool"] + "/output/{assem}.chrom",
        pep   = config["tool"] + "/output/{assem}.pep"
    output:
        check = config["tool"] + "/output/checks/{assem}.check"
    threads: 1
    run:
        # read in the assembly header names from the .fasta.pass file
        genomeheaders = []
        with open(input.check, "r") as f:
            for line in f:
                line = line.strip()
                genomeheaders.append(line)

        # read in the protein fasta file and save the headers
        pepheaders = set()
        with open(input.pep, "r") as f:
            for record in SeqIO.parse(f, "fasta"):
                if str(record.id) in pepheaders:
                    raise Exception("Protein {} occurs more than once in the .pep file".format(record.id))
                pepheaders.add(str(record.id))

        # read in the proteins from col1 and the scaffolds from col2 of the chrom file
        # also make sure that the proteins only occur once
        chrom_proteins = set()
        with open(input.chrom, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = line.split("\t")
                    if fields[0] in chrom_proteins:
                        raise Exception("Protein {} occurs more than once in the .chrom file".format(fields[0]))
                    chrom_proteins.add(fields[0])
                    if fields[0] not in pepheaders:
                        raise Exception("Protein {} is in the .chrom file is not in the pep file".format(fields[0]))
                    if fields[1] not in genomeheaders:
                        raise Exception("Scaffold {} is in the .chrom file is not in the genome file".format(fields[1]))

        # now write the file that this has passed checks
        with open(output.check, "w") as f:
            f.write("passed checks\n")
