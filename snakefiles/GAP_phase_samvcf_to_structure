"""
This is a snakefile that takes as input a .sam.gz file, a vcf.gz file, a reference genome in fasta format, and a par file to produce a phased genome map.

Workflow Overview:
1. The input sam.gz files may be very large, so we first split them into smaller files,
   each containing 10 million unique read IDs (which means ~20 million SAM entries for paired-end reads).
2. This splitting is done using a Snakemake checkpoint because we don't know ahead of time
   how many parts will be created (depends on the file size).
3. After splitting, we parallelize the processing of these smaller files through sam2seg.

Checkpoint Workflow:
- The 'split_samgz_by_readid' checkpoint splits the sam.gz file and creates a part_count.txt file
- The 'get_all_split_outputs' function reads the part_count.txt to determine how many parts were created
- Downstream rules can then process each part in parallel using the {part} wildcard

#!/bin/bash
#
#SBATCH --job-name=genpairs
#SBATCH --partition=tttt
#SBATCH --time=6-00:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=32G

# The point of this script is to see if I can get the hickit working with the phasing data previously used.

# Mbo1/NlaIII
BAM=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.bam
SAMGZ=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.sam.gz
VCFGZ=/scratch/users/darrints/donor21_phasing2_202511/GAP_phase/output/final_output/complete_assembly.hap.phased.vcf.gz
PARBED=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/pseudoautosomal.bed
#HICKIT=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/hickit/hickit.js
BINDIR=/home/users/darrints/bin

${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
${BINDIR}/hickit.js sam2seg -v phased_SNP.tsv ${SAMGZ} | \
     ${BINDIR}/hickit.js chronly - | \
     ${BINDIR}/hickit.js bedflt ${PARBED} - | \
     gzip > contacts.seg.gz
${BINDIR}/hickit -i contacts.seg.gz -o - | \
    bgzip > contacts.pairs.gz

Use the example config file from GAP_phase_multi_library.yaml to provide the sam.gz file as input to this module.
"""

import os

configfile: "config.yaml"
config["tool"] = "GAP_phase_samvcf_to_structure"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# For each Hi-C library, require that a .sam.gz file is provided in the config. This won't work otherwise
# Only Hi-C and Chicago libraries need sam.gz files for this workflow

# Validate sam.gz file paths - check for leading/trailing whitespace
for libtype in config["libraries"]:
    # Only check for samgz in Hi-C and Chicago libraries
    if libtype not in ["hic", "chi"]:
        continue
    
    for libname in config["libraries"][libtype]:
        # Check if samgz key exists
        if "samgz" not in config["libraries"][libtype][libname]:
            raise ValueError(
                f"\n\nERROR: samgz file path is required for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please add 'samgz: /path/to/file.sam.gz' to your config file."
            )
        
        samgz_path = config["libraries"][libtype][libname]["samgz"]
        
        # Skip if samgz is None or empty string
        if samgz_path is None or samgz_path == "":
            raise ValueError(
                f"\n\nERROR: samgz file path cannot be None or empty for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please provide a valid path or remove this library from the config."
            )
        
        if samgz_path != samgz_path.strip():
            raise ValueError(
                f"\n\nERROR: samgz file path has leading or trailing whitespace!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n"
                f"Fixed path should be: '{samgz_path.strip()}'\n\n"
                f"Please fix your config file and remove the extra whitespace."
            )
        # make sure the samgz path exists
        if not os.path.isfile(samgz_path):
            raise FileNotFoundError(
                f"\n\nERROR: samgz file not found!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n\n"
                f"Please fix your config file with the correct path."
            )

# Validate that the phased VCF files are provided in the config and do not have whitespace issues
for vcf_key in config["PHASED_VCFs"]:
    phased_vcf_path = config["PHASED_VCFs"][vcf_key]["path"]
    if phased_vcf_path != phased_vcf_path.strip():
        raise ValueError(
            f"\n\nERROR: Phased VCF file path has leading or trailing whitespace!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n"
            f"Fixed path should be: '{phased_vcf_path.strip()}'\n\n"
            f"Please fix your config file and remove the extra whitespace."
        )
    # make sure the phased VCF path exists
    if not os.path.isfile(phased_vcf_path):
        raise FileNotFoundError(
            f"\n\nERROR: Phased VCF file not found!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n\n"
            f"Please fix your config file with the correct path."
        )

# The phased VCF and the sam.gz files are now validated and can be used in the workflow.

# Build library lists for Hi-C and Chicago only (these are the ones we process in this workflow)
config["LIB_NAMES"] = {}
for libtype in ["hic", "chi"]:
    if libtype in config["libraries"]:
        config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
    else:
        config["LIB_NAMES"][libtype] = []

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi"]:
    if len(config["LIB_NAMES"][libtype]) == 0:
        if libtype in config["libraries"]:
            del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data and sam.gz files)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    phasedvcf="[A-Za-z0-9]+",
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    datatype="[A-Za-z]+",
    part="[0-9]+"

######################################################################
# Helper functions for checkpoint aggregation
######################################################################

def get_all_split_outputs(wildcards):
    """
    Aggregation function that depends on the checkpoint.
    This will be called after the checkpoint completes and will determine
    how many parts were created for each library.
    """
    all_outputs = []
    
    for libtype in config["ACTIVE_LIBTYPES"]:
        for libname in config["LIB_NAMES"][libtype]:
            # Access the checkpoint output to get the part_count file
            checkpoint_output = checkpoints.split_samgz_by_readid.get(
                libtype=libtype, libname=libname
            ).output[0]
            
            # Read the part count file to determine how many parts were created
            part_count_file = config["tool"] + f"/output/samgz_split/{libtype}.{libname}.part_count.txt"
            with open(part_count_file, 'r') as f:
                num_parts = int(f.read().strip())
            
            # Generate output file paths for all parts
            for part_num in range(num_parts):
                all_outputs.append(
                    config["tool"] + f"/output/samgz_split/{libtype}.{libname}.part{part_num:04d}.sam.gz"
                )
    
    return all_outputs

rule all:
    input:
        # This will trigger the aggregation function that depends on the checkpoint
        get_all_split_outputs

######################################################################
# Checkpoint: Split sam.gz files by unique read IDs
######################################################################

checkpoint split_samgz_by_readid:
    """
    Split a sam.gz file into multiple smaller files, each containing 10 million unique read IDs.
    Since paired-end reads have the same read ID, each file will have ~20 million SAM entries.
    
    This is a checkpoint because we don't know ahead of time how many parts will be created.
    The number of parts depends on the total number of unique read IDs in the input file.
    
    Output:
    - Multiple .sam.gz files: {libtype}.{libname}.part0000.sam.gz, part0001.sam.gz, etc.
    - A part_count.txt file indicating the total number of parts created
    """
    input:
        samgz = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["samgz"]
    output:
        part_count = config["tool"] + "/output/samgz_split/{libtype}.{libname}.part_count.txt"
    params:
        outdir = config["tool"] + "/output/samgz_split",
        prefix = lambda wildcards: f"{wildcards.libtype}.{wildcards.libname}",
        reads_per_file = 10000000  # 10 million unique read IDs per file
    threads: 2
    resources:
        mem_mb  = 8000,
        runtime = 600  # 10 hours for very large files
    run:
        import gzip
        import os
        from pathlib import Path
        
        # Create output directory
        Path(params.outdir).mkdir(parents=True, exist_ok=True)
        
        print(f"Processing {input.samgz}")
        print(f"Writing parts to {params.outdir} with prefix {params.prefix}")
        print(f"Target: {params.reads_per_file} unique read IDs per file")
        
        # Get file size for progress tracking
        file_size = os.path.getsize(input.samgz)
        print(f"Input file size: {file_size / (1024**3):.2f} GB")
        
        # First pass: Scan file to find split points (byte positions)
        print("\nPass 1: Scanning file to identify split points...")
        headers = []
        split_points = []  # List of (byte_position, read_count) tuples
        current_readids = set()
        last_readid = None
        header_end_pos = 0
        last_progress = -1
        
        with gzip.open(input.samgz, 'rb') as infile:
            # Read headers
            while True:
                pos_before = infile.tell()
                line = infile.readline()
                if not line:
                    break
                if not line.startswith(b'@'):
                    # This is the first alignment line - save position before it
                    header_end_pos = pos_before
                    # Decode and process this first alignment line
                    line_str = line.decode('utf-8')
                    fields = line_str.split('\t', 1)
                    if len(fields) >= 1:
                        readid = fields[0]
                        current_readids.add(readid)
                        last_readid = readid
                    break
                else:
                    headers.append(line.decode('utf-8'))
            
            print(f"Found {len(headers)} header lines")
            print(f"Scanning alignments to find split points...")
            
            # Continue reading alignment lines
            line_count = 1
            for line in infile:
                line_count += 1
                
                # Show progress every 5%
                current_pos = infile.tell()
                progress = int((current_pos / file_size) * 100)
                if progress >= last_progress + 5:
                    last_progress = progress
                    print(f"  Progress: {progress}% - {line_count:,} lines, {len(current_readids):,} unique reads, {len(split_points)} splits")
                
                line_str = line.decode('utf-8')
                fields = line_str.split('\t', 1)
                if len(fields) < 1:
                    continue
                
                readid = fields[0]
                
                # Check if this is a new unique read ID
                if readid != last_readid and readid not in current_readids:
                    current_readids.add(readid)
                    
                    # Check if we've hit the limit for this chunk
                    if len(current_readids) >= params.reads_per_file:
                        # Record the byte position BEFORE this line as a split point
                        # We want to ensure all reads with the same ID stay together
                        split_points.append((infile.tell() - len(line), len(current_readids)))
                        current_readids = set([readid])  # Start new chunk with this read
                
                last_readid = readid
        
        print(f"\nScan complete:")
        print(f"  Total lines: {line_count:,}")
        print(f"  Split points identified: {len(split_points)}")
        print(f"  Will create {len(split_points) + 1} output files")
        
        # Second pass: Write split files based on identified byte positions
        print("\nPass 2: Writing split files...")
        
        split_points = [header_end_pos] + [sp[0] for sp in split_points]
        total_parts = len(split_points)
        
        with gzip.open(input.samgz, 'rb') as infile:
            for part_num in range(len(split_points)):
                outfile_path = os.path.join(params.outdir, f"{params.prefix}.part{part_num:04d}.sam.gz")
                
                # Determine byte range for this part
                start_pos = split_points[part_num]
                end_pos = split_points[part_num + 1] if part_num + 1 < len(split_points) else None
                
                progress_pct = int((part_num / total_parts) * 100)
                print(f"  Writing part {part_num:04d}/{total_parts-1:04d} ({progress_pct}%)...")
                
                with gzip.open(outfile_path, 'wt') as outfile:
                    # Write headers
                    for header in headers:
                        outfile.write(header)
                    
                    # Seek to start position and write until end position
                    infile.seek(start_pos)
                    bytes_to_read = (end_pos - start_pos) if end_pos is not None else None
                    
                    if bytes_to_read is not None:
                        # Read exact byte range
                        chunk_data = infile.read(bytes_to_read)
                        outfile.write(chunk_data.decode('utf-8'))
                    else:
                        # Read to end of file
                        for line in infile:
                            outfile.write(line.decode('utf-8'))
        
        total_parts = len(split_points)
        
        print(f"\nSplitting complete:")
        print(f"  Total parts created: {total_parts}")
        
        # Write the part count to the output file
        with open(output.part_count, 'w') as f:
            f.write(str(total_parts))


