"""
This is a snakefile that takes as input a .sam.gz file, a vcf.gz file, a reference genome in fasta format, and a par file to produce a phased genome map.

Workflow Overview:
1. The input sam.gz files may be very large, so we first split them into smaller files,
   each containing 10 million unique read IDs (which means ~20 million SAM entries for paired-end reads).
2. This splitting is done using a Snakemake checkpoint because we don't know ahead of time
   how many parts will be created (depends on the file size).
3. After splitting, we parallelize the processing of these smaller files through sam2seg.

Checkpoint Workflow:
- The 'split_samgz_by_readid' checkpoint splits the sam.gz file and creates a part_count.txt file
- The 'get_all_split_outputs' function reads the part_count.txt to determine how many parts were created
- Downstream rules can then process each part in parallel using the {part} wildcard

#!/bin/bash
#
#SBATCH --job-name=genpairs
#SBATCH --partition=tttt
#SBATCH --time=6-00:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=32G

# The point of this script is to see if I can get the hickit working with the phasing data previously used.

# Mbo1/NlaIII
BAM=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.bam
SAMGZ=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.sam.gz
VCFGZ=/scratch/users/darrints/donor21_phasing2_202511/GAP_phase/output/final_output/complete_assembly.hap.phased.vcf.gz
PARBED=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/pseudoautosomal.bed
#HICKIT=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/hickit/hickit.js
BINDIR=/home/users/darrints/bin

${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
${BINDIR}/hickit.js sam2seg -v phased_SNP.tsv ${SAMGZ} | \
     ${BINDIR}/hickit.js chronly - | \
     ${BINDIR}/hickit.js bedflt ${PARBED} - | \
     gzip > contacts.seg.gz
${BINDIR}/hickit -i contacts.seg.gz -o - | \
    bgzip > contacts.pairs.gz

Use the example config file from GAP_phase_multi_library.yaml to provide the sam.gz file as input to this module.
"""

import os

configfile: "config.yaml"
config["tool"] = "GAP_phase_samvcf_to_structure"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

# Paths to hickit tools
K8 = os.path.join(filepath, "../bin/k8")
HICKIT_DIR = os.path.join(filepath, "../dependencies/hickit")
HICKIT_JS = os.path.join(HICKIT_DIR, "hickit.js")
HICKIT_BIN = os.path.join(HICKIT_DIR, "hickit")
BEDTOOLS = os.path.join(filepath, "../bin/bedtools")
BGZIP = os.path.join(filepath, "../bin/bgzip")

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# For each Hi-C library, require that a .sam.gz file is provided in the config. This won't work otherwise
# Only Hi-C and Chicago libraries need sam.gz files for this workflow

# Validate sam.gz file paths - check for leading/trailing whitespace
for libtype in config["libraries"]:
    # Only check for samgz in Hi-C and Chicago libraries
    if libtype not in ["hic", "chi"]:
        continue
    
    for libname in config["libraries"][libtype]:
        # Check if samgz key exists
        if "samgz" not in config["libraries"][libtype][libname]:
            raise ValueError(
                f"\n\nERROR: samgz file path is required for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please add 'samgz: /path/to/file.sam.gz' to your config file."
            )
        
        samgz_path = config["libraries"][libtype][libname]["samgz"]
        
        # Skip if samgz is None or empty string
        if samgz_path is None or samgz_path == "":
            raise ValueError(
                f"\n\nERROR: samgz file path cannot be None or empty for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please provide a valid path or remove this library from the config."
            )
        
        if samgz_path != samgz_path.strip():
            raise ValueError(
                f"\n\nERROR: samgz file path has leading or trailing whitespace!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n"
                f"Fixed path should be: '{samgz_path.strip()}'\n\n"
                f"Please fix your config file and remove the extra whitespace."
            )
        # make sure the samgz path exists
        if not os.path.isfile(samgz_path):
            raise FileNotFoundError(
                f"\n\nERROR: samgz file not found!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n\n"
                f"Please fix your config file with the correct path."
            )

# Validate that the phased VCF files are provided in the config and do not have whitespace issues
for vcf_key in config["PHASED_VCFs"]:
    phased_vcf_path = config["PHASED_VCFs"][vcf_key]["path"]
    if phased_vcf_path != phased_vcf_path.strip():
        raise ValueError(
            f"\n\nERROR: Phased VCF file path has leading or trailing whitespace!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n"
            f"Fixed path should be: '{phased_vcf_path.strip()}'\n\n"
            f"Please fix your config file and remove the extra whitespace."
        )
    # make sure the phased VCF path exists
    if not os.path.isfile(phased_vcf_path):
        raise FileNotFoundError(
            f"\n\nERROR: Phased VCF file not found!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n\n"
            f"Please fix your config file with the correct path."
        )

# The phased VCF and the sam.gz files are now validated and can be used in the workflow.

# Build library lists for Hi-C and Chicago only (these are the ones we process in this workflow)
config["LIB_NAMES"] = {}
for libtype in ["hic", "chi"]:
    if libtype in config["libraries"]:
        config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
    else:
        config["LIB_NAMES"][libtype] = []

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi"]:
    if len(config["LIB_NAMES"][libtype]) == 0:
        if libtype in config["libraries"]:
            del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data and sam.gz files)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

# Optional: PAR BED files (Pseudoautosomal regions)
# If not provided, create a dummy "nopar" entry to skip PAR filtering
if "PAR_BEDs" not in config or len(config["PAR_BEDs"]) == 0:
    config["PAR_BEDs"] = {"nopar": {"path": "", "note": "No PAR filtering"}}

# Optional: SNP filter BED files
# If not provided, set to empty dict - workflow will only use "nofilter" option
if "SNP_FILTER_BEDs" not in config:
    config["SNP_FILTER_BEDs"] = {}

wildcard_constraints:
    vcfname="[A-Za-z0-9]+",
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    par="[A-Za-z0-9]+",
    snpfilter="[A-Za-z0-9]+",
    datatype="[A-Za-z]+",
    part="[0-9]+"

# Configuration: reads per chunk
READS_PER_CHUNK = 10_000_000  # 10 million unique read IDs per chunk

# Helper function to get split file outputs from checkpoint
def get_all_split_outputs(wildcards):
    """
    Read the part_count.txt file created by the checkpoint to determine
    how many parts were created, then return all the expected output files.
    """
    checkpoint_output = checkpoints.split_samgz_by_readid.get(
        libtype=wildcards.libtype, 
        libname=wildcards.libname
    ).output.part_count
    
    # Read the part count file
    with open(checkpoint_output, 'r') as f:
        num_parts = int(f.read().strip())
    
    # Return list of all expected split files
    return [
        config["tool"] + f"/output/samgz_split/{wildcards.libtype}.{wildcards.libname}.part{part:04d}.sam.gz"
        for part in range(num_parts)
    ]

# Helper function to get all seg.gz outputs for a library across all PAR beds
def get_all_seg_outputs(wildcards):
    """
    Read the part_count.txt file to determine how many parts were created,
    then return all expected seg.gz files for all PAR beds.
    """
    checkpoint_output = checkpoints.split_samgz_by_readid.get(
        libtype=wildcards.libtype,
        libname=wildcards.libname
    ).output.part_count
    
    # Read the part count file
    with open(checkpoint_output, 'r') as f:
        num_parts = int(f.read().strip())
    
    # Return list of all expected seg.gz files for all PAR beds
    return [
        config["tool"] + f"/output/contacts_seg/{wildcards.vcfname}/{par}/{wildcards.libtype}.{wildcards.libname}.part{part:04d}.seg.gz"
        for par in config["PAR_BEDs"].keys()
        for part in range(num_parts)
    ]

# Helper function to get all pairs.gz outputs for a library across all PAR beds
def get_all_pairs_outputs(wildcards):
    """
    Read the part_count.txt file to determine how many parts were created,
    then return all expected pairs.gz files for all PAR beds.
    """
    checkpoint_output = checkpoints.split_samgz_by_readid.get(
        libtype=wildcards.libtype,
        libname=wildcards.libname
    ).output.part_count
    
    # Read the part count file
    with open(checkpoint_output, 'r') as f:
        num_parts = int(f.read().strip())
    
    # Return list of all expected pairs.gz files for all PAR beds
    return [
        config["tool"] + f"/output/contacts_pairs/{wildcards.vcfname}/{par}/{wildcards.libtype}.{wildcards.libname}.part{part:04d}.pairs.gz"
        for par in config["PAR_BEDs"].keys()
        for part in range(num_parts)
    ]

rule all:
    input:
        # Aggregate all .con.gz files for all VCFs, libraries, PAR beds, and SNP filters
        [config["tool"] + f"/output/contacts_con/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.con.gz"
         for vcfname in config["PHASED_VCFs"].keys()
         for libtype in config["ACTIVE_LIBTYPES"]
         for libname in config["LIB_NAMES"][libtype]
         for par in config["PAR_BEDs"].keys()
         for snpfilter in list(config["SNP_FILTER_BEDs"].keys()) + ["nofilter"]],
        # Aggregate all Hi-C heatmaps
        [config["tool"] + f"/output/hic_heatmaps/{vcfname}/{par}/{snpfilter}/{vcfname}.{par}.{snpfilter}.{libtype}.{libname}.bin2.5Mb.heatmap.png"
         for vcfname in config["PHASED_VCFs"].keys()
         for libtype in config["ACTIVE_LIBTYPES"]
         for libname in config["LIB_NAMES"][libtype]
         for par in config["PAR_BEDs"].keys()
         for snpfilter in list(config["SNP_FILTER_BEDs"].keys()) + ["nofilter"]]

######################################################################
# Checkpoint: Split sam.gz file by counting unique read IDs
######################################################################

checkpoint split_samgz_by_readid:
    """
    Split a sam.gz file into chunks, each containing READS_PER_CHUNK unique read IDs.
    
    Strategy:
    1. Copy sam.gz to $TMPDIR for faster I/O
    2. Read through sequentially, tracking unique read IDs
    3. When we hit READS_PER_CHUNK unique IDs, close current file and start new one
    4. Move completed files from $TMPDIR to target directory
    5. Clean up temporary files
    
    This is a checkpoint because we don't know ahead of time how many parts will be created.
    """
    input:
        samgz = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["samgz"]
    output:
        part_count = config["tool"] + "/output/samgz_split/{libtype}.{libname}/part_count.txt"
    params:
        outdir = config["tool"] + "/output/samgz_split",
        reads_per_chunk = READS_PER_CHUNK
    threads: 2
    resources:
        mem_mb = 8000,
        runtime = 1800  # 30 hours for very large files
    run:
        import gzip
        import os
        import shutil
        from pathlib import Path
        
        # Create output directories
        Path(params.outdir).mkdir(parents=True, exist_ok=True)
        Path(output.part_count).parent.mkdir(parents=True, exist_ok=True)
        
        # Get library info for naming
        libtype = wildcards.libtype
        libname = wildcards.libname
        prefix = f"{libtype}.{libname}"
        
        # Copy sam.gz to TMPDIR
        tmpdir = os.environ.get('TMPDIR', '/tmp')
        local_samgz = os.path.join(tmpdir, f"{prefix}.sam.gz")
        
        print(f"Copying {input.samgz} to {local_samgz}...", flush=True)
        shutil.copy2(input.samgz, local_samgz)
        print(f"Copy complete. Starting split...", flush=True)
        
        # Read headers first
        headers = []
        with gzip.open(local_samgz, 'rt') as infile:
            for line in infile:
                if line.startswith('@'):
                    headers.append(line)
                else:
                    break
        
        print(f"Found {len(headers)} header lines", flush=True)
        
        # Now split by unique read IDs
        part_num = 0
        unique_readids = set()
        current_readid = None
        lines_buffer = []
        
        # Temporary output file in TMPDIR
        tmp_outfile_path = os.path.join(tmpdir, f"{prefix}.part{part_num:04d}.sam.gz")
        outfile = gzip.open(tmp_outfile_path, 'wt', compresslevel=1)
        
        # Write headers to first file
        for header in headers:
            outfile.write(header)
        
        print(f"\nProcessing reads (target: {params.reads_per_chunk:,} unique read IDs per part)...", flush=True)
        print(f"Writing part {part_num:04d}...", flush=True)
        
        with gzip.open(local_samgz, 'rt') as infile:
            for line in infile:
                # Skip headers
                if line.startswith('@'):
                    continue
                
                # Get read ID
                fields = line.split('\t', 1)
                if len(fields) < 1:
                    outfile.write(line)
                    continue
                
                readid = fields[0]
                
                # Track unique read IDs
                if readid not in unique_readids:
                    unique_readids.add(readid)
                    
                    # Check if we've hit the threshold
                    if len(unique_readids) >= params.reads_per_chunk:
                        # Close current file
                        outfile.close()
                        
                        # Move to target directory
                        final_path = os.path.join(params.outdir, f"{prefix}.part{part_num:04d}.sam.gz")
                        shutil.move(tmp_outfile_path, final_path)
                        print(f"Part {part_num:04d} complete: {len(unique_readids):,} unique reads", flush=True)
                        
                        # Start new file
                        part_num += 1
                        unique_readids = set()
                        tmp_outfile_path = os.path.join(tmpdir, f"{prefix}.part{part_num:04d}.sam.gz")
                        outfile = gzip.open(tmp_outfile_path, 'wt', compresslevel=1)
                        
                        # Write headers to new file
                        for header in headers:
                            outfile.write(header)
                        
                        print(f"Writing part {part_num:04d}...", flush=True)
                
                # Write line to current file
                outfile.write(line)
        
        # Close and move the last file
        outfile.close()
        if len(unique_readids) > 0:
            final_path = os.path.join(params.outdir, f"{prefix}.part{part_num:04d}.sam.gz")
            shutil.move(tmp_outfile_path, final_path)
            print(f"Part {part_num:04d} complete: {len(unique_readids):,} unique reads", flush=True)
            part_num += 1
        
        # Clean up local copy
        print(f"\nRemoving local copy: {local_samgz}", flush=True)
        os.remove(local_samgz)
        
        # Write part count to checkpoint file
        total_parts = part_num
        with open(output.part_count, 'w') as f:
            f.write(str(total_parts))
        
        print(f"\nSplitting complete! Created {total_parts} parts", flush=True)

######################################################################
# Aggregate rule to mark completion of splitting
######################################################################

rule aggregate_split_files:
    """
    Dummy rule to aggregate all split files after checkpoint completes.
    This allows the checkpoint to determine how many files were created.
    """
    input:
        get_all_split_outputs
    output:
        done = config["tool"] + "/output/samgz_split/{libtype}.{libname}/complete.txt"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        echo "All split files created for {wildcards.libtype}.{wildcards.libname}" > {output.done}
        """

######################################################################
# Aggregate rule to mark completion of seg.gz processing
######################################################################

rule aggregate_seg_files:
    """
    Aggregate all seg.gz files for a library after all parts are processed
    through sam2seg for all PAR bed configurations.
    """
    input:
        get_all_seg_outputs
    output:
        done = config["tool"] + "/output/contacts_seg/{vcfname}/{libtype}.{libname}/complete.txt"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        echo "All seg.gz files created for {wildcards.vcfname} {wildcards.libtype}.{wildcards.libname}" > {output.done}
        """

##############################################################################
# Step 2: make a rule to get the SNPs for dip-c/hickit's input requirements
##############################################################################

rule softlinks_phased_vcf:
    input:
        # we make softlinks of the phased VCFs here in the input directory
        vcf = lambda wildcards: config["PHASED_VCFs"][wildcards.vcfname]["path"]
    output:
        vcf_link = config["tool"] + "/input/phased_vcfs/{vcfname}_phased.vcf.gz"
    params:
        outdir = config["tool"] + "/input/phased_vcfs"
    threads: 1
    resources:
        mem_mb  = 1000,
        runtime = 10
    shell:
        """
        mkdir -p {params.outdir}
        ln -sf {input.vcf} {output.vcf_link}
        """

rule process_phased_vcf:
    """
    Does this step from the hickit pipeline
        ${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
    
    Strategy:
    1. Verify input VCF is properly gzipped
    2. Copy VCF to $TMPDIR using utility functions
    3. Process locally for faster I/O
    4. Copy TSV to destination with verification
    5. Clean up temporary files
    """
    input:
        vcf = config["tool"] + "/input/phased_vcfs/{vcfname}_phased.vcf.gz"
    output:
        tsv = config["tool"] + "/output/phased_vcfs/{vcfname}_phased_SNP.tsv"
    params:
        bindir = HICKIT_DIR
    threads: 1
    retries: 4
    resources:
        mem_mb  = 7999,
        runtime = 180
    shell:
        """
        # Source utility functions
        source {filepath}/../scripts/file_utils.sh
        
        # Get unique job ID for temporary files
        JOB_ID=$(get_job_id)
        TEMP_DIR=$TMPDIR/vcf2tsv_${{JOB_ID}}_{wildcards.vcfname}
        mkdir -p $TEMP_DIR
        
        # Setup local paths
        LOCAL_VCF=$TEMP_DIR/$(basename {input.vcf})
        LOCAL_TSV=$TEMP_DIR/{wildcards.vcfname}_phased_SNP.tsv
        
        echo "Working directory: $TEMP_DIR"
        
        # Step 1: Verify input VCF is properly gzipped
        verify_gzipped --file {input.vcf} --description "input VCF"
        
        # Step 2: Copy VCF to local node with retry logic
        copy_with_retry --source {input.vcf} --dest $LOCAL_VCF --description "phased VCF"
        
        # Step 3: Process from local disk
        echo "Processing VCF to TSV..."
        {HICKIT_JS} vcf2tsv $LOCAL_VCF > $LOCAL_TSV
        
        # Step 4: Copy TSV to destination with verification
        echo "Copying TSV to destination..."
        mkdir -p $(dirname {output.tsv})
        copy_with_retry --source $LOCAL_TSV --dest {output.tsv} --description "phased SNP TSV"
        
        # Step 5: Clean up temporary directory
        echo "Cleaning up temporary directory: $TEMP_DIR"
        rm -rf $TEMP_DIR
        
        echo "VCF processing complete"
        """

######################################################################
# Step 3: make a rule to run sam2seg on each split sam.gz file
######################################################################

rule sam2seg_per_split:
    """
    Process each split sam.gz file through the hickit pipeline:
        hickit.js sam2seg -v phased_SNP.tsv ${SAMGZ} | \
            hickit.js chronly - | \
            hickit.js bedflt ${PARBED} - | \
            gzip > contacts.seg.gz
    
    This processes each split sam.gz part in parallel.
    
    Strategy: 
    1. Verify input sam.gz is properly gzipped
    2. Copy sam.gz and TSV to $TMPDIR using utility functions
    3. Process locally for faster I/O
    4. Verify output seg.gz is properly gzipped
    5. Copy seg.gz to destination with verification
    6. Clean up temporary files
    """
    input:
        samgz = config["tool"] + "/output/samgz_split/{libtype}.{libname}.part{part}.sam.gz",
        tsv = config["tool"] + "/output/phased_vcfs/{vcfname}_phased_SNP.tsv",
        parbed = lambda wildcards: config["PAR_BEDs"][wildcards.par]["path"] if wildcards.par != "nopar" else []
    output:
        seg = temp(config["tool"] + "/output/contacts_seg/{vcfname}/{par}/{libtype}.{libname}.part{part}.seg.gz")
    threads: 1
    retries: 4
    resources:
        mem_mb = 8000,
        runtime = 120
    shell:
        """
        # Source utility functions
        source {filepath}/../scripts/file_utils.sh
        
        # Get unique job ID for temporary files
        JOB_ID=$(get_job_id)
        TEMP_DIR=$TMPDIR/sam2seg_${{JOB_ID}}_{wildcards.libtype}_{wildcards.libname}_{wildcards.part}
        mkdir -p $TEMP_DIR
        
        # Setup local paths
        LOCAL_SAMGZ=$TEMP_DIR/$(basename {input.samgz})
        LOCAL_TSV=$TEMP_DIR/$(basename {input.tsv})
        LOCAL_SEG=$TEMP_DIR/$(basename {output.seg})
        
        echo "Working directory: $TEMP_DIR"
        
        # Step 1: Verify input sam.gz is properly gzipped
        verify_gzipped --file {input.samgz} --description "input sam.gz"
        
        # Step 2: Copy sam.gz to local node with retry logic
        copy_with_retry --source {input.samgz} --dest $LOCAL_SAMGZ --description "split sam.gz"
        
        # Step 3: Copy TSV to local node with retry logic
        copy_with_retry --source {input.tsv} --dest $LOCAL_TSV --description "phased SNP TSV"
        
        # Step 4: BED file stays where it is (network storage)
        
        # Step 5: Process with hickit pipeline on local node
        echo "Processing with hickit pipeline..."
        if [ "{wildcards.par}" != "nopar" ]; then
            # With PAR bed filtering
            echo "Using PAR bed: {input.parbed}"
            {K8} {HICKIT_JS} sam2seg -v $LOCAL_TSV $LOCAL_SAMGZ | \
                {K8} {HICKIT_JS} chronly - | \
                {K8} {HICKIT_JS} bedflt {input.parbed} - | \
                gzip > $LOCAL_SEG
        else
            # Without PAR bed filtering
            echo "No PAR bed filtering"
            {K8} {HICKIT_JS} sam2seg -v $LOCAL_TSV $LOCAL_SAMGZ | \
                {K8} {HICKIT_JS} chronly - | \
                gzip > $LOCAL_SEG
        fi
        
        # Step 6: Verify output seg.gz is properly gzipped (fail if not)
        verify_gzipped --file $LOCAL_SEG --description "output seg.gz"
        
        # Step 7: Copy seg.gz to destination with verification
        echo "Copying seg.gz to destination with verification..."
        mkdir -p $(dirname {output.seg})
        copy_with_retry --source $LOCAL_SEG --dest {output.seg} --description "output seg.gz"
        
        # Step 8: Clean up temporary directory
        echo "Cleaning up temporary directory: $TEMP_DIR"
        rm -rf $TEMP_DIR
        
        echo "sam2seg processing complete"
        """

######################################################################
# Step 4: Convert seg.gz files to pairs.gz using hickit
######################################################################

rule seg_to_pairs:
    """
    Convert contacts.seg.gz files to pairs.gz format using hickit:
        hickit -i contacts.seg.gz -o - | bgzip > contacts.pairs.gz
    
    This processes each seg.gz file independently to generate pairs.gz files.
    
    Strategy:
    1. Verify input seg.gz is properly gzipped
    2. Copy seg.gz to $TMPDIR using utility functions
    3. Process locally with hickit binary
    4. Verify output pairs.gz is properly gzipped
    5. Copy pairs.gz to destination with verification
    6. Clean up temporary files
    """
    input:
        seg = config["tool"] + "/output/contacts_seg/{vcfname}/{par}/{libtype}.{libname}.part{part}.seg.gz"
    output:
        pairs = temp(config["tool"] + "/output/contacts_pairs/{vcfname}/{par}/{libtype}.{libname}.part{part}.pairs.gz")
    threads: 1
    retries: 4
    resources:
        mem_mb = 7999,
        runtime = 120
    shell:
        """
        # Source utility functions
        source {filepath}/../scripts/file_utils.sh
        
        # Get unique job ID for temporary files
        JOB_ID=$(get_job_id)
        TEMP_DIR=$TMPDIR/seg2pairs_${{JOB_ID}}_{wildcards.libtype}_{wildcards.libname}_{wildcards.part}
        mkdir -p $TEMP_DIR
        
        # Setup local paths
        LOCAL_SEG=$TEMP_DIR/$(basename {input.seg})
        LOCAL_PAIRS=$TEMP_DIR/$(basename {output.pairs})
        
        echo "Working directory: $TEMP_DIR"
        
        # Step 1: Verify input seg.gz is properly gzipped
        verify_gzipped --file {input.seg} --description "input seg.gz"
        
        # Step 2: Copy seg.gz to local node with retry logic
        copy_with_retry --source {input.seg} --dest $LOCAL_SEG --description "seg.gz file"
        
        # Step 3: Process with hickit binary on local node
        echo "Converting seg.gz to pairs.gz with hickit..."
        {HICKIT_BIN} -i $LOCAL_SEG -o - | {BGZIP} > $LOCAL_PAIRS
        
        # Step 4: Verify output pairs.gz is properly gzipped (fail if not)
        verify_gzipped --file $LOCAL_PAIRS --description "output pairs.gz"
        
        # Step 5: Copy pairs.gz to destination with verification
        echo "Copying pairs.gz to destination with verification..."
        mkdir -p $(dirname {output.pairs})
        copy_with_retry --source $LOCAL_PAIRS --dest {output.pairs} --description "output pairs.gz"
        
        # Step 6: Clean up temporary directory
        echo "Cleaning up temporary directory: $TEMP_DIR"
        rm -rf $TEMP_DIR
        
        echo "seg_to_pairs processing complete"
        """

######################################################################
# Aggregate rule to mark completion of pairs.gz processing
######################################################################

rule aggregate_pairs_files:
    """
    Aggregate all pairs.gz files for a library after all parts are processed
    through hickit for all PAR bed configurations.
    """
    input:
        get_all_pairs_outputs
    output:
        done = config["tool"] + "/output/contacts_pairs/{vcfname}/{libtype}.{libname}/complete.txt"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        echo "All pairs.gz files created for {wildcards.vcfname} {wildcards.libtype}.{wildcards.libname}" > {output.done}
        """

######################################################################
# Step 5: Merge and sort all pairs.gz files for each library+PAR 
######################################################################

rule merge_and_sort_pairs:
    """
    Merge and sort all pairs.gz parts for a library+PAR combination.
    
    Takes all part*.pairs.gz files and:
    1. Verifies all input files are properly gzipped
    2. Decompresses with zcat
    3. Removes header lines with grep -v '^#'
    4. Sorts by chr2, chr4, pos3, pos5 with parallel sort
    5. Compresses with bgzip
    6. Verifies output and copies to destination
    7. Cleans up temporary files
    
    This creates a single merged and sorted pairs.gz file per library+PAR combination.
    
    Memory estimation: ~26x the total compressed input size
    (Based on empirical observation: 6.8GB compressed â†’ 174GB RAM needed)
    """
    input:
        lambda wildcards: [
            config["tool"] + f"/output/contacts_pairs/{wildcards.vcfname}/{wildcards.par}/{wildcards.libtype}.{wildcards.libname}.part{part:04d}.pairs.gz"
            for part in range(
                int(open(checkpoints.split_samgz_by_readid.get(
                    libtype=wildcards.libtype,
                    libname=wildcards.libname
                ).output.part_count).read().strip())
            )
        ]
    output:
        merged = config["tool"] + "/output/contacts_pairs_merged/{vcfname}/{par}/{libtype}.{libname}.merged.sorted.pairs.gz"
    threads: 64
    resources:
        mem_mb = lambda wildcards, input: int(sum(os.path.getsize(f) for f in input) / (1024**2) * 26 * 1.5),
        runtime = lambda wildcards, input: int(sum(os.path.getsize(f) for f in input) / (1024**3) * 88) # 88 minutes per GB of pairs.gz. Should take 11 minutes per GB but some systems are slow.
    shell:
        """
        # Source utility functions
        source {filepath}/../scripts/file_utils.sh
        
        echo "Merge and sort job started at $(date)"
        
        # Get unique job ID (SLURM-aware)
        JOB_ID=$(get_job_id)
        TEMP_DIR=$TMPDIR/merge_sort_${{JOB_ID}}_{wildcards.libtype}_{wildcards.libname}
        mkdir -p $TEMP_DIR
        
        echo "Working directory: $TEMP_DIR"
        
        # Step 1: Verify all input files are properly gzipped
        echo "Verifying all input files are gzipped..."
        for FILE in {input}; do
            verify_gzipped --file "$FILE" --description "pairs.gz input"
        done
        echo "All input files verified as gzipped"
        
        # Calculate buffer size (80% of allocated memory for sort buffer)
        BUFFER_SIZE=$(echo "{resources.mem_mb} * 0.8" | bc | cut -d. -f1)

        LOCAL_MERGED=$TEMP_DIR/merged_pairs.gz
        
        echo "Writing output to local file: $LOCAL_MERGED"

        # Step 2: Extract header from first file
        zcat {input[0]} | grep '^#' | {BGZIP} > $LOCAL_MERGED

        # Step 3: Merge all parts (excluding headers), sort, and append
        zcat {input} | \
        grep -v '^#' | \
        sort -k2,2V -k4,4V -k3,3n -k5,5n --parallel={threads} --buffer-size=${{BUFFER_SIZE}}M | \
        {BGZIP} >> $LOCAL_MERGED
        
        # Step 4: Verify output is properly gzipped (fail if not)
        verify_gzipped --file $LOCAL_MERGED --description "merged pairs.gz output"
        
        # Step 5: Copy to destination with verification
        echo "Copying merged file to destination with verification..."
        mkdir -p $(dirname {output.merged})
        copy_with_retry --source $LOCAL_MERGED --dest {output.merged} --description "merged pairs.gz"
        
        # Step 6: Clean up temporary directory
        echo "Cleaning up temporary directory: $TEMP_DIR"
        rm -rf $TEMP_DIR
        
        echo "Merge and sort job finished at $(date)"
        """

rule filter_phased_pairs:
    """
    Filter merged pairs.gz to keep only rows where BOTH phase columns (8 and 9) are not ".".
    This removes unphaseable contacts before applying SNP-based filtering.
    
    Keeps the header lines (starting with #) and only data rows with valid phasing info.
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_merged/{vcfname}/{par}/{libtype}.{libname}.merged.sorted.pairs.gz"
    output:
        phased = config["tool"] + "/output/contacts_pairs_phased/{vcfname}/{par}/{libtype}.{libname}.phased.pairs.gz"
    threads: 4
    resources:
        mem_mb = 8000,
        runtime = 120
    shell:
        """
        echo "Filtering pairs to keep only phased contacts..."
        
        # Keep header and filter data rows where columns 8 AND 9 are not "."
        zcat {input.pairs} | \
          awk 'BEGIN{{OFS="\\t"}}
               /^#/ {{print; next}}
               $8 != "." && $9 != "." {{print}}' | \
          {BGZIP} > {output.phased}
        
        echo "Phased pairs filtering complete"
        """

rule make_pairs_side1_bed:
    """
    Create BED file for side 1 (first contact end) with line numbers.
    Side 1: chrom (chr2=$2), chromStart (pos3-1), chromEnd (pos3=$3), name (line number=NR)
    
    This is computed once per library+PAR combination and reused across all SNP filters.
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_phased/{vcfname}/{par}/{libtype}.{libname}.phased.pairs.gz"
    output:
        side1 = config["tool"] + "/output/contacts_pairs_sides/{vcfname}/{par}/{libtype}.{libname}.side1.bed"
    threads: 2
    resources:
        mem_mb = 7999,
        runtime = 59
    shell:
        """
        echo "Creating side 1 BED file with line numbers..."
        zcat {input.pairs} | grep -v '^#' | \
          awk 'BEGIN{{OFS="\\t"}} {{print $2, $3-1, $3, NR}}' > {output.side1}
        echo "Side 1 BED file created"
        """

rule make_pairs_side2_bed:
    """
    Create BED file for side 2 (second contact end) with line numbers.
    Side 2: chrom (chr4=$4), chromStart (pos5-1), chromEnd (pos5=$5), name (line number=NR)
    
    This is computed once per library+PAR combination and reused across all SNP filters.
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_phased/{vcfname}/{par}/{libtype}.{libname}.phased.pairs.gz"
    output:
        side2 = config["tool"] + "/output/contacts_pairs_sides/{vcfname}/{par}/{libtype}.{libname}.side2.bed"
    threads: 2
    resources:
        mem_mb = 7999,
        runtime = 59
    shell:
        """
        echo "Creating side 2 BED file with line numbers..."
        zcat {input.pairs} | grep -v '^#' | \
          awk 'BEGIN{{OFS="\\t"}} {{print $4, $5-1, $5, NR}}' > {output.side2}
        echo "Side 2 BED file created"
        """

rule filter_pairs:
    """
    Filter merged pairs.gz files through BED regions of interest.
    
    Keeps only pairs where BOTH ends fall within the BED regions.
    Special case: snpfilter='nofilter' passes through unfiltered.
    
    Process:
    1. Verify input pairs.gz is properly gzipped
    2. Use precomputed side1 and side2 BED files (created once per library+PAR)
    3. Copy files to TMPDIR using utility functions
    4. Find line numbers where side 1 intersects the BED regions
    5. Find line numbers where side 2 intersects the BED regions
    6. Keep only line numbers that passed filtering on BOTH sides
    7. Extract header and those specific lines from the original pairs file
    8. Verify output and copy to destination
    9. Clean up temporary files
    
    This approach uses line numbers instead of read IDs since all read IDs may be "."
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_phased/{vcfname}/{par}/{libtype}.{libname}.phased.pairs.gz",
        side1 = config["tool"] + "/output/contacts_pairs_sides/{vcfname}/{par}/{libtype}.{libname}.side1.bed",
        side2 = config["tool"] + "/output/contacts_pairs_sides/{vcfname}/{par}/{libtype}.{libname}.side2.bed",
        bed = lambda wildcards: config["SNP_FILTER_BEDs"][wildcards.snpfilter]["path"] if wildcards.snpfilter != "nofilter" else []
    output:
        filtered = config["tool"] + "/output/contacts_pairs_filtered/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.filtered.pairs.gz"
    threads: 4
    retries: 4
    resources:
        mem_mb = 32000,
        runtime = 180
    shell:
        """
        # Source utility functions
        source {filepath}/../scripts/file_utils.sh
        
        if [ "{wildcards.snpfilter}" = "nofilter" ]; then
            # No filtering - verify and copy the input
            echo "No SNP filter applied, verifying and copying input..."
            
            # Verify input is properly gzipped
            verify_gzipped --file {input.pairs} --description "input phased pairs.gz"
            
            # Copy with verification
            mkdir -p $(dirname {output.filtered})
            copy_with_retry --source {input.pairs} --dest {output.filtered} --description "unfiltered pairs.gz"
        else
            echo "Filtering pairs with BED: {input.bed}"
            
            # Get unique job ID for temporary files
            JOB_ID=$(get_job_id)
            TEMP_DIR=$TMPDIR/filter_pairs_${{JOB_ID}}_{wildcards.libtype}_{wildcards.libname}
            mkdir -p $TEMP_DIR
            
            echo "Working directory: $TEMP_DIR"
            
            # Setup local paths
            PAIRS_TMP=$TEMP_DIR/pairs.gz
            BED_TMP=$TEMP_DIR/bed.bed
            SIDE1_BED=$TEMP_DIR/side1.bed
            SIDE2_BED=$TEMP_DIR/side2.bed
            SIDE1_IDS=$TEMP_DIR/side1_ids.txt
            SIDE2_IDS=$TEMP_DIR/side2_ids.txt
            KEEP_IDS=$TEMP_DIR/keep_ids.txt
            OUTPUT_TMP=$TEMP_DIR/filtered.pairs.gz
            
            # Step 1: Verify input pairs.gz is properly gzipped
            verify_gzipped --file {input.pairs} --description "input phased pairs.gz"
            
            # Step 2-3: Copy input files to TMPDIR with retry logic
            echo "Copying input files to TMPDIR for faster I/O..."
            copy_with_retry --source {input.pairs} --dest $PAIRS_TMP --description "phased pairs.gz"
            copy_with_retry --source {input.bed} --dest $BED_TMP --description "SNP filter BED"
            copy_with_retry --source {input.side1} --dest $SIDE1_BED --description "side1 BED"
            copy_with_retry --source {input.side2} --dest $SIDE2_BED --description "side2 BED"
            
            # Step 4: Find line numbers where side 1 intersects BED regions
            echo "Finding line numbers where side 1 intersects BED regions..."
            {BEDTOOLS} intersect -a $SIDE1_BED -b $BED_TMP -wa | \
              cut -f4 | sort --parallel={threads} --buffer-size=24G -u > $SIDE1_IDS
            
            # Step 5: Find line numbers where side 2 intersects BED regions
            echo "Finding line numbers where side 2 intersects BED regions..."
            {BEDTOOLS} intersect -a $SIDE2_BED -b $BED_TMP -wa | \
              cut -f4 | sort --parallel={threads} --buffer-size=24G -u > $SIDE2_IDS
            
            # Step 6: Keep only line numbers that passed filtering on BOTH sides
            echo "Finding line numbers that passed filtering on BOTH sides..."
            comm -12 $SIDE1_IDS $SIDE2_IDS > $KEEP_IDS
            
            KEEP_COUNT=$(wc -l < $KEEP_IDS)
            echo "Keeping $KEEP_COUNT read pairs that passed filtering on both sides"
            
            # Step 7: Filter pairs file
            echo "Filtering pairs file..."
            pigz -dc -p {threads} $PAIRS_TMP | \
              awk 'BEGIN{{while((getline < "'$KEEP_IDS'") > 0) keep[$1]=1}}
                   /^#/ {{print; next}}
                   {{line++; if(keep[line]) print}}' | \
              pigz -p {threads} > $OUTPUT_TMP
            
            # Step 8: Verify output and copy to destination
            verify_gzipped --file $OUTPUT_TMP --description "filtered pairs.gz output"
            
            echo "Copying filtered pairs to destination with verification..."
            mkdir -p $(dirname {output.filtered})
            copy_with_retry --source $OUTPUT_TMP --dest {output.filtered} --description "filtered pairs.gz"
            
            # Step 9: Clean up temporary directory
            echo "Cleaning up temporary directory: $TEMP_DIR"
            rm -rf $TEMP_DIR
            
            echo "Filtering complete"
        fi
        """

######################################################################
# Step 6: Convert filtered pairs.gz to .con.gz format
######################################################################

rule pairs_to_con:
    """
    Convert filtered pairs.gz files to .con.gz format for dip-c analysis.
    
    Format conversion:
    - Input (pairs.gz): readID chr1 pos1 chr2 pos2 strand1 strand2 phase1 phase2
    - Output (con.gz): chr1,pos1,phase1 chr2,pos2,phase2
    
    Only processes rows where both phase columns (8 and 9) are not "." (already filtered upstream).
    Uses pigz for parallel compression with available threads.
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_filtered/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.filtered.pairs.gz"
    output:
        con = config["tool"] + "/output/contacts_con/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.con.gz"
    threads: 2
    resources:
        mem_mb = 7999,
        runtime = 60
    shell:
        """
        echo "Converting pairs.gz to con.gz format..."
        
        # Extract phased contacts and format as: chr1,pos1,phase1 chr2,pos2,phase2
        zcat {input.pairs} | \
          awk -F'\\t' 'BEGIN {{ OFS="\\t" }}
               !/^#/ && $8 != "." && $9 != "." {{
                   print $2","$3","$8, $4","$5","$9
               }}' | \
          pigz -p {threads} > {output.con}
        
        echo "Conversion complete"
        """

######################################################################
# Step 7: Bin pairs into contact matrix
######################################################################

rule bin_pairs_to_matrix:
    """
    Bin filtered pairs.gz files into contact matrix for visualization.
    
    Takes filtered pairs.gz files and bins contacts into 2.5 Mb bins (configurable).
    Creates diploid chromosome representation (chr1.0, chr1.1, ..., chrX.0, chrX.1, chrY.0, chrY.1).
    Stores upper triangle only for efficiency.
    
    Multiple pairs.gz files (from different libraries/filters) can be merged by providing
    them all as input - the rule will accumulate all contacts into a single matrix.
    """
    input:
        pairs = config["tool"] + "/output/contacts_pairs_filtered/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.filtered.pairs.gz"
    output:
        matrix = config["tool"] + "/output/contact_matrices/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.matrix.npz",
        metadata = config["tool"] + "/output/contact_matrices/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.matrix.metadata.npz"
    params:
        bin_size = 2500000  # 2.5 Mb bins
    threads: 1
    resources:
        mem_mb = 16000,
        runtime = 120
    shell:
        """
        python {filepath}/../scripts/bin_pairs.py \
          --input {input.pairs} \
          --output {output.matrix} \
          --bin-size {params.bin_size}
        """

######################################################################
# Step 8: Plot Hi-C heatmap
######################################################################

rule plot_hic_heatmap:
    """
    Generate Hi-C contact heatmap from binned matrix.
    
    Creates a publication-quality heatmap with:
    - Log-scale coloring (white to #B10000)
    - Symmetric matrix across diagonal
    - Chromosome boundaries and labels
    - Native resolution output (2500x2500 pixels for 2.5 Mb bins)
    """
    input:
        matrix = config["tool"] + "/output/contact_matrices/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.matrix.npz",
        metadata = config["tool"] + "/output/contact_matrices/{vcfname}/{par}/{snpfilter}/{libtype}.{libname}.matrix.metadata.npz"
    output:
        heatmap = config["tool"] + "/output/hic_heatmaps/{vcfname}/{par}/{snpfilter}/{vcfname}.{par}.{snpfilter}.{libtype}.{libname}.bin2.5Mb.heatmap.png"
    threads: 1
    resources:
        mem_mb = 16000,  # Need memory to convert sparse to dense
        runtime = 30
    shell:
        """
        python {filepath}/../scripts/plot_hic.py \
          --input {input.matrix} \
          --output {output.heatmap}
        """
