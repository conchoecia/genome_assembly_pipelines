"""
This is a snakefile that takes as input a .sam.gz file, a vcf.gz file, a reference genome in fasta format, and a par file to produce a phased genome map.

Workflow Overview:
1. The input sam.gz files may be very large, so we first split them into smaller files,
   each containing 10 million unique read IDs (which means ~20 million SAM entries for paired-end reads).
2. This splitting is done using a Snakemake checkpoint because we don't know ahead of time
   how many parts will be created (depends on the file size).
3. After splitting, we parallelize the processing of these smaller files through sam2seg.

Checkpoint Workflow:
- The 'split_samgz_by_readid' checkpoint splits the sam.gz file and creates a part_count.txt file
- The 'get_all_split_outputs' function reads the part_count.txt to determine how many parts were created
- Downstream rules can then process each part in parallel using the {part} wildcard

#!/bin/bash
#
#SBATCH --job-name=genpairs
#SBATCH --partition=tttt
#SBATCH --time=6-00:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=32G

# The point of this script is to see if I can get the hickit working with the phasing data previously used.

# Mbo1/NlaIII
BAM=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.bam
SAMGZ=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.sam.gz
VCFGZ=/scratch/users/darrints/donor21_phasing2_202511/GAP_phase/output/final_output/complete_assembly.hap.phased.vcf.gz
PARBED=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/pseudoautosomal.bed
#HICKIT=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/hickit/hickit.js
BINDIR=/home/users/darrints/bin

${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
${BINDIR}/hickit.js sam2seg -v phased_SNP.tsv ${SAMGZ} | \
     ${BINDIR}/hickit.js chronly - | \
     ${BINDIR}/hickit.js bedflt ${PARBED} - | \
     gzip > contacts.seg.gz
${BINDIR}/hickit -i contacts.seg.gz -o - | \
    bgzip > contacts.pairs.gz

Use the example config file from GAP_phase_multi_library.yaml to provide the sam.gz file as input to this module.
"""

import os

configfile: "config.yaml"
config["tool"] = "GAP_phase_samvcf_to_structure"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# For each Hi-C library, require that a .sam.gz file is provided in the config. This won't work otherwise
# Only Hi-C and Chicago libraries need sam.gz files for this workflow

# Validate sam.gz file paths - check for leading/trailing whitespace
for libtype in config["libraries"]:
    # Only check for samgz in Hi-C and Chicago libraries
    if libtype not in ["hic", "chi"]:
        continue
    
    for libname in config["libraries"][libtype]:
        # Check if samgz key exists
        if "samgz" not in config["libraries"][libtype][libname]:
            raise ValueError(
                f"\n\nERROR: samgz file path is required for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please add 'samgz: /path/to/file.sam.gz' to your config file."
            )
        
        samgz_path = config["libraries"][libtype][libname]["samgz"]
        
        # Skip if samgz is None or empty string
        if samgz_path is None or samgz_path == "":
            raise ValueError(
                f"\n\nERROR: samgz file path cannot be None or empty for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please provide a valid path or remove this library from the config."
            )
        
        if samgz_path != samgz_path.strip():
            raise ValueError(
                f"\n\nERROR: samgz file path has leading or trailing whitespace!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n"
                f"Fixed path should be: '{samgz_path.strip()}'\n\n"
                f"Please fix your config file and remove the extra whitespace."
            )
        # make sure the samgz path exists
        if not os.path.isfile(samgz_path):
            raise FileNotFoundError(
                f"\n\nERROR: samgz file not found!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n\n"
                f"Please fix your config file with the correct path."
            )

# Validate that the phased VCF files are provided in the config and do not have whitespace issues
for vcf_key in config["PHASED_VCFs"]:
    phased_vcf_path = config["PHASED_VCFs"][vcf_key]["path"]
    if phased_vcf_path != phased_vcf_path.strip():
        raise ValueError(
            f"\n\nERROR: Phased VCF file path has leading or trailing whitespace!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n"
            f"Fixed path should be: '{phased_vcf_path.strip()}'\n\n"
            f"Please fix your config file and remove the extra whitespace."
        )
    # make sure the phased VCF path exists
    if not os.path.isfile(phased_vcf_path):
        raise FileNotFoundError(
            f"\n\nERROR: Phased VCF file not found!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n\n"
            f"Please fix your config file with the correct path."
        )

# The phased VCF and the sam.gz files are now validated and can be used in the workflow.

# Build library lists for Hi-C and Chicago only (these are the ones we process in this workflow)
config["LIB_NAMES"] = {}
for libtype in ["hic", "chi"]:
    if libtype in config["libraries"]:
        config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
    else:
        config["LIB_NAMES"][libtype] = []

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi"]:
    if len(config["LIB_NAMES"][libtype]) == 0:
        if libtype in config["libraries"]:
            del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data and sam.gz files)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    phasedvcf="[A-Za-z0-9]+",
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    datatype="[A-Za-z]+",
    part="[0-9]+"

# Configuration: Number of chunks to split each sam.gz file into
NUM_CHUNKS = 100

rule all:
    input:
        # Generate all expected split files for all libraries
        [config["tool"] + f"/output/samgz_split/{libtype}.{libname}.part{part:04d}.sam.gz"
         for libtype in config["ACTIVE_LIBTYPES"]
         for libname in config["LIB_NAMES"][libtype]
         for part in range(NUM_CHUNKS)]

######################################################################
# Step 1: Find split points in the sam.gz file
######################################################################

rule find_samgz_split_points:
    """
    Scan the sam.gz file to find byte positions where we can safely split
    (at boundaries where read IDs change). Save these positions to a file.
    
    This is separate from the actual splitting so we can reuse the split
    points without re-scanning the file every time.
    """
    input:
        samgz = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["samgz"]
    output:
        split_points = config["tool"] + "/output/samgz_split/{libtype}.{libname}.split_points.txt",
        headers = config["tool"] + "/output/samgz_split/{libtype}.{libname}.headers.txt"
    params:
        num_chunks = NUM_CHUNKS
    threads: 2
    resources:
        mem_mb  = 4000,
        runtime = 120  # 2 hours should be enough to scan and find split points
    run:
        import gzip
        import os
        
        print(f"Scanning {input.samgz}")
        
        # Get file size for split calculation
        file_size = os.path.getsize(input.samgz)
        print(f"Input file size: {file_size / (1024**3):.2f} GB")
        
        # Use the specified number of chunks
        num_chunks = params.num_chunks
        print(f"Target: {num_chunks} chunks (~{file_size/num_chunks/(1024**2):.1f} MB each)")
        
        # Read headers and find header end position
        print("\nReading headers...")
        headers = []
        header_end_pos = 0
        
        with gzip.open(input.samgz, 'rb') as infile:
            # Read headers
            while True:
                pos_before = infile.tell()
                line = infile.readline()
                if not line:
                    break
                if not line.startswith(b'@'):
                    header_end_pos = pos_before
                    break
                else:
                    headers.append(line.decode('utf-8'))
            
            print(f"Found {len(headers)} header lines")
        
        # Save headers to file
        with open(output.headers, 'w') as f:
            for header in headers:
                f.write(header)
        
        # Calculate approximate chunk positions
        alignment_section_size = file_size - header_end_pos
        chunk_size = alignment_section_size // num_chunks
        
        print(f"Header end position: {header_end_pos / (1024**2):.2f} MB")
        print(f"Alignment section: ~{alignment_section_size / (1024**3):.2f} GB")
        print(f"Approximate chunk size: {chunk_size / (1024**2):.1f} MB")
        
        # Find exact split points near the approximate positions
        print("\nFinding safe split points (where read ID changes)...")
        split_points = [header_end_pos]  # Start with the first alignment
        
        with gzip.open(input.samgz, 'rb') as infile:
            for i in range(1, num_chunks):
                # Calculate approximate position
                approx_pos = header_end_pos + (i * chunk_size)
                
                print(f"  Finding split {i}/{num_chunks-1} near {approx_pos / (1024**3):.2f} GB ({int(approx_pos/file_size*100)}%)...", end='', flush=True)
                
                # Seek to approximate position
                infile.seek(approx_pos)
                
                # Read forward to find the next newline
                infile.readline()  # Skip partial line
                
                # Now find where the read ID changes
                prev_readid = None
                scan_count = 0
                max_scan = 100000  # Don't scan more than 100k lines
                
                while scan_count < max_scan:
                    pos_before = infile.tell()
                    line = infile.readline()
                    if not line:
                        break
                    
                    scan_count += 1
                    line_str = line.decode('utf-8')
                    
                    # Skip non-alignment lines
                    if line_str.startswith('@'):
                        continue
                    
                    fields = line_str.split('\t', 1)
                    if len(fields) < 1:
                        continue
                    
                    current_readid = fields[0]
                    
                    # Found a read ID change - this is a safe split point
                    if prev_readid is not None and current_readid != prev_readid:
                        split_points.append((pos_before, prev_readid, current_readid))
                        print(f" found at {pos_before / (1024**3):.2f} GB (scanned {scan_count} lines)")
                        break
                    
                    prev_readid = current_readid
                
                # If we didn't find a split point, warn the user
                if scan_count >= max_scan:
                    print(f" WARNING: couldn't find split point after {max_scan} lines, using approximate position")
        
        print(f"\nFound {len(split_points)} split points")
        
        # Save split points to file with read IDs
        with open(output.split_points, 'w') as f:
            # First split point is just the header end position (no read IDs yet)
            if len(split_points) > 0:
                if isinstance(split_points[0], tuple):
                    # All entries have read IDs
                    for pos, prev_id, next_id in split_points:
                        f.write(f"{pos}\t{prev_id}\t{next_id}\n")
                else:
                    # First entry is just header_end_pos
                    f.write(f"{split_points[0]}\tN/A\tN/A\n")
                    for pos, prev_id, next_id in split_points[1:]:
                        f.write(f"{pos}\t{prev_id}\t{next_id}\n")
        
        print(f"Split points saved to {output.split_points}")

######################################################################
# Step 2: Split sam.gz file using pre-computed split points
######################################################################

rule split_samgz_by_readid:
    """
    Split a sam.gz file into multiple chunks using pre-computed split points.
    Creates exactly NUM_CHUNKS output files.
    """
    input:
        samgz = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["samgz"],
        split_points = config["tool"] + "/output/samgz_split/{libtype}.{libname}.split_points.txt",
        headers = config["tool"] + "/output/samgz_split/{libtype}.{libname}.headers.txt"
    output:
        sam_parts = expand(config["tool"] + "/output/samgz_split/{{libtype}}.{{libname}}.part{part:04d}.sam.gz",
                          part=range(NUM_CHUNKS))
    params:
        outdir = config["tool"] + "/output/samgz_split",
        prefix = lambda wildcards: f"{wildcards.libtype}.{wildcards.libname}"
    threads: 2
    resources:
        mem_mb  = 4000,
        runtime = 600  # 10 hours for very large files
    run:
        import gzip
        import os
        from pathlib import Path
        
        # Create output directory
        Path(params.outdir).mkdir(parents=True, exist_ok=True)
        
        print(f"Splitting {input.samgz}")
        print(f"Writing parts to {params.outdir} with prefix {params.prefix}")
        
        # Load headers
        print("Loading headers...")
        headers = []
        with open(input.headers, 'r') as f:
            headers = f.readlines()
        print(f"Loaded {len(headers)} header lines")
        
        # Load split points (first column is byte position)
        print("Loading split points...")
        split_points = []
        with open(input.split_points, 'r') as f:
            for line in f:
                fields = line.strip().split('\t')
                split_points.append(int(fields[0]))
        
        print(f"Loaded {len(split_points)} split points")
        print(f"Will create {len(split_points)} output files")
        
        # Write split files
        print("\nWriting split files...")
        
        with gzip.open(input.samgz, 'rb') as infile:
            for part_num in range(len(split_points)):
                outfile_path = os.path.join(params.outdir, f"{params.prefix}.part{part_num:04d}.sam.gz")
                
                start_pos = split_points[part_num]
                end_pos = split_points[part_num + 1] if part_num + 1 < len(split_points) else None
                
                print(f"  Writing part {part_num:04d}/{len(split_points)-1:04d}...")
                
                with gzip.open(outfile_path, 'wt') as outfile:
                    # Write headers
                    for header in headers:
                        outfile.write(header)
                    
                    # Seek to start position
                    infile.seek(start_pos)
                    
                    if end_pos is not None:
                        # Read exact byte range
                        bytes_to_read = end_pos - start_pos
                        chunk_data = infile.read(bytes_to_read)
                        outfile.write(chunk_data.decode('utf-8'))
                    else:
                        # Read to end of file
                        for line in infile:
                            outfile.write(line.decode('utf-8'))
        
        print(f"\nSplitting complete! Created {len(split_points)} files")


