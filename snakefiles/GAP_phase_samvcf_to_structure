"""
This is a snakefile that takes as input a .sam.gz file, a vcf.gz file, a reference genome in fasta format, and a par file to produce a phased genome map.

Workflow Overview:
1. The input sam.gz files may be very large, so we first split them into smaller files,
   each containing 10 million unique read IDs (which means ~20 million SAM entries for paired-end reads).
2. This splitting is done using a Snakemake checkpoint because we don't know ahead of time
   how many parts will be created (depends on the file size).
3. After splitting, we parallelize the processing of these smaller files through sam2seg.

Checkpoint Workflow:
- The 'split_samgz_by_readid' checkpoint splits the sam.gz file and creates a part_count.txt file
- The 'get_all_split_outputs' function reads the part_count.txt to determine how many parts were created
- Downstream rules can then process each part in parallel using the {part} wildcard

#!/bin/bash
#
#SBATCH --job-name=genpairs
#SBATCH --partition=tttt
#SBATCH --time=6-00:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=32G

# The point of this script is to see if I can get the hickit working with the phasing data previously used.

# Mbo1/NlaIII
BAM=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.bam
SAMGZ=/oak/stanford/groups/tttt/users/lynx/research/dip-c/processed/250920_donor21_bulk_cerebellum_regular/aln.sam.gz
VCFGZ=/scratch/users/darrints/donor21_phasing2_202511/GAP_phase/output/final_output/complete_assembly.hap.phased.vcf.gz
PARBED=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/pseudoautosomal.bed
#HICKIT=/scratch/users/darrints/donor21_phasing2_202511/testing_hickit/hickit/hickit.js
BINDIR=/home/users/darrints/bin

${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
${BINDIR}/hickit.js sam2seg -v phased_SNP.tsv ${SAMGZ} | \
     ${BINDIR}/hickit.js chronly - | \
     ${BINDIR}/hickit.js bedflt ${PARBED} - | \
     gzip > contacts.seg.gz
${BINDIR}/hickit -i contacts.seg.gz -o - | \
    bgzip > contacts.pairs.gz

Use the example config file from GAP_phase_multi_library.yaml to provide the sam.gz file as input to this module.
"""

import os

configfile: "config.yaml"
config["tool"] = "GAP_phase_samvcf_to_structure"

# make softlinks of the input files
# files end up here:
#   assem = config["tool"] + "/input/assembly/{nom}_input.fasta",
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))
softlinks_rule_path=os.path.join(filepath, "snakemake_includes/assembly_softlinks")
include: softlinks_rule_path

# Paths to hickit tools
K8 = os.path.join(filepath, "../bin/k8")
HICKIT_DIR = os.path.join(filepath, "../dependencies/hickit")
HICKIT_JS = os.path.join(HICKIT_DIR, "hickit.js")
HICKIT_BIN = os.path.join(HICKIT_DIR, "hickit")

###############################################################################
# Parse library configuration - supports both old and new config formats
# ALL LIBRARIES NOW USE BAM FILES (not FASTQ)
###############################################################################

# For each Hi-C library, require that a .sam.gz file is provided in the config. This won't work otherwise
# Only Hi-C and Chicago libraries need sam.gz files for this workflow

# Validate sam.gz file paths - check for leading/trailing whitespace
for libtype in config["libraries"]:
    # Only check for samgz in Hi-C and Chicago libraries
    if libtype not in ["hic", "chi"]:
        continue
    
    for libname in config["libraries"][libtype]:
        # Check if samgz key exists
        if "samgz" not in config["libraries"][libtype][libname]:
            raise ValueError(
                f"\n\nERROR: samgz file path is required for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please add 'samgz: /path/to/file.sam.gz' to your config file."
            )
        
        samgz_path = config["libraries"][libtype][libname]["samgz"]
        
        # Skip if samgz is None or empty string
        if samgz_path is None or samgz_path == "":
            raise ValueError(
                f"\n\nERROR: samgz file path cannot be None or empty for {libtype} libraries!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n\n"
                f"Please provide a valid path or remove this library from the config."
            )
        
        if samgz_path != samgz_path.strip():
            raise ValueError(
                f"\n\nERROR: samgz file path has leading or trailing whitespace!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n"
                f"Fixed path should be: '{samgz_path.strip()}'\n\n"
                f"Please fix your config file and remove the extra whitespace."
            )
        # make sure the samgz path exists
        if not os.path.isfile(samgz_path):
            raise FileNotFoundError(
                f"\n\nERROR: samgz file not found!\n"
                f"Library type: {libtype}\n"
                f"Library name: {libname}\n"
                f"Path: '{samgz_path}'\n\n"
                f"Please fix your config file with the correct path."
            )

# Validate that the phased VCF files are provided in the config and do not have whitespace issues
for vcf_key in config["PHASED_VCFs"]:
    phased_vcf_path = config["PHASED_VCFs"][vcf_key]["path"]
    if phased_vcf_path != phased_vcf_path.strip():
        raise ValueError(
            f"\n\nERROR: Phased VCF file path has leading or trailing whitespace!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n"
            f"Fixed path should be: '{phased_vcf_path.strip()}'\n\n"
            f"Please fix your config file and remove the extra whitespace."
        )
    # make sure the phased VCF path exists
    if not os.path.isfile(phased_vcf_path):
        raise FileNotFoundError(
            f"\n\nERROR: Phased VCF file not found!\n"
            f"VCF key: {vcf_key}\n"
            f"Path: '{phased_vcf_path}'\n\n"
            f"Please fix your config file with the correct path."
        )

# The phased VCF and the sam.gz files are now validated and can be used in the workflow.

# Build library lists for Hi-C and Chicago only (these are the ones we process in this workflow)
config["LIB_NAMES"] = {}
for libtype in ["hic", "chi"]:
    if libtype in config["libraries"]:
        config["LIB_NAMES"][libtype] = list(config["libraries"][libtype].keys())
    else:
        config["LIB_NAMES"][libtype] = []

# Remove empty library types from the workflow
# No need to process library types with no data
for libtype in ["hic", "chi"]:
    if len(config["LIB_NAMES"][libtype]) == 0:
        if libtype in config["libraries"]:
            del config["libraries"][libtype]
        del config["LIB_NAMES"][libtype]

# Get list of active library types (those with actual data and sam.gz files)
config["ACTIVE_LIBTYPES"] = list(config["LIB_NAMES"].keys())

# make sure everything in the config["CHROMS"] is a string
config["CHROMS"] = [str(chrom) for chrom in config["CHROMS"]]

wildcard_constraints:
    phasedvcf="[A-Za-z0-9]+",
    libtype="[A-Za-z]+",
    libname="[A-Za-z0-9]+",
    datatype="[A-Za-z]+",
    part="[0-9]+"

# Configuration: reads per chunk
READS_PER_CHUNK = 10_000_000  # 10 million unique read IDs per chunk

# Helper function to get split file outputs from checkpoint
def get_all_split_outputs(wildcards):
    """
    Read the part_count.txt file created by the checkpoint to determine
    how many parts were created, then return all the expected output files.
    """
    checkpoint_output = checkpoints.split_samgz_by_readid.get(
        libtype=wildcards.libtype, 
        libname=wildcards.libname
    ).output.part_count
    
    # Read the part count file
    with open(checkpoint_output, 'r') as f:
        num_parts = int(f.read().strip())
    
    # Return list of all expected split files
    return [
        config["tool"] + f"/output/samgz_split/{wildcards.libtype}.{wildcards.libname}.part{part:04d}.sam.gz"
        for part in range(num_parts)
    ]

rule all:
    input:
        # Aggregate all split files for all libraries
        [config["tool"] + f"/output/samgz_split/{libtype}.{libname}/complete.txt"
         for libtype in config["ACTIVE_LIBTYPES"]
         for libname in config["LIB_NAMES"][libtype]]

######################################################################
# Checkpoint: Split sam.gz file by counting unique read IDs
######################################################################

checkpoint split_samgz_by_readid:
    """
    Split a sam.gz file into chunks, each containing READS_PER_CHUNK unique read IDs.
    
    Strategy:
    1. Copy sam.gz to $TMPDIR for faster I/O
    2. Read through sequentially, tracking unique read IDs
    3. When we hit READS_PER_CHUNK unique IDs, close current file and start new one
    4. Move completed files from $TMPDIR to target directory
    5. Clean up temporary files
    
    This is a checkpoint because we don't know ahead of time how many parts will be created.
    """
    input:
        samgz = lambda wildcards: config["libraries"][wildcards.libtype][wildcards.libname]["samgz"]
    output:
        part_count = config["tool"] + "/output/samgz_split/{libtype}.{libname}/part_count.txt"
    params:
        outdir = config["tool"] + "/output/samgz_split",
        reads_per_chunk = READS_PER_CHUNK
    threads: 2
    resources:
        mem_mb = 8000,
        runtime = 1800  # 30 hours for very large files
    run:
        import gzip
        import os
        import shutil
        from pathlib import Path
        
        # Create output directories
        Path(params.outdir).mkdir(parents=True, exist_ok=True)
        Path(output.part_count).parent.mkdir(parents=True, exist_ok=True)
        
        # Get library info for naming
        libtype = wildcards.libtype
        libname = wildcards.libname
        prefix = f"{libtype}.{libname}"
        
        # Copy sam.gz to TMPDIR
        tmpdir = os.environ.get('TMPDIR', '/tmp')
        local_samgz = os.path.join(tmpdir, f"{prefix}.sam.gz")
        
        print(f"Copying {input.samgz} to {local_samgz}...", flush=True)
        shutil.copy2(input.samgz, local_samgz)
        print(f"Copy complete. Starting split...", flush=True)
        
        # Read headers first
        headers = []
        with gzip.open(local_samgz, 'rt') as infile:
            for line in infile:
                if line.startswith('@'):
                    headers.append(line)
                else:
                    break
        
        print(f"Found {len(headers)} header lines", flush=True)
        
        # Now split by unique read IDs
        part_num = 0
        unique_readids = set()
        current_readid = None
        lines_buffer = []
        
        # Temporary output file in TMPDIR
        tmp_outfile_path = os.path.join(tmpdir, f"{prefix}.part{part_num:04d}.sam.gz")
        outfile = gzip.open(tmp_outfile_path, 'wt', compresslevel=1)
        
        # Write headers to first file
        for header in headers:
            outfile.write(header)
        
        print(f"\nProcessing reads (target: {params.reads_per_chunk:,} unique read IDs per part)...", flush=True)
        print(f"Writing part {part_num:04d}...", flush=True)
        
        with gzip.open(local_samgz, 'rt') as infile:
            for line in infile:
                # Skip headers
                if line.startswith('@'):
                    continue
                
                # Get read ID
                fields = line.split('\t', 1)
                if len(fields) < 1:
                    outfile.write(line)
                    continue
                
                readid = fields[0]
                
                # Track unique read IDs
                if readid not in unique_readids:
                    unique_readids.add(readid)
                    
                    # Check if we've hit the threshold
                    if len(unique_readids) >= params.reads_per_chunk:
                        # Close current file
                        outfile.close()
                        
                        # Move to target directory
                        final_path = os.path.join(params.outdir, f"{prefix}.part{part_num:04d}.sam.gz")
                        shutil.move(tmp_outfile_path, final_path)
                        print(f"Part {part_num:04d} complete: {len(unique_readids):,} unique reads", flush=True)
                        
                        # Start new file
                        part_num += 1
                        unique_readids = set()
                        tmp_outfile_path = os.path.join(tmpdir, f"{prefix}.part{part_num:04d}.sam.gz")
                        outfile = gzip.open(tmp_outfile_path, 'wt', compresslevel=1)
                        
                        # Write headers to new file
                        for header in headers:
                            outfile.write(header)
                        
                        print(f"Writing part {part_num:04d}...", flush=True)
                
                # Write line to current file
                outfile.write(line)
        
        # Close and move the last file
        outfile.close()
        if len(unique_readids) > 0:
            final_path = os.path.join(params.outdir, f"{prefix}.part{part_num:04d}.sam.gz")
            shutil.move(tmp_outfile_path, final_path)
            print(f"Part {part_num:04d} complete: {len(unique_readids):,} unique reads", flush=True)
            part_num += 1
        
        # Clean up local copy
        print(f"\nRemoving local copy: {local_samgz}", flush=True)
        os.remove(local_samgz)
        
        # Write part count to checkpoint file
        total_parts = part_num
        with open(output.part_count, 'w') as f:
            f.write(str(total_parts))
        
        print(f"\nSplitting complete! Created {total_parts} parts", flush=True)

######################################################################
# Aggregate rule to mark completion
######################################################################

rule aggregate_split_files:
    """
    Dummy rule to aggregate all split files after checkpoint completes.
    This allows the checkpoint to determine how many files were created.
    """
    input:
        get_all_split_outputs
    output:
        done = config["tool"] + "/output/samgz_split/{libtype}.{libname}/complete.txt"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    shell:
        """
        echo "All split files created for {wildcards.libtype}.{wildcards.libname}" > {output.done}
        """


##############################################################################
# Step 2: make a rule to get the SNPs for dip-c/hickit's input requirements
##############################################################################

#rule softlinks_phased_vcf:
#    input:
#        # we make softlinks of the phased VCFs here in the input directory
#        vcf = lambda wildcards: config["PHASED_VCFs"][wildcards.phasedvcf]["path"]
#    output:
#        vcf_link = config["tool"] + "/input/phased_vcfs/{phasedvcf}_phased.vcf.gz"
#    params:
#        outdir = config["tool"] + "/input/phased_vcfs"
#    threads: 1
#    resources:
#        mem_mb  = 1000,
#        runtime = 10
#    shell:
#        """
#        mkdir -p {params.outdir}
#        ln -sf {input.vcf} {output.vcf_link}
#        """
#
#rule process_phased_vcf:
#    """
#    Does this step from the hickit pipeline
#        ${BINDIR}/hickit.js vcf2tsv ${VCFGZ} > phased_SNP.tsv
#    """
#    input:
#        vcf = config["tool"] + "/input/phased_vcfs/{phasedvcf}_phased.vcf.gz"
#    output:
#        tsv = config["tool"] + "/output/phased_vcfs/{phasedvcf}_phased_SNP.tsv"
#    params:
#        bindir = HICKIT_DIR
#    threads: 1
#    resources:
#        mem_mb  = 4000,
#        runtime = 60
#    shell: