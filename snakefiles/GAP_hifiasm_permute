"""
Permutes options in hifiasm and tests the final results
Assembles a genome using HiFi and Hi-C data.

Doesn't do a mapping-based QC step because it takes too long.
"""
import itertools
import time
import pandas as pd
from math import ceil as ceil

from functions import percent_of_data_mapping,run_fasta_stats, parse_flagstat, parse_permap

configfile: "config.yaml"
# TODO this needs to be update to have better file structuring later
config["tool"] = "GAP_hifiasm_permute"

### CHECK IF THE CONFIG FILE HAS THE PREFIX
if "prefix" not in config:
    raise Exception("You must specify a prefix for the out filenames in the config file")

### THIS SECTION SETS UP THE ASSEMBLIES TO RUN

# Check HiFi files exist and determine which are BAM vs FASTQ
config["hifi_bam"] = []
config["hifi_fastq"] = []
if "hifi" not in config:
    raise Exception("You must specify HiFi reads in the config file")
for hifi_file in config["hifi"]:
    if not os.path.exists(hifi_file):
        raise Exception(f"The following HiFi file does not exist: {hifi_file}")
    if hifi_file.endswith(".bam"):
        config["hifi_bam"].append(hifi_file)
    elif hifi_file.endswith(".fastq.gz") or hifi_file.endswith(".fq.gz"):
        config["hifi_fastq"].append(hifi_file)
    else:
        raise Exception(f"HiFi file must be .bam, .fastq.gz, or .fq.gz: {hifi_file}")

# Create list of all HiFi fastq files (including those to be converted from BAM)
config["hifi_fastq_final"] = config["hifi_fastq"][:]
for bam_file in config["hifi_bam"]:
    # Convert path to the output fastq.gz location
    fastq_path = "step0_bam_to_fastq/" + os.path.basename(bam_file).replace(".bam", ".fastq.gz")
    config["hifi_fastq_final"].append(fastq_path)

# Set up the HiFi dictionary to ask later
config["hic_dict"] = {"None": ""}
# add a step to check that these files exist
if "hic" in config:
    # check that all of the hi-c files exist
    failed = []
    for lib in config["hic"]:
        for direction in ["R1", "R2"]:
            for libfile in config["hic"][lib][direction]:
                if not os.path.exists(libfile):
                    failed.append(libfile)
    if len(failed) > 0:
        raise Exception("The following Hi-C files do not exist:\n" + "\n".join(failed))
    # if they exist, proceed
    hic_R1_string = "--h1 " + ",".join([filepath for filepath in config["hic"][lib]["R1"] for lib in config["hic"]])
    hic_R2_string = "--h2 " + ",".join([filepath for filepath in config["hic"][lib]["R2"] for lib in config["hic"]])
    config["hic_dict"]["HiC"] = hic_R1_string + " " + hic_R2_string

# set up the ultralong parameters
config["ul_dict"] = {"None": ""}
if "ul" in config:
    # check that all of the ultralong reads exist
    failed = []
    for treatment in config["ul"]:
        for libfile in config["ul"][treatment]:
            if not os.path.exists(libfile):
                failed.append(libfile)
    if len(failed) > 0:
        raise Exception("The following ultralong read files do not exist:\n" + "\n".join(failed))
    # proceed since we are sure they are all present
    for treatment in config["ul"]:
        UL_string = "--ul " + ",".join(config["ul"][treatment])
        config["ul_dict"]["UL" + treatment] = UL_string

# set up the parameters
config["parameter_dict"] = {"None": ""}
if "hifiasm_params" in config:
    for param_name in config["hifiasm_params"]:
        config["parameter_dict"][param_name] = config["hifiasm_params"][param_name]

# now that we have the parameters and the types of reads, make permutations
# first make permutations of the parameters
permutations = []
for L in range(len(config["parameter_dict"]) + 1):
    for subset in itertools.combinations(config["parameter_dict"], L):
        permutations.append(list(subset)) #TODO THIS NEEDS TO BE SORTED

permutations = [x for x in permutations if len(x) > 0]

permutations2 = []
for entry in permutations:
    for ul in config["ul_dict"]:
        for hic in config["hic_dict"]:
            permutations2.append([entry,[ul, hic]])

permutations = permutations2

base_analyses = set()
analyses = set()
for entry in permutations:
    # we must idependently set up the prefix (parameters) and suffix (which reads we use)
    prefix = entry[0]
    prefix_string = ""
    if (len(prefix) == 1) and (prefix[0] == "None"):
        prefix_string = "None"
    else:
        prefix_string = "_".join([y for y in prefix if y != "None"])
    base_analyses.add(prefix_string)
    suffix = entry[1]
    suffix_string = "_".join(suffix)
    analyses.add(prefix_string + "-" + suffix_string)

config["base_analyses"] = [x+"-None_None" for x in base_analyses]
config["ul_analyses"]   = [x for x in list(sorted(analyses)) if (x not in config["base_analyses"]) and ("HiC" not in x)]
config["hic_analyses"]      = [x for x in list(sorted(analyses)) if (x not in config["base_analyses"]) and (x not in config["ul_analyses"])]
config["all_analyses"] = list(set(config["base_analyses"] + config["ul_analyses"] + config["hic_analyses"]))

merqury_out_files = []
config["merqury_reads"] = []
# If we want to perform merqury analyses, figure out which files we will use
# also, we need to sure that the genome size is specified
if "perform_merqury" in config:
    config["meryl_source"] = []
    if config["perform_merqury"] == True:
        # now we determine which read files to use
        if "merqury_on_hifi" in config:
            # add hifi reads
            if config["merqury_on_hifi"] == True:
                config["meryl_source"].append("hifi")
                for readfile in config["hifi"]:
                    # check if this file exists
                    if not os.path.exists(readfile):
                        raise Exception("The following HiFi file does not exist:\n" + readfile)
                    config["merqury_reads"].append(readfile)
            # add the illumina reads
            if config["merqury_on_illumina"] == True:
                config["meryl_source"].append("illumina")
                for lib in config["illumina"]:
                    for direction in ["R1", "R2"]:
                        for readfile in config["illumina"][lib][direction]:
                            # check if this file exists
                            if not os.path.exists(readfile):
                                raise Exception("The following illumina file does not exist:\n" + readfile)
                            config["merqury_reads"].append(readfile)
            # condense the meryl_source to a string joined by _
            config["meryl_source"] = "_".join(config["meryl_source"])
        # now we must define the output files that snakemake will look for
        meryl_db = "step3_merqury/" + config["prefix"] + ".meryl/merylIndex"
        for this_analysis in config["all_analyses"]:
            this_result  = "step3_merqury/runs/{all_analyses}/{all_analyses}.completeness.stats".format(all_analyses = this_analysis)
            merqury_out_files.append(this_result)
            this_result  = "step3_merqury/runs/{all_analyses}/{all_analyses}.qv".format(all_analyses = this_analysis)
            merqury_out_files.append(this_result)

rule all:
    input:
        expand("step2_assemblies/" + config["prefix"] +  ".{all_analyses}.hap{hap}.p_ctg.fasta", all_analyses = config["all_analyses"],  hap = [1,2]),
        "stepFINAL/final_results.tsv",
        merqury_out_files
        #expand("step3_stats/" + config["prefix"] +  ".{all_analyses}.hap{hap}.p_ctg.stats", all_analyses = set(config["base_analyses"] + config["ul_analyses"] + config["hic_analyses"]),  hap = [1,2]),

rule bam_to_fastq:
    """
    Convert BAM files to FASTQ.GZ for use with hifiasm
    First copies BAM to local disk, processes it, then cleans up
    Runtime: 30 minutes per GB of input BAM file
    """
    input:
        bam = lambda wildcards: [b for b in config["hifi_bam"] if os.path.basename(b).replace(".bam", "") == wildcards.bam_basename][0]
    output:
        fastq = "step0_bam_to_fastq/{bam_basename}.fastq.gz"
    params:
        local_bam = lambda wildcards: f"$TMPDIR/{wildcards.bam_basename}.bam"
    threads: 2
    resources:
        mem_mb = 4000,
        runtime = lambda wildcards, input: max(60, int(os.path.getsize([b for b in config["hifi_bam"] if os.path.basename(b).replace(".bam", "") == wildcards.bam_basename][0]) / (1024**3) * 30))
    shell:
        """
        mkdir -p step0_bam_to_fastq
        
        # Copy BAM to local disk
        echo "Copying BAM to local disk: {params.local_bam}"
        cp {input.bam} {params.local_bam}
        
        # Process from local BAM
        echo "Converting BAM to FASTQ..."
        samtools view {params.local_bam} | awk '{{print "@"$1"\\n"$10"\\n+\\n"$11}}' | pigz -p {threads} -c > {output.fastq}
        
        # Clean up local BAM
        echo "Removing local BAM copy..."
        rm -f {params.local_bam}
        """

rule base_assemble:
    input:
        LR  = config["hifi_fastq_final"]
    output:
        ec_bin  = "step1_hifiasm/base/{base}/" + config["prefix"] + ".ec.bin",
        re_bin  = "step1_hifiasm/base/{base}/" + config["prefix"] + ".ovlp.reverse.bin",
        so_bin  = "step1_hifiasm/base/{base}/" + config["prefix"] + ".ovlp.source.bin",
        hap1gfa = "step1_hifiasm/base/{base}/" + config["prefix"] + ".bp.hap1.p_ctg.gfa",
        hap2gfa = "step1_hifiasm/base/{base}/" + config["prefix"] + ".bp.hap2.p_ctg.gfa",
    threads: 48
    resources:
        mem_mb = 150000,
        runtime = 2880
    params:
        prefix = config["prefix"],
        HiFi_string        = lambda wildcards, input: " ".join([os.path.abspath(f) for f in input.LR]),
        analysis         = lambda wildcards: wildcards.base,
        parameter_string = lambda wildcards: " ".join([config["parameter_dict"][x] for x in wildcards.base.split("-")[0].split("_")]),
        outdir           = lambda wildcards: os.path.abspath(f"step1_hifiasm/base/{wildcards.base}")
    shell:
        """
        # Create output directory
        mkdir -p {params.outdir}
        
        # Run hifiasm in the output directory
        cd {params.outdir}
        hifiasm -o {params.prefix} \
          -t {threads} \
          {params.parameter_string} \
          {params.HiFi_string} >> assemble_log.txt 2>&1
        """

rule UL_assemblies:
    input:
        LR  = config["hifi_fastq_final"],
        ec_bin = expand("step1_hifiasm/base/{base}/" + config["prefix"] + ".ec.bin", base = config["base_analyses"]),
        re_bin = expand("step1_hifiasm/base/{base}/" + config["prefix"] + ".ovlp.reverse.bin", base = config["base_analyses"]),
        so_bin = expand("step1_hifiasm/base/{base}/" + config["prefix"] + ".ovlp.source.bin", base = config["base_analyses"])
    output:
        hap1gfa    = "step1_hifiasm/ul_analyses/{ul_analysis}/" + config["prefix"] + ".bp.hap1.p_ctg.gfa",
        hap2gfa    = "step1_hifiasm/ul_analyses/{ul_analysis}/" + config["prefix"] + ".bp.hap2.p_ctg.gfa",
        uidx_bin   = "step1_hifiasm/ul_analyses/{ul_analysis}/" + config["prefix"] + ".uidx.bin",
        reuidx_bin = "step1_hifiasm/ul_analyses/{ul_analysis}/" + config["prefix"] + ".re.uidx.bin",
    threads: 48
    resources:
        mem_mb = 150000,
        runtime = 2880
    params:
        input_direc      = lambda wildcards: "step1_hifiasm/base/" + wildcards.ul_analysis.split("-")[0] + "-None_None", 
        prefix           = config["prefix"],
        HiFi_string        = " ".join(config["hifi_fastq_final"]),
        analysis         = lambda wildcards: wildcards.ul_analysis,
        parameter_string = lambda wildcards: " ".join([config["parameter_dict"][x] for x in wildcards.ul_analysis.split("-")[0].split("_")]),
        ul_string        = lambda wildcards: config["ul_dict"][ wildcards.ul_analysis.split("-")[1].split("_")[0]]
    shell:
        """
        mkdir -p step1_hifiasm
        cd step1_hifiasm
        mkdir -p ul_analyses
        cd ul_analyses
        mkdir -p {params.analysis}
        cd {params.analysis}
        cp ../../../{params.input_direc}/*.bin ./
        hifiasm -o {params.prefix} \
          -t {threads} \
          {params.parameter_string} \
          {params.ul_string} \
          {params.HiFi_string} >> assemble_log.txt 2>&1
        """

rule HiC_assemblies:
    input:
        LR  = config["hifi_fastq_final"],
        hic_R1 = lambda wildcards: [filepath for lib in config["hic"] for filepath in config["hic"][lib]["R1"]],
        hic_R2 = lambda wildcards: [filepath for lib in config["hic"] for filepath in config["hic"][lib]["R2"]]
    output:
        hap1gfa    = "step1_hifiasm/hic_analyses/{hic_analysis}/" + config["prefix"] + ".hic.hap1.p_ctg.gfa",
        hap2gfa    = "step1_hifiasm/hic_analyses/{hic_analysis}/" + config["prefix"] + ".hic.hap2.p_ctg.gfa",
    threads: 48
    resources:
        mem_mb = 300000,
        runtime = 5760
    params:
        prefix           = config["prefix"],
        HiFi_string      = lambda wildcards, input: " ".join([os.path.abspath(f) for f in input.LR]),
        parameter_string = lambda wildcards: " ".join([config["parameter_dict"][x] for x in wildcards.hic_analysis.split("-")[0].split("_")]),
        hic_string       = lambda wildcards: config["hic_dict"][wildcards.hic_analysis.split("-")[1].split("_")[1]],
        outdir           = lambda wildcards: os.path.abspath(f"step1_hifiasm/hic_analyses/{wildcards.hic_analysis}")
    shell:
        """
        # Create output directory
        mkdir -p {params.outdir}

        # Run hifiasm in the output directory
        cd {params.outdir}
        hifiasm -o {params.prefix} \
          -t {threads} \
          {params.parameter_string} \
          {params.hic_string} \
          {params.HiFi_string} >> assemble_log.txt 2>&1
        """

rule base_gfa_to_fasta:
    input:
        gfa = "step1_hifiasm/base/{base}/" + config["prefix"] + ".bp.hap{hap}.p_ctg.gfa",
    output:
        gfa = temp("step2_assemblies/base/" + config["prefix"] +  ".{base}.hap{hap}.p_ctg.fasta")
    threads: 1
    resources:
        mem_mb = 4000,
        runtime = 30
    shell:
        """
        cat {input.gfa} | \
          awk '{{if ($1 == "S"){{printf(">%s\\n%s\\n", $2, $3)}} }}' | \
          fold -60 > {output.gfa}
        """

rule ul_gfa_to_fasta:
    input:
        gfa = "step1_hifiasm/ul_analyses/{ul_analysis}/" + config["prefix"] + ".bp.hap{hap}.p_ctg.gfa",
    output:
        gfa = temp("step2_assemblies/ul_analyses/" + config["prefix"] +  ".{ul_analysis}.hap{hap}.p_ctg.fasta")
    threads: 1
    resources:
        mem_mb = 4000,
        runtime = 30
    shell:
        """
        cat {input.gfa} | \
          awk '{{if ($1 == "S"){{printf(">%s\\n%s\\n", $2, $3)}} }}' | \
          fold -60 > {output.gfa}
        """

rule hic_gfa_to_fasta:
    input:
        gfa = "step1_hifiasm/hic_analyses/{hic_analysis}/" + config["prefix"] + ".hic.hap{hap}.p_ctg.gfa",
    output:
        gfa = temp("step2_assemblies/hic_analyses/" + config["prefix"] +  ".{hic_analysis}.hap{hap}.p_ctg.fasta")
    threads: 1
    resources:
        mem_mb = 4000,
        runtime = 30
    shell:
        """
        cat {input.gfa} | \
          awk '{{if ($1 == "S"){{printf(">%s\\n%s\\n", $2, $3)}} }}' | \
          fold -60 > {output.gfa}
        """

rule collate_assemblies:
    input:
        base         = expand("step2_assemblies/base/"         + config["prefix"] +  ".{base}.hap{hap}.p_ctg.fasta",         base = config["base_analyses"], hap = [1,2]),
        ul_analysis  = expand("step2_assemblies/ul_analyses/"  + config["prefix"] +  ".{ul_analysis}.hap{hap}.p_ctg.fasta",  ul_analysis = config["ul_analyses"],   hap = [1,2]),
        hic_analysis = expand("step2_assemblies/hic_analyses/" + config["prefix"] +  ".{hic_analysis}.hap{hap}.p_ctg.fasta", hic_analysis = config["hic_analyses"],  hap = [1,2])
    output:
        assemblies = expand("step2_assemblies/" + config["prefix"] +  ".{all_analyses}.hap{hap}.p_ctg.fasta", all_analyses = config["all_analyses"],  hap = [1,2])
    threads: 1
    resources:
        mem_mb = 2000,
        runtime = 30
    shell:
        r"""
        for assembly in {input.base} {input.ul_analysis} {input.hic_analysis}; do
          # only copy if the file does not yet exist in the new location
            if [ ! -f $(echo $assembly | sed 's/base\///g' | sed 's/ul_analyses\///g' | sed 's/hic_analyses\///g') ]; then
                mv $assembly step2_assemblies/
            fi
        #rm -rf step2_assemblies/base step2_assemblies/ul_analyses step2_assemblies/hic_analyses
        done
        """

rule get_kmer_size:
    output:
        kmer_estimate = "step3_merqury/" + config["prefix"] + ".k_estimate.txt"
    params:
        genome_size = config.get("genome_size", 1000000000) * 2
    threads: 1
    resources:
        mem_mb = 2000,
        runtime = 10
    shell:
        """
        # get the kmer size from the merqury output
        sh $MERQURY/best_k.sh {params.genome_size} > {output.kmer_estimate}
        """

rule calculate_final_kmer_size:
    """
    get the final kmer size
    """
    input:
        kmer_estimate = "step3_merqury/" + config["prefix"] + ".k_estimate.txt"
    output:
        kmer_final    = "step3_merqury/" + config["prefix"] + ".k_estimate.final.txt"
    threads: 1
    resources:
        mem_mb = 2000,
        runtime = 5
    run:
        # open the input file and get the last line as an int
        kmer_size = 0
        with open(input.kmer_estimate, "r") as f:
            last_line = f.readlines()[-1]
            kmer_size = float(last_line.split()[0])
        # kmer_size is a float, so round it up to the nearest int
        kmer_size = int(ceil(kmer_size))
        # round up the kmer_size to the nearest odd number
        if kmer_size % 2 == 0:
            kmer_size += 1
        # write the final kmer size to the output file
        with open(output.kmer_final, "w") as f:
            f.write(str(kmer_size))

rule fofn_reads:
    input:
        reads = config["merqury_reads"]
    output:
        fofn = "step3_merqury/" + config["prefix"] + ".reads.fofn"
    threads: 1
    resources:
        mem_mb = 1000,
        runtime = 5
    run:
        # print each read file to its own line
        with open(output.fofn, "w") as f:
            for read in input.reads:
                f.write(read + "\n")

rule meryl_count_reads:
    """
    Uses meryl to count the kmers in the reads we want to use
    """
    input:
        reads     = config["merqury_reads"],
        kmer_size = "step3_merqury/" + config["prefix"] + ".k_estimate.final.txt",
        fofn      = "step3_merqury/" + config["prefix"] + ".reads.fofn"
    output:
        meryl_db = "step3_merqury/" + config["prefix"] + ".meryl/merylIndex"
    params:
        reads_string = " ".join(config["merqury_reads"])
    threads: 32
    resources:
        mem_mb = 64000,
        runtime = 1440
    shell:
        """
        KMER_SIZE=$(cat {input.kmer_size})
        meryl k=$KMER_SIZE threads={threads} count {params.reads_string} output {output.meryl_db}
        """

rule merqury_basic:
    """
    Performs a basic merqury run on the assembly and puts the stats in a file
    """
    input:
        merylindex = "step3_merqury/" + config["prefix"] + ".meryl/merylIndex",
        hap1       = "step2_assemblies/" + config["prefix"] +  ".{all_analyses}.hap1.p_ctg.fasta",
        hap2       = "step2_assemblies/" + config["prefix"] +  ".{all_analyses}.hap2.p_ctg.fasta"
    output:
        completeness = "step3_merqury/runs/{all_analyses}/{all_analyses}.completeness.stats",
        qv           = "step3_merqury/runs/{all_analyses}/{all_analyses}.qv"
    params:
        abs_meryl = os.path.abspath("step3_merqury/" + config["prefix"] + ".meryl/"),
        abs_hap1  = os.path.abspath("step2_assemblies/" + config["prefix"] + ".{all_analyses}.hap1.p_ctg.fasta"),
        abs_hap2  = os.path.abspath("step2_assemblies/" + config["prefix"] + ".{all_analyses}.hap2.p_ctg.fasta")
    threads: 16
    resources:
        mem_mb = 32000,
        runtime = 240
    shell:
        """
        mkdir -p step3_merqury/
        cd step3_merqury/
        mkdir -p runs/
        cd runs/
        mkdir -p {wildcards.all_analyses}
        cd {wildcards.all_analyses}
        $MERQURY/merqury.sh {params.abs_meryl} {params.abs_hap1} {params.abs_hap2} {wildcards.all_analyses}
        rm -rf TEST.{wildcards.all_analyses}.hap1.p_ctg.meryl
        rm -rf TEST.{wildcards.all_analyses}.hap2.p_ctg.meryl
        """

def parse_completeness(filepath):
    """
    open the completeness file and return a dictionary of the results
    """
    results_dict = {}
    with open(filepath, "r") as f:
        counter = 0
        for line in f:
            fields = line.strip().split()
            if counter == 0:
                results_dict["hap1_completeness"] = fields[-1]
            elif counter == 1:
                results_dict["hap2_completeness"] = fields[-1]
            elif counter == 2:
                results_dict["bothHap_completeness"] = fields[-1]
            counter += 1
    return results_dict

def parse_QV(filepath):
    """
    open the QV file from Merqury and parse the results
    """
    results_dict = {}
    with open(filepath, "r") as f:
        counter = 0
        for line in f:
            fields = line.strip().split()
            if counter == 0:
                results_dict["hap1_QV"] = fields[-2]
            elif counter == 1:
                results_dict["hap2_QV"] = fields[-2]
            counter += 1
    return results_dict

rule make_genome_stats_table:
    """
    makes a table of all the relevant genome stats
    """
    input:
        assembly     = expand("step2_assemblies/" + config["prefix"] +  ".{all_analyses}.hap{hap}.p_ctg.fasta", all_analyses = config["all_analyses"],  hap = [1,2]),
        completeness = expand("step3_merqury/runs/{all_analyses}/{all_analyses}.completeness.stats", all_analyses = config["all_analyses"]) if config.get("perform_merqury", False) else [],
        qv           = expand("step3_merqury/runs/{all_analyses}/{all_analyses}.qv", all_analyses = config["all_analyses"]) if config.get("perform_merqury", False) else []
    output:
        results_table = "stepFINAL/final_results.tsv"
    threads: 1
    resources:
        mem_mb = 8000,
        runtime = 60
    run:
        all_samples = []
        for nom in config["all_analyses"]:
            this_dict = {}
            this_dict["name"] = nom
            this_dict["params"] = " ".join([config["parameter_dict"][x] for x in nom.split("-")[0].split("_")])
            this_dict["UL"] = nom.split("-")[1].split("_")[0]
            this_dict["HiC"] = nom.split("-")[1].split("_")[1]
            for thishap in [1,2]:
                tfile = "step2_assemblies/" + config["prefix"] + ".{}.hap{}.p_ctg.fasta".format(nom, thishap)
                # now get the other assembly info from fasta_stats
                print("fasta_stats on {}".format(tfile))
                fstats_dict = run_fasta_stats(tfile)
                temp_dict = {"hap{}_{}".format(thishap, k): v for k,v in fstats_dict.items()}

                this_dict = {**this_dict, **temp_dict}
                time.sleep(0.1)
            
            # read in the kmer-completeness info only if perform_merqury is True
            if config.get("perform_merqury", False):
                this_completeness = "step3_merqury/runs/{}/{}.completeness.stats".format(nom, nom)
                this_dict = {**this_dict, **parse_completeness(this_completeness)}
                # read in the qv info
                this_qv = "step3_merqury/runs/{}/{}.qv".format(nom, nom)
                this_dict = {**this_dict, **parse_QV(this_qv)}

            all_samples.append(this_dict)
        # now make a df with all the results
        df = pd.DataFrame(all_samples)
        # filter out all of the columns with 'scaffold' in the name
        df = df.loc[:,~df.columns.str.contains('scaffold')]
        # filter out all of the columns with 'Gap' in the name
        df = df.loc[:,~df.columns.str.contains('Gap')]
        # make the text of some of the columns more legible by diving by 1000000 and appending the text MB
        list_of_cols_to_change = list(set([x for x in df.columns if "size" in x] + [x for x in df.columns if "contig_N" in x]))
        for thiscol in list_of_cols_to_change:
            df[thiscol] = df[thiscol].astype(int)/1000000
            # round to two decimal places
            df[thiscol] = df[thiscol].round(decimals=2)
            df[thiscol] = df[thiscol].astype(str) + " MB"

        df.to_csv(output.results_table, sep='\t', index=False)
